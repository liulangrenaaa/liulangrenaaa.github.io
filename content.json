{"pages":[{"title":"About Me","text":"code change the world how to contact me: email: sh_def@163.com where: PuTong ShangHai 如果不是特别注明，博客代码都是基于 当时 linux-stable 版本.","link":"/about/index.html"},{"title":"categories","text":"","link":"/categories/index.html"},{"title":"tags","text":"","link":"/tags/index.html"}],"posts":[{"title":"docker与cgroup如何联系起来的","text":"一直说Docker 是基于linux三大技术 Cgroups, Namespace, OverlayFs 技术。Cgroups, Namespace 着重的是 docker 的运行时，OverlayFs 着重的是 docker Image.但是Docker 与 Cgroups, Namespace, OverlayFs 是怎么联系起来的呢，却很少有人说的明白，我是一个好奇心很重的人，我会用几篇文章来记录分析一下他们怎么练习起来的。 这篇文章主要实操看一看 Docker 与 Cgroup 的联系。 首先去 /sys/fs/cgroup/ 目录看一下当前支持的子系统controller，各个子系统controller下面一般情况下都是空的，表明系统中没有额外的cgroup，基本都只有root_cgroup.虽然4.5版本之后 cgroup v2 就进入mainline了，但是 现在（2020.12.01）docker 基本还是在使用 cgroup v1, 应该也算是一个历史原因了吧，不知道何时 docker能迁移到 cgroup v2 上，期待ing 1234567891011121314151617181920212223242526272829303132333435tencent_clould@ubuntu: /sys/fs/cgroup# lsblkio cpuacct cpuset freezer memory net_cls,net_prio perf_event rdma unifiedcpu cpu,cpuacct devices hugetlb net_cls net_prio pids systemdtencent_clould@ubuntu: /sys/fs/cgroup# mount | grep cgrouptmpfs on /sys/fs/cgroup type tmpfs (ro,nosuid,nodev,noexec,mode=755)cgroup2 on /sys/fs/cgroup/unified type cgroup2 (rw,nosuid,nodev,noexec,relatime)cgroup on /sys/fs/cgroup/systemd type cgroup (rw,nosuid,nodev,noexec,relatime,xattr,name=systemd)cgroup on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,cpuset,clone_children)cgroup on /sys/fs/cgroup/rdma type cgroup (rw,nosuid,nodev,noexec,relatime,rdma)cgroup on /sys/fs/cgroup/hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,hugetlb)cgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory)cgroup on /sys/fs/cgroup/net_cls,net_prio type cgroup (rw,nosuid,nodev,noexec,relatime,net_cls,net_prio)cgroup on /sys/fs/cgroup/perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,perf_event)cgroup on /sys/fs/cgroup/devices type cgroup (rw,nosuid,nodev,noexec,relatime,devices)cgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,cpu,cpuacct)cgroup on /sys/fs/cgroup/pids type cgroup (rw,nosuid,nodev,noexec,relatime,pids)cgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio)freezer on /sys/fs/cgroup/freezer type cgroup (rw,relatime,freezer)tencent_clould@ubuntu: /sys/fs/cgroup# ls memorycgroup.clone_children memory.kmem.failcnt memory.limit_in_bytes memory.usage_in_bytescgroup.event_control memory.kmem.limit_in_bytes memory.max_usage_in_bytes memory.use_hierarchycgroup.procs memory.kmem.max_usage_in_bytes memory.move_charge_at_immigrate notify_on_releasecgroup.sane_behavior memory.kmem.slabinfo memory.numa_stat release_agentdocker memory.kmem.tcp.failcnt memory.oom_control system.sliceinit.scope memory.kmem.tcp.limit_in_bytes memory.pressure_level tasksmachine.slice memory.kmem.tcp.max_usage_in_bytes memory.soft_limit_in_bytes usermemory.failcnt memory.kmem.tcp.usage_in_bytes memory.stat user.slicememory.force_empty memory.kmem.usage_in_bytes memory.swappinesstencent_clould@ubuntu: /sys/fs/cgroup# cat memory/tasks234....17286921731590 基于 ubuntu image新建一个docker，限制memory 为 4MB 123tencent_clould@ubuntu: /sys/fs/cgroup# sudo docker run -t -i -m 4MB ubuntu /bin/bashWARNING: Your kernel does not support swap limit capabilities or the cgroup is not mounted. Memory limited without swap.root@b3c618bb4df9:/# 警告可以忽略，可以看到docker的 id是 b3c618bb4df9。 可以发现各个子系统目录都会新建一个docker目录，且docker目录下有一个目录就是上面的docker container 的id。 1234567891011121314151617181920212223242526272829tencent_clould@ubuntu: /sys/fs/cgroup/memory# lscgroup.clone_children memory.kmem.slabinfo memory.soft_limit_in_bytescgroup.event_control memory.kmem.tcp.failcnt memory.statcgroup.procs memory.kmem.tcp.limit_in_bytes memory.swappinesscgroup.sane_behavior memory.kmem.tcp.max_usage_in_bytes memory.usage_in_bytesdocker memory.kmem.tcp.usage_in_bytes memory.use_hierarchyinit.scope memory.kmem.usage_in_bytes notify_on_releasemachine.slice memory.limit_in_bytes release_agentmemory.failcnt memory.max_usage_in_bytes system.slicememory.force_empty memory.move_charge_at_immigrate tasksmemory.kmem.failcnt memory.numa_stat usermemory.kmem.limit_in_bytes memory.oom_control user.slicememory.kmem.max_usage_in_bytes memory.pressure_leveltencent_clould@ubuntu: /sys/fs/cgroup/memory# ls docker1f3a24f9105950f6f463da0effa2465a518039c3bec76de71c6e20626dcfb286 memory.kmem.usage_in_bytesb3c618bb4df96362b371860f84e5de5248339c9675e4eac7fd7fee6ebb5602df memory.limit_in_bytescgroup.clone_children memory.max_usage_in_bytescgroup.event_control memory.move_charge_at_immigratecgroup.procs memory.numa_statmemory.failcnt memory.oom_controlmemory.force_empty memory.pressure_levelmemory.kmem.failcnt memory.soft_limit_in_bytesmemory.kmem.limit_in_bytes memory.statmemory.kmem.max_usage_in_bytes memory.swappinessmemory.kmem.slabinfo memory.usage_in_bytesmemory.kmem.tcp.failcnt memory.use_hierarchymemory.kmem.tcp.limit_in_bytes notify_on_releasememory.kmem.tcp.max_usage_in_bytes tasksmemory.kmem.tcp.usage_in_bytes 其中 docker 目录下除了 container id的目录其他目录都是无效的，docker这个目录只是限制所有container id的一个顶层目录而已。通过 docker/tasks 为空就可以看出来。可以看到 memory/docker/b3c618bb4df9/ 目录下的最大内存限制，就是创建这个容器时设置的最大内存使用示4MB的限制。 12345678910111213tencent_clould@ubuntu: /sys/fs/cgroup/memory/docker# cat taskstencent_clould@ubuntu: /sys/fs/cgroup/memory/docker#tencent_clould@ubuntu: /sys/fs/cgroup/memory/docker/b3c618bb4df96362b371860f84e5de5248339c9675e4eac7fd7fee6ebb5602df# cat memory.limit_in_bytes4194304tencent_clould@ubuntu: /sys/fs/cgroup/memory/docker/b3c618bb4df96362b371860f84e5de5248339c9675e4eac7fd7fee6ebb5602df# cat tasks1733194tencent_clould@ubuntu: /sys/fs/cgroup/memory/docker/b3c618bb4df96362b371860f84e5de5248339c9675e4eac7fd7fee6ebb5602df# ps -aux | grep 1733194root 1733194 0.0 0.1 4108 3296 pts/0 Ss+ 19:03 0:00 /bin/bashubuntu 1737657 0.0 0.0 6432 736 pts/0 S+ 19:11 0:00 grep --color=auto --exclude-dir=.bzr --exclude-dir=CVS --exclude-dir=.git --exclude-dir=.hg --exclude-dir=.svn --exclude-dir=.idea --exclude-dir=.tox tencent_clould@ubuntu: /sys/fs/cgroup/memory/docker/b3c618bb4df96362b371860f84e5de5248339c9675e4eac7fd7fee6ebb5602df# ps -aux | grep 1733194root 1733194 0.0 0.1 4108 3296 pts/0 Ss+ 19:03 0:00 /bin/bashubuntu 1737657 0.0 0.0 6432 736 pts/0 S+ 19:11 0:00 grep --color=auto --exclude-dir=.bzr --exclude-dir=CVS --exclude-dir=.git --exclude-dir=.hg --exclude-dir=.svn --exclude-dir=.idea --exclude-dir=.tox 1733194 可以看到这个 1733194 进程就是启动 容器时的 /bin/bash 进程 由于其他的 cgroup子系统我们没有对刚刚启动的 container 进行限制，所以也理论上从 cgroup上看到的也是没有限制的，可以看看 cpu controller. 123456789tencent_clould@ubuntu: /sys/fs/cgroup/cpu/docker# cat taskstencent_clould@ubuntu: /sys/fs/cgroup/cpu/docker# cd -/sys/fs/cgroup/cpu/docker/b3c618bb4df96362b371860f84e5de5248339c9675e4eac7fd7fee6ebb5602dftencent_clould@ubuntu: /sys/fs/cgroup/cpu/docker/b3c618bb4df96362b371860f84e5de5248339c9675e4eac7fd7fee6ebb5602df# cat tasks1733194tencent_clould@ubuntu: /sys/fs/cgroup/cpu/docker/b3c618bb4df96362b371860f84e5de5248339c9675e4eac7fd7fee6ebb5602df# cat cpu.cfs_period_us100000tencent_clould@ubuntu: /sys/fs/cgroup/cpu/docker/b3c618bb4df96362b371860f84e5de5248339c9675e4eac7fd7fee6ebb5602df# cat cpu.cfs_quota_us-1 这里就不具体介绍 各个子系统目录下文件含义了。","link":"/2020/12/12/Docker%E6%9C%BA%E5%88%B6%E5%88%86%E6%9E%90/docker%E4%B8%8Ecgroup%E5%A6%82%E4%BD%95%E8%81%94%E7%B3%BB%E8%B5%B7%E6%9D%A5%E7%9A%84/"},{"title":"docker与cgroup如何联系起来的","text":"这篇文章主要实操看一看 Docker 与 Namespace 的联系。 首先去 /sys/fs/cgroup/ 目录看一下当前支持的子系统controller，各个子系统controller下面一般情况下都是空的，表明系统中没有额外的cgroup，基本都只有root_cgroup.虽然4.5版本之后 cgroup v2 就进入mainline了，但是 现在（2020.12.01）docker 基本还是在使用 cgroup v1, 应该也算是一个历史原因了吧，不知道何时 docker能迁移到 cgroup v2 上，期待ing 1234567891011121314151617181920212223242526272829303132333435tencent_clould@ubuntu: /sys/fs/cgroup# lsblkio cpuacct cpuset freezer memory net_cls,net_prio perf_event rdma unifiedcpu cpu,cpuacct devices hugetlb net_cls net_prio pids systemdtencent_clould@ubuntu: /sys/fs/cgroup# mount | grep cgrouptmpfs on /sys/fs/cgroup type tmpfs (ro,nosuid,nodev,noexec,mode=755)cgroup2 on /sys/fs/cgroup/unified type cgroup2 (rw,nosuid,nodev,noexec,relatime)cgroup on /sys/fs/cgroup/systemd type cgroup (rw,nosuid,nodev,noexec,relatime,xattr,name=systemd)cgroup on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,cpuset,clone_children)cgroup on /sys/fs/cgroup/rdma type cgroup (rw,nosuid,nodev,noexec,relatime,rdma)cgroup on /sys/fs/cgroup/hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,hugetlb)cgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory)cgroup on /sys/fs/cgroup/net_cls,net_prio type cgroup (rw,nosuid,nodev,noexec,relatime,net_cls,net_prio)cgroup on /sys/fs/cgroup/perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,perf_event)cgroup on /sys/fs/cgroup/devices type cgroup (rw,nosuid,nodev,noexec,relatime,devices)cgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,cpu,cpuacct)cgroup on /sys/fs/cgroup/pids type cgroup (rw,nosuid,nodev,noexec,relatime,pids)cgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio)freezer on /sys/fs/cgroup/freezer type cgroup (rw,relatime,freezer)tencent_clould@ubuntu: /sys/fs/cgroup# ls memorycgroup.clone_children memory.kmem.failcnt memory.limit_in_bytes memory.usage_in_bytescgroup.event_control memory.kmem.limit_in_bytes memory.max_usage_in_bytes memory.use_hierarchycgroup.procs memory.kmem.max_usage_in_bytes memory.move_charge_at_immigrate notify_on_releasecgroup.sane_behavior memory.kmem.slabinfo memory.numa_stat release_agentdocker memory.kmem.tcp.failcnt memory.oom_control system.sliceinit.scope memory.kmem.tcp.limit_in_bytes memory.pressure_level tasksmachine.slice memory.kmem.tcp.max_usage_in_bytes memory.soft_limit_in_bytes usermemory.failcnt memory.kmem.tcp.usage_in_bytes memory.stat user.slicememory.force_empty memory.kmem.usage_in_bytes memory.swappinesstencent_clould@ubuntu: /sys/fs/cgroup# cat memory/tasks234....17286921731590 基于 ubuntu image新建一个docker，限制memory 为 4MB 123tencent_clould@ubuntu: /sys/fs/cgroup# sudo docker run -t -i -m 4MB ubuntu /bin/bashWARNING: Your kernel does not support swap limit capabilities or the cgroup is not mounted. Memory limited without swap.root@b3c618bb4df9:/# 警告可以忽略，可以看到docker的 id是 b3c618bb4df9。 可以发现各个子系统目录都会新建一个docker目录，且docker目录下有一个目录就是上面的docker container 的id。 1234567891011121314151617181920212223242526272829tencent_clould@ubuntu: /sys/fs/cgroup/memory# lscgroup.clone_children memory.kmem.slabinfo memory.soft_limit_in_bytescgroup.event_control memory.kmem.tcp.failcnt memory.statcgroup.procs memory.kmem.tcp.limit_in_bytes memory.swappinesscgroup.sane_behavior memory.kmem.tcp.max_usage_in_bytes memory.usage_in_bytesdocker memory.kmem.tcp.usage_in_bytes memory.use_hierarchyinit.scope memory.kmem.usage_in_bytes notify_on_releasemachine.slice memory.limit_in_bytes release_agentmemory.failcnt memory.max_usage_in_bytes system.slicememory.force_empty memory.move_charge_at_immigrate tasksmemory.kmem.failcnt memory.numa_stat usermemory.kmem.limit_in_bytes memory.oom_control user.slicememory.kmem.max_usage_in_bytes memory.pressure_leveltencent_clould@ubuntu: /sys/fs/cgroup/memory# ls docker1f3a24f9105950f6f463da0effa2465a518039c3bec76de71c6e20626dcfb286 memory.kmem.usage_in_bytesb3c618bb4df96362b371860f84e5de5248339c9675e4eac7fd7fee6ebb5602df memory.limit_in_bytescgroup.clone_children memory.max_usage_in_bytescgroup.event_control memory.move_charge_at_immigratecgroup.procs memory.numa_statmemory.failcnt memory.oom_controlmemory.force_empty memory.pressure_levelmemory.kmem.failcnt memory.soft_limit_in_bytesmemory.kmem.limit_in_bytes memory.statmemory.kmem.max_usage_in_bytes memory.swappinessmemory.kmem.slabinfo memory.usage_in_bytesmemory.kmem.tcp.failcnt memory.use_hierarchymemory.kmem.tcp.limit_in_bytes notify_on_releasememory.kmem.tcp.max_usage_in_bytes tasksmemory.kmem.tcp.usage_in_bytes 其中 docker 目录下除了 container id的目录其他目录都是无效的，docker这个目录只是限制所有container id的一个顶层目录而已。通过 docker/tasks 为空就可以看出来。可以看到 memory/docker/b3c618bb4df9/ 目录下的最大内存限制，就是创建这个容器时设置的最大内存使用示4MB的限制。 12345678910111213tencent_clould@ubuntu: /sys/fs/cgroup/memory/docker# cat taskstencent_clould@ubuntu: /sys/fs/cgroup/memory/docker#tencent_clould@ubuntu: /sys/fs/cgroup/memory/docker/b3c618bb4df96362b371860f84e5de5248339c9675e4eac7fd7fee6ebb5602df# cat memory.limit_in_bytes4194304tencent_clould@ubuntu: /sys/fs/cgroup/memory/docker/b3c618bb4df96362b371860f84e5de5248339c9675e4eac7fd7fee6ebb5602df# cat tasks1733194tencent_clould@ubuntu: /sys/fs/cgroup/memory/docker/b3c618bb4df96362b371860f84e5de5248339c9675e4eac7fd7fee6ebb5602df# ps -aux | grep 1733194root 1733194 0.0 0.1 4108 3296 pts/0 Ss+ 19:03 0:00 /bin/bashubuntu 1737657 0.0 0.0 6432 736 pts/0 S+ 19:11 0:00 grep --color=auto --exclude-dir=.bzr --exclude-dir=CVS --exclude-dir=.git --exclude-dir=.hg --exclude-dir=.svn --exclude-dir=.idea --exclude-dir=.tox tencent_clould@ubuntu: /sys/fs/cgroup/memory/docker/b3c618bb4df96362b371860f84e5de5248339c9675e4eac7fd7fee6ebb5602df# ps -aux | grep 1733194root 1733194 0.0 0.1 4108 3296 pts/0 Ss+ 19:03 0:00 /bin/bashubuntu 1737657 0.0 0.0 6432 736 pts/0 S+ 19:11 0:00 grep --color=auto --exclude-dir=.bzr --exclude-dir=CVS --exclude-dir=.git --exclude-dir=.hg --exclude-dir=.svn --exclude-dir=.idea --exclude-dir=.tox 1733194 可以看到这个 1733194 进程就是启动 容器时的 /bin/bash 进程 由于其他的 cgroup子系统我们没有对刚刚启动的 container 进行限制，所以也理论上从 cgroup上看到的也是没有限制的，可以看看 cpu controller. 123456789tencent_clould@ubuntu: /sys/fs/cgroup/cpu/docker# cat taskstencent_clould@ubuntu: /sys/fs/cgroup/cpu/docker# cd -/sys/fs/cgroup/cpu/docker/b3c618bb4df96362b371860f84e5de5248339c9675e4eac7fd7fee6ebb5602dftencent_clould@ubuntu: /sys/fs/cgroup/cpu/docker/b3c618bb4df96362b371860f84e5de5248339c9675e4eac7fd7fee6ebb5602df# cat tasks1733194tencent_clould@ubuntu: /sys/fs/cgroup/cpu/docker/b3c618bb4df96362b371860f84e5de5248339c9675e4eac7fd7fee6ebb5602df# cat cpu.cfs_period_us100000tencent_clould@ubuntu: /sys/fs/cgroup/cpu/docker/b3c618bb4df96362b371860f84e5de5248339c9675e4eac7fd7fee6ebb5602df# cat cpu.cfs_quota_us-1 这里就不具体介绍 各个子系统目录下文件含义了。","link":"/2020/12/12/Docker%E6%9C%BA%E5%88%B6%E5%88%86%E6%9E%90/docker%E4%B8%8Enamespace%E5%A6%82%E4%BD%95%E8%81%94%E7%B3%BB%E8%B5%B7%E6%9D%A5%E7%9A%84/"},{"title":"docker如何使用","text":"这是一个简单概括docker 安装，拉取image,docker 命令使用的 记录。 在ubuntu平台上安装 docker 1sudo apt install docker 拉取ubuntu的 image 1sudo docker pull ubuntu 基于ubuntu的 image 创建并运行一个docker，限制4MB内存，开启 bash 1sudo docker run -t -i -m 4MB ubuntu /bin/bash 查看当前有哪些运行的docker 123tencent_clould@ubuntu: ~/workspace# sudo docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES1f3a24f91059 ubuntu &quot;/bin/bash&quot; 47 hours ago Up 47 hours hhhh 查看当前有哪些的docker(运行中 + 非运行中) 123tencent_clould@ubuntu: ~/workspace# sudo docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES1f3a24f91059 ubuntu &quot;/bin/bash&quot; 47 hours ago Up 47 hours hhhhh 给 hhh 的容器重命名为 my_ubuntu 1sudo docker rename hhh my_ubuntu 停止 my_ubuntu 容器 1sudo docker stop my_ubuntu 重启开启 my_ubuntu 容器 1sudo docker start my_ubuntu 进入运行中的 my_ubuntu 容器的 bash 1sudo docker exec -ti my_ubuntu /bin/bash 其他常用命令想起来再记录 这些命令还是比较长的，可以缩写到 bashrc 的alias里面，比较方便 12345678tencent_clould@ubuntu: ~/workspace# cat ~/.zshrc | grep &quot;alias d&quot;alias dn=&quot;sudo docker run -t -i -m 128MB ubuntu /bin/bash&quot;alias ds=&quot;sudo docker start my_ubuntu&quot;alias dk=&quot;sudo docker stop my_ubuntu&quot;alias di=&quot;sudo docker exec -ti my_ubuntu /bin/bash&quot;alias dps=&quot;sudo docker ps&quot;alias dpsa=&quot;sudo docker ps -a&quot;alias drma=&quot;sudo docker container prune&quot;","link":"/2020/12/01/Docker%E6%9C%BA%E5%88%B6%E5%88%86%E6%9E%90/docker%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8/"},{"title":"x86 平台常用寄存器和函数调用分析","text":"后续填坑。。","link":"/2021/01/23/crash%E4%B8%93%E9%A2%98/x86%20%E5%B9%B3%E5%8F%B0%E5%B8%B8%E7%94%A8%E5%AF%84%E5%AD%98%E5%99%A8%E5%92%8C%E5%87%BD%E6%95%B0%E8%B0%83%E7%94%A8%E5%88%86%E6%9E%90/"},{"title":"最简单的空指针oops","text":"只是做一个记录，为了演示最简单的空指针case, 写了一个demo, 可以参考github 代码 用 crash 分析insmod 出错之后已经生成了相关 dump文件。下面直接使用 crash 工具分析： 12345678910111213141516171819202122232425262728293031stable_kernel@kernel: /var/crash/202101211201# sudo crash vmlinux dumpcrash 7.2.9++GNU gdb (GDB) 7.6Copyright (C) 2013 Free Software Foundation, Inc.License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;This is free software: you are free to change and redistribute it.There is NO WARRANTY, to the extent permitted by law. Type &quot;show copying&quot;and &quot;show warranty&quot; for details.This GDB was configured as &quot;x86_64-unknown-linux-gnu&quot;...WARNING: kernel relocated [704MB]: patching 137170 gdb minimal_symbol values KERNEL: vmlinux DUMPFILE: dump.202101211201 [PARTIAL DUMP] CPUS: 4 DATE: Thu Jan 21 12:00:56 CST 2021 UPTIME: 00:12:53LOAD AVERAGE: 0.16, 0.11, 0.10 TASKS: 454 NODENAME: rlk-Standard-PC-i440FX-PIIX-1996 RELEASE: 5.11.0-rc4+ VERSION: #5 SMP Wed Jan 20 20:41:47 CST 2021 MACHINE: x86_64 (3692 Mhz) MEMORY: 2 GB PANIC: &quot;Oops: 0002 [#1] SMP NOPTI&quot; (check log for details) PID: 3605 COMMAND: &quot;krace_thread&quot; TASK: ffffa1b006754b40 [THREAD_INFO: ffffa1b006754b40] CPU: 2 STATE: TASK_RUNNING (PANIC)crash&gt; 可以看到 发生问题的 kernel 版本是 5.11.0-rc4+，编译时间是 #5 SMP Wed Jan 20 20:41:47 CST 2021，内存大小是 2G，出问题时刻的负载是0.16, 0.11, 0.10 PANIC 原因是&quot;Oops: 0002 [#1] SMP NOPTI&quot; (check log for details)，CPU:2 上的TASK（krace_thread-3605）: ffffa1b006754b40发生了 oops，具体原因需要看 日志来得到。 bt 查看出问题的taskcrash 运行之后默认的task是出问题的task，可以通过 set 查看 123456crash&gt; set PID: 3605COMMAND: &quot;krace_thread&quot; TASK: ffffa1b006754b40 [THREAD_INFO: ffffa1b006754b40] CPU: 2 STATE: TASK_RUNNING (PANIC) bt 可以查看当前追踪的task的 backtrace 12345678910111213crash&gt; btPID: 3605 TASK: ffffa1b006754b40 CPU: 2 COMMAND: &quot;krace_thread&quot; #0 [ffffbbbf004ebc40] machine_kexec at ffffffffad04d87c #1 [ffffbbbf004ebc88] __crash_kexec at ffffffffad1283b8 #2 [ffffbbbf004ebd50] crash_kexec at ffffffffad1290d0 #3 [ffffbbbf004ebd60] oops_end at ffffffffad021d75 #4 [ffffbbbf004ebd80] no_context at ffffffffad0570e0 #5 [ffffbbbf004ebdf0] __bad_area_nosemaphore at ffffffffad0572c7 #6 [ffffbbbf004ebe38] exc_page_fault at ffffffffadd16b67 #7 [ffffbbbf004ebe60] asm_exc_page_fault at ffffffffade00ace #8 [ffffbbbf004ebee8] create_oops at ffffffffc0371027 [01_null_pointer] #9 [ffffbbbf004ebf10] kthread at ffffffffad0930da#10 [ffffbbbf004ebf50] ret_from_fork at ffffffffad001ae2 bt -c 1： 可以查看 cpu:1 上当前运行的线程的backtracebt -a ： 可以查看 当前所有 cpu上运行的线程的backtrace 这个case 十分显然，是 create_oops 这里出现了问题。 dis 查看bug地址dis 是 disassemble 反汇编的缩写，可以 查看出问题 text 地址内容 某个函数 symbol 符号内容 某个函数 symbol 符号 + 偏移的内容 某个符号 或者 text 与 代码行显示在一起（如果是module 中crash需要加载 module.ko） 123456789101112131415161718crash&gt; dis ffffffffc03710270xffffffffc0371027 &lt;create_oops+39&gt;: movl $0x0,0x0crash&gt;crash&gt; dis create_oops0xffffffffc0371000 &lt;create_oops&gt;: mov $0x1388,%edi0xffffffffc0371005 &lt;create_oops+5&gt;: callq 0xffffffffad104b80 &lt;msleep&gt;0xffffffffc037100a &lt;create_oops+10&gt;: mov $0xffffffffc037203c,%rdi0xffffffffc0371011 &lt;create_oops+17&gt;: callq 0xffffffffadcc96da &lt;printk&gt;0xffffffffc0371016 &lt;create_oops+22&gt;: mov $0x1388,%edi0xffffffffc037101b &lt;create_oops+27&gt;: callq 0xffffffffad104b80 &lt;msleep&gt;0xffffffffc0371020 &lt;create_oops+32&gt;: mov $0xffffffffc037204f,%rdi0xffffffffc0371027 &lt;create_oops+39&gt;: movl $0x0,0x00xffffffffc0371032 &lt;create_oops+50&gt;: callq 0xffffffffadcc96da &lt;printk&gt;0xffffffffc0371037 &lt;create_oops+55&gt;: xor %eax,%eax0xffffffffc0371039 &lt;create_oops+57&gt;: retqcrash&gt; dis create_oops+390xffffffffc0371027 &lt;create_oops+39&gt;: movl $0x0,0x0crash&gt; 直接可以看出问题是，将立即数$0x0 赋值到 地址0x0中，所以直接 oops了 10xffffffffc0371027 &lt;create_oops+39&gt;: movl $0x0,0x0 但是在哪一行呢，这就需要加载 ko文件了 12345678910111213crash&gt; lsmod MODULE NAME SIZE OBJECT FILEffffffffc0373000 01_null_pointer 16384 (not loaded) [CONFIG_KALLSYMS]crash&gt;crash&gt;crash&gt; mod -s 01_null_pointer /tmp/share/test_modules/null_pointer/01_null_pointer/01_null_pointer.ko MODULE NAME SIZE OBJECT FILEffffffffc0373000 01_null_pointer 16384 /tmp/share/test_modules/null_pointer/01_null_pointer/01_null_pointer.kocrash&gt;crash&gt; lsmod MODULE NAME SIZE OBJECT FILEffffffffc0373000 01_null_pointer 16384 /tmp/share/test_modules/null_pointer/01_null_pointer/01_null_pointer.kocrash&gt; 加载 ko文件之后，直接 dis -l 反汇编 出问题的函数 123456789101112131415161718crash&gt; dis -l create_oops/home/ubuntu/workspace/share/test_modules/null_pointer/01_null_pointer/01_null_pointer.c: 120xffffffffc0371000 &lt;create_oops&gt;: mov $0x1388,%edi0xffffffffc0371005 &lt;create_oops+5&gt;: callq 0xffffffffad104b80 &lt;msleep&gt;/home/ubuntu/workspace/share/test_modules/null_pointer/01_null_pointer/01_null_pointer.c: 130xffffffffc037100a &lt;create_oops+10&gt;: mov $0xffffffffc037203c,%rdi0xffffffffc0371011 &lt;create_oops+17&gt;: callq 0xffffffffadcc96da &lt;printk&gt;/home/ubuntu/workspace/share/test_modules/null_pointer/01_null_pointer/01_null_pointer.c: 140xffffffffc0371016 &lt;create_oops+22&gt;: mov $0x1388,%edi0xffffffffc037101b &lt;create_oops+27&gt;: callq 0xffffffffad104b80 &lt;msleep&gt;/home/ubuntu/workspace/share/test_modules/null_pointer/01_null_pointer/01_null_pointer.c: 160xffffffffc0371020 &lt;create_oops+32&gt;: mov $0xffffffffc037204f,%rdi0xffffffffc0371027 &lt;create_oops+39&gt;: movl $0x0,0x0/home/ubuntu/workspace/share/test_modules/null_pointer/01_null_pointer/01_null_pointer.c: 170xffffffffc0371032 &lt;create_oops+50&gt;: callq 0xffffffffadcc96da &lt;printk&gt;/home/ubuntu/workspace/share/test_modules/null_pointer/01_null_pointer/01_null_pointer.c: 180xffffffffc0371037 &lt;create_oops+55&gt;: xor %eax,%eax0xffffffffc0371039 &lt;create_oops+57&gt;: retq 直接定位到 12301_null_pointer.c: 160xffffffffc0371020 &lt;create_oops+32&gt;: mov $0xffffffffc037204f,%rdi0xffffffffc0371027 &lt;create_oops+39&gt;: movl $0x0,0x0 代码中看看 1*(int *)0 = 0; 问题很快解决了。 试着查看x86 如何调用函数传参的123/home/ubuntu/workspace/share/test_modules/null_pointer/01_null_pointer/01_null_pointer.c: 120xffffffffc0371000 &lt;create_oops&gt;: mov $0x1388,%edi0xffffffffc0371005 &lt;create_oops+5&gt;: callq 0xffffffffad104b80 &lt;msleep&gt; 对应代码是，0x1388 就是十六进制的 5000 1msleep(5000); 是不是第一个整形参数是存在 edi 寄存器中的 123/home/ubuntu/workspace/share/test_modules/null_pointer/01_null_pointer/01_null_pointer.c: 130xffffffffc037100a &lt;create_oops+10&gt;: mov $0xffffffffc037203c,%rdi0xffffffffc0371011 &lt;create_oops+17&gt;: callq 0xffffffffadcc96da &lt;printk&gt; 对应的代码是 1printk(&quot;create_oops start\\n&quot;); 0xffffffffc037203c 是啥呢？可以使用 rd 命令读取一下，原来是字符串的起始的地址 1234crash&gt; rd 0xffffffffc037203c 4ffffffffc037203c: 6f5f657461657263 726174732073706f create_oops starffffffffc037204c: 7461657263000a74 652073706f6f5f65 t..create_oops ecrash&gt; 是不是 第一个地址型参数是存放在 rdi 中的呢？ 后面会用不同个数参数，不同类型参数的函数 crash，来实验一下这个是不是对～ 找到一篇讲解x86-64寄存器和函数调用的文章，上面说的猜想就是扯淡。。","link":"/2021/01/21/crash%E4%B8%93%E9%A2%98/%E6%9C%80%E7%AE%80%E5%8D%95%E7%9A%84%E7%A9%BA%E6%8C%87%E9%92%88oops/"},{"title":"page cache如何产生的","text":"WHY Page cache?CPU如果要访问外部磁盘上的文件，需要首先将这些文件的内容拷贝到内存中，由于硬件的限制，从磁盘到内存的数据传输速度是很慢的，如果现在物理内存有空余，干嘛不用这些空闲内存来缓存一些磁盘的文件内容呢，这部分用作缓存磁盘文件的内存就叫做page cache。 用户进程启动read()系统调用后，内核会首先查看page cache里有没有用户要读取的文件内容，如果有（cache hit），那就直接读取，没有的话（cache miss）再启动I/O操作从磁盘上读取，然后放到page cache中，下次再访问这部分内容的时候，就又可以cache hit，不用忍受磁盘的龟速了（相比内存慢几个数量级）。 由此可见 page_cache 会对 磁盘性能，应用性能有极大提高 但是相对于磁盘，内存的容量还是很有限的，所以没必要缓存整个文件，只需要当文件的某部分内容真正被访问到时，再将这部分内容调入内存缓存起来就可以了，这种方式叫做demand paging（按需调页），把对需求的满足延迟到最后一刻，很懒很实用。 Page cache 组成filePage cache 是文件部分或者全部在内核中缓存的部分，首先需要了解 文件在磁盘和linux中的表现: 在磁盘等存储介质上，文件都是分块存储在磁盘上的， 磁盘inode 是文件唯一标识。 linux系统中为了表示文件，也有文件系统inode，一般会跟文件系统相关，是 从物理磁盘 inode 读取到内存之后的形态 linux系统中虚拟文件系统VFS以实现多文件系统支持，vfs inode是VFS层文件内存数据结构，大多数是所有 文件系统 inode 公共成员 ext4 fs for example: 12345678910111213/* * fourth extended file system inode data in memory */struct ext4_inode_info { __le32 i_data[15]; /* unconverted */ __u32 i_dtime; ext4_fsblk_t i_file_acl; struct rw_semaphore xattr_sem; struct list_head i_orphan; /* unlinked but open inodes */ struct rw_semaphore i_data_sem; struct inode vfs_inode; struct jbd2_inode *jinode; ext4_inode_info 就是对应文件系统inode, vfs_inode 就是 vfs层的inode. 123456789101112131415/* * Structure of an inode on the disk */struct ext4_inode { __le16 i_mode; /* File mode */ __le16 i_uid; /* Low 16 bits of Owner Uid */ __le32 i_size_lo; /* Size in bytes */ __le32 i_atime; /* Access time */ __le32 i_ctime; /* Inode Change time */ __le32 i_mtime; /* Modification time */ __le32 i_dtime; /* Deletion Time */ __le16 i_gid; /* Low 16 bits of Group Id */ __le16 i_links_count; /* Links count */ __le32 i_blocks_lo; /* Blocks count */ ..... ext4_inode 就是对应 物理磁盘的inode. 大多数成员是 记录和物理磁盘 和 物理文件真实 相关的信息 address_space实际情况中，一个文件可能有 100M - 10G这么大，kernel会给文件在内存中分配很多page cache,这些pagecache是如何管理起来的呢，这就引出了 第二个主要结构 address_space – 地址空间。 首先看 address_space 定义 12345678910111213141516171819202122232425262728293031323334353637383940/** * struct address_space - Contents of a cacheable, mappable object. * @host: Owner, either the inode or the block_device. * @i_pages: Cached pages. * @gfp_mask: Memory allocation flags to use for allocating pages. * @i_mmap_writable: Number of VM_SHARED mappings. * @nr_thps: Number of THPs in the pagecache (non-shmem only). * @i_mmap: Tree of private and shared mappings. * @i_mmap_rwsem: Protects @i_mmap and @i_mmap_writable. * @nrpages: Number of page entries, protected by the i_pages lock. * @nrexceptional: Shadow or DAX entries, protected by the i_pages lock. * @writeback_index: Writeback starts here. * @a_ops: Methods. * @flags: Error bits and flags (AS_*). * @wb_err: The most recent error which has occurred. * @private_lock: For use by the owner of the address_space. * @private_list: For use by the owner of the address_space. * @private_data: For use by the owner of the address_space. */struct address_space { struct inode *host; //一般就是 inode 与 文件关联 struct xarray i_pages; // xarray 管理着这个地址空间里面所有的 page,之前kernel版本是 radix tree gfp_t gfp_mask; atomic_t i_mmap_writable;#ifdef CONFIG_READ_ONLY_THP_FOR_FS /* number of thp, only for non-shmem files */ atomic_t nr_thps;#endif struct rb_root_cached i_mmap; // i_mmap 红黑树的根节点，会将 page 按照 某种？ 序列组织起来，便于查找 struct rw_semaphore i_mmap_rwsem; unsigned long nrpages; unsigned long nrexceptional; pgoff_t writeback_index; const struct address_space_operations *a_ops; unsigned long flags; errseq_t wb_err; spinlock_t private_lock; struct list_head private_list; void *private_data;} __attribute__((aligned(sizeof(long)))) __randomize_layout; 这个定义没有 inode 那么长，但是很核心 这样 inode 与 many pages 通过 address_space 的 host 与 i_pages 成员 相互连接起来。 其中 inode 既可以是 磁盘文件的 inode，也可以是 内存文件系统 的 inode（proc sys等）还可以是 swap 文件的 inode. a_ops 同样也是一个基类指针，定义了抽象的文件系统交互接口，由具体文件系统负责实现。例如如果文件是存储在ext4文件系统之上，那么该结构便被初始化为 ext4_aops （见fs/ext4/inode.c）。 如何查找 一个文件的page_cache?inode –&gt; address_space: container_of 通过inode 找到 地址空间address_space –&gt; i_pages: 成员变量访问 address_space 是Linux内核中的一个关键抽象，它是页缓存和外部设备中文件系统的桥梁。 上层应用读取数据会进入到该结构内的page cache，上层应用对文件的写入内容也会缓存于该结构内的page cache。 $$ 这里配图 dentry这和 pagecache 关系没有那密切。 假如需要查找 /etc/apt/aaa 这个文件，linux系统会如何去查找呢？文件目录这些信息就涉及到 dentry 的信息了，dentry也是实现Linux文件系统目录层次结构的关键. dentry 从另外一个层面描述文件：文件名. 更准确地说，是保存文件名和文件inode号 与 inode 一样， dentry 除了VFS层dentry结构，每种具体文件系统也有自身的内存dentry和 磁盘dentry结构 ext4 fs for example: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647struct dentry { /* RCU lookup touched fields */ unsigned int d_flags; /* protected by d_lock */ seqcount_spinlock_t d_seq; /* per dentry seqlock */ struct hlist_bl_node d_hash; /* lookup hash list */ struct dentry *d_parent; /* parent directory */ struct qstr d_name; struct inode *d_inode; /* Where the name belongs to - NULL is * negative */ unsigned char d_iname[DNAME_INLINE_LEN]; /* small names */ /* Ref lookup also touches following */ struct lockref d_lockref; /* per-dentry lock and refcount */ const struct dentry_operations *d_op; struct super_block *d_sb; /* The root of the dentry tree */ unsigned long d_time; /* used by d_revalidate */ void *d_fsdata; /* fs-specific data */ union { struct list_head d_lru; /* LRU list */ wait_queue_head_t *d_wait; /* in-lookup ones only */ }; struct list_head d_child; /* child of parent list */ struct list_head d_subdirs; /* our children */ /* * d_alias and d_rcu can share memory */ union { struct hlist_node d_alias; /* inode alias list */ struct hlist_bl_node d_in_lookup_hash; /* only for in-lookup ones */ struct rcu_head d_rcu; } d_u;} __randomize_layout;/* * The new version of the directory entry. Since EXT4 structures are * stored in intel byte order, and the name_len field could never be * bigger than 255 chars, it's safe to reclaim the extra byte for the * file_type field. */struct ext4_dir_entry_2 { __le32 inode; /* Inode number */ __le16 rec_len; /* Directory entry length */ __u8 name_len; /* Name length */ __u8 file_type; /* See file type macros EXT4_FT_* below */ char name[EXT4_NAME_LEN]; /* File name */}; 在ext4中内存与磁盘dentry结构 ext4_dir_entry_2 看起来保持了一致。磁盘文件系统dentry也必须被持久化存储在磁盘上。","link":"/2020/09/07/page_cache/page%20cache%E4%BA%A7%E7%94%9F/"},{"title":"我的服务器配置","text":"服务器配置我之前一直是使用 笔记本windows+ubuntu虚拟机 的开发配置，但是这样有几个限制： ubuntu 虚拟机比较耗费资源，一到夏天笔记本风扇狂转，影响体验 只在本地使用没有问题，但是有时候还想在公司或者出差的时候使用 基于以上种种限制，我就搞了腾讯云的服务器，1H2G1M3Y 价格好像是 300￥，还是比较便宜的。 但是直到前些天才真正用起来，主要用于编译和测试，但是一直有几个缺点： 云服务器基本都是基于 KVM的，然后他还禁止了租户继续使用KVM进行虚拟化 云服务器禁止了 邮件服务的端口，导致无法收发邮件 云服务器在低配置（1核心2G）下，费用较低，一旦配置升高，价格急剧上升，性价比不高 1核2G用来编译效率太低 云服务器带宽资源比较昂贵，在本地连接云服务器的时候经常会需要连接好久才能连接上 考虑最近AMD cpu很强势，便配置了一个AMD 4650G的ITX机器，用作我自己的服务器。 物理服务器安装我直接选择了 ubuntu20.04.1 版本，通过UltraISO 工具将iso写入SD卡中，然后开启U盘启动，安装ubuntu。还需要通过 主板 BIOS开启 KVM，通过CPU设置开启 SVM:如果不开启KVM直接用qemu开启 ubuntu虚拟机，效率比开启KVM虚拟机要慢 50倍左右，这也是我不想用 云服务器的原因。换用清华源 物理服务器安装qemu-ubuntu虚拟机可以参考文章:qemu起ubuntu server 在云服务器上按着文章成功配置了qemu虚拟机，但是在服务器上一直是失败的，原因就是起了服务器之后一直无法登陆。所以最后我用了桌面版本ubuntu 20.04.1，而不是 ubuntu server。 下载 ubuntu iso 12wget https://mirrors.tuna.tsinghua.edu.cn/ubuntu-releases/20.04.1/ubuntu-20.04.1-desktop-amd64.isomv ubuntu-20.04.1-desktop-amd64.iso ubuntu.iso 创建磁盘 1qemu-img create -q -f qcow2 stable_ubuntu.img 32G 安装虚拟机这一步尽量不要用 ssh做，直接在物理机器上搞比较快，第一次安装需要 -cdrom指定iso文件位置，后面就不需要了。在安装的之后需要指定 用户名密码。 12安装sudo qemu-system-x86_64 ubuntu.img -m 1024 -cdrom ubuntu.iso --enable-kvm 启动虚拟机启动虚拟机分为两种:a. 直接启动原来内核的虚拟机(可以直接联网，下载软件)也可以ssh链接： ssh -v rlk@127.0.0.1 -p 2222 12345678910sudo qemu-system-x86_64 \\ -hda /home/ubuntu/myspace/qemu_build/stable_ubuntu.img \\ -smp 4 \\ -m 4096 \\ --enable-kvm \\ -net nic \\ -net user,hostfwd=tcp::2222-:22 \\ --nographic \\ -fsdev local,id=fs1,path=/home/ubuntu/workspace/share,security_model=none \\ -device virtio-9p-pci,fsdev=fs1,mount_tag=host_share b. 用新kernel代替 stable_ubuntu.img中 的 kernel(替代内核) 123456789101112sudo qemu-system-x86_64 \\ -kernel /home/ubuntu/workspace/share/stable/bzImage \\ -hda /home/ubuntu/myspace/qemu_build/stable_ubuntu.img \\ -append &quot;root=/dev/sda5 console=ttyS0&quot; \\ -smp 4 \\ -m 4096 \\ --enable-kvm \\ -net nic \\ -net user,hostfwd=tcp::2222-:22 \\ --nographic \\ -fsdev local,id=fs1,path=/home/ubuntu/workspace/share,security_model=none \\ -device virtio-9p-pci,fsdev=fs1,mount_tag=host_share 至于开机启动都是 /etc/rc.local 里面配置https://gitee.com/shshshsh/cloud_server_shell test 相关a. ltp（linux kernel test project）：linux内核测试 project 编译安装，运行https://github.com/linux-test-project/ltp b. mmetst 可以测试 linux kernel 不同版本之间性能差异https://github.com/gormanm/mmtestshttps://lwn.net/Articles/820823/https://lwn.net/Articles/463339/https://blog.csdn.net/Linux_Everything/article/details/106485101 c. lkp （linux kernel performance）:是 intel发起的一个kernel performance test项目https://github.com/fengguang/lkp-https://01.org/lkphttps://01.org/blogs/jdu1/2017/lkp-tests-linux-kernel-performance-test-and-analysis-toolhttps://github.com/sammcj/kernel-cihttp://hejq.me/2014/10/30/lkp-tests-notes/https://libraries.io/github/openthos/lkp-analysis 学习kvmhttps://www.linux-kvm.org/page/Kvmtoolshttps://blog.csdn.net/leoufung/article/details/48781119git://git.kernel.org/pub/scm/linux/kernel/git/will/kvmtool.git 远程访问本地服务器由于本地服务器其实是没有固定ip的，所以需要一台固定ip的服务器做中转，这样才能从任何地方远程访问到我的物理服务器，正好有一个腾讯云服务器，就充当了这个角色。 我使用的是 nps 这个开源代理工具，相比于其他工具来说，配置简单，直接在网页上就可以配置参考超好用轻量级NPS内网穿透 NPS问题 稳定性：这个我其实已经使用了超过1个月了，机器都没重启过，服务都还正常，对于个人使用来说完全足够了 带宽由于我的腾讯云服务器带宽了 1Mbps,其实峰值带宽也就 128Kb/s。 我使用服务器主要场景是 1231. 有编译任务2. ssh 到服务器3. vscode ssh 到服务器看代码，写代码 其实 1 和 2对 带宽要求都很低，但是 vscode ssh 对于带宽要求还是比较高的，从腾讯云后台监控数据上看，我的服务器 每次都是 vscode ssh 到腾讯云的时候或者服务器的时候带宽都用满了，导致有时候ssh需要连 几min 才能连接上。 vscode ssh 在连接上之后对于带宽使用率其实很低，在连接瞬间有较高要求。 我无奈之下就想给 腾讯云的服务搞成按流量计费的，一通操作下来没想到腾讯云居然不支持活动时候买的服务器改网络配置。。 那就只能升级到2M带宽 或者更高了，升级了一下到 2Mbps,然后再用 vscode shh连接 就很丝滑了，然后我就想能不能通过调整vscode ssh配置来达到这样目的的，发现好像还真有个 timeout 时间，默认 15s，我改到了30s，也可能如果不升级带宽,仅仅将配置改到30s vscode经常超时的问题就好了呢？ 等到5月份之后不续费再继续看看情况。。。","link":"/2020/12/19/qemu/%E6%88%91%E7%9A%84%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AEqemu/"},{"title":"mmap_sem可扩展问题","text":"在 Linux 中，同进程的不同线程虽然对应不同的 task_struct ，但都共享同一个 mm_struct ，即 task_struct::mm 指向同一个变量，其描述了该进程的整个虚拟地址空间。 mm_struct 包含的主要成员如下： 1234567struct mm_struct { struct vm_area_struct *mmap; /* list of VMAs */ struct rb_root mm_rb; pgd_t * pgd; struct rw_semaphore mmap_sem;...}; 其中， mmap 指向了 vm_area_struct 双向链表，每个 vm_area_struct (VMA) 描述了虚拟地址空间的一个区间，比如 text / data / bss / heap / stack 各对应一个 VMA ，mmap 每次调用会产生 VMA(如果可和之前合并的话则不会产生新 VMA)。同时为了加速 VMA 查找，vm_area_struct 之间亦通过红黑树串起来，通过 mm_rb 指向。 接下来就是大家都很熟悉的页表，在 x86_64 下，分页采用 PGD - PUD - PMD - PTE 四级页表，在此通过 pgd 指向 PGD 页表项。 mmap_sem 是一把很大的锁 (信号量)，进程内大部分的内存操作都需要拿它： 所有对 VMA 的操作 (mmap、munmap)所有对页表的修改 (page fault 等)madvise (如 jemalloc 经常使用的 MADV_DONTNEED) Problem某业务进程包含若干个工作线程和一个数据加载线程，每隔一段时间数据加载线程会映射一份新的数据到内存供工作线程使用，并负责将工作线程不再使用的上一份数据释放。结果发现在释放期间，工作线程受到了影响，导致失败率上升。通过一波分析后发现是 mummap 大块内存 (20G+) 造成的。 我们来看内核中 munmap 的实现： 123456789101112SYSCALL_DEFINE2(munmap, unsigned long, addr, size_t, len){ int ret; struct mm_struct *mm = current-&gt;mm; profile_munmap(addr); if (down_write_killable(&amp;mm-&gt;mmap_sem)) return -EINTR; ret = do_munmap(mm, addr, len); up_write(&amp;mm-&gt;mmap_sem); return ret;} 可以发现刚进来就把 mmap_sem 的写锁拿上了，然后执行 do_munmap ： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384int do_munmap(struct mm_struct *mm, unsigned long start, size_t len){ unsigned long end; struct vm_area_struct *vma, *prev, *last; if ((offset_in_page(start)) || start &gt; TASK_SIZE || len &gt; TASK_SIZE-start) return -EINVAL; len = PAGE_ALIGN(len); if (len == 0) return -EINVAL; /* Find the first overlapping VMA */ vma = find_vma(mm, start); if (!vma) return 0; prev = vma-&gt;vm_prev; /* we have start &lt; vma-&gt;vm_end */ /* if it doesn't overlap, we have nothing.. */ end = start + len; if (vma-&gt;vm_start &gt;= end) return 0; /* * If we need to split any vma, do it now to save pain later. * * Note: mremap's move_vma VM_ACCOUNT handling assumes a partially * unmapped vm_area_struct will remain in use: so lower split_vma * places tmp vma above, and higher split_vma places tmp vma below. */ if (start &gt; vma-&gt;vm_start) { int error; /* * Make sure that map_count on return from munmap() will * not exceed its limit; but let map_count go just above * its limit temporarily, to help free resources as expected. */ if (end &lt; vma-&gt;vm_end &amp;&amp; mm-&gt;map_count &gt;= sysctl_max_map_count) return -ENOMEM; error = __split_vma(mm, vma, start, 0); if (error) return error; prev = vma; } /* Does it split the last one? */ last = find_vma(mm, end); if (last &amp;&amp; end &gt; last-&gt;vm_start) { int error = __split_vma(mm, last, end, 1); if (error) return error; } vma = prev ? prev-&gt;vm_next : mm-&gt;mmap; /* * unlock any mlock()ed ranges before detaching vmas */ if (mm-&gt;locked_vm) { struct vm_area_struct *tmp = vma; while (tmp &amp;&amp; tmp-&gt;vm_start &lt; end) { if (tmp-&gt;vm_flags &amp; VM_LOCKED) { mm-&gt;locked_vm -= vma_pages(tmp); munlock_vma_pages_all(tmp); } tmp = tmp-&gt;vm_next; } } /* * Remove the vma's, and unmap the actual pages */ detach_vmas_to_be_unmapped(mm, vma, prev, end); unmap_region(mm, vma, prev, start, end); arch_unmap(mm, vma, start, end); /* Fix up all other VM information */ remove_vma_list(mm, vma); return 0;} 个函数做了很多事：首先通过 find_vma 找到 start 到 end 之间相应的 VMA ，必要时通过 __split_vma 对 VMA 进行切分。随后将 VMA 从链表和红黑树中移除，并对移除的区域调用 unmap_region =&gt; free_pgtables 清空该区域对应的页表项。 对于 mummap 大块内存 (20G+) 这样的操作，其中最耗时的应该是 free_pgtables ：20G 内存对应 20 * 1024 * 1024 / 4 = 5242880 个 4KB 页，要清空这 5242880 个页的页表项无疑是非常耗时的，实测需要好几秒。在此期间，进程的 mmap_sem 的写锁一直被拿住，导致其他线程的对内存的相关操作阻塞在 mmap_sem 锁上，从而导致了性能抖动。 为了解决这个问题，同事实现了分段 unmap ：在业务程序对 munmap 再进行一层封装，当遇到大块内存释放时，将内存切分成若干段，比如 128MB 一段，依次对每段调用 munmap ，并在调用后 sleep 若干毫秒，这样避免了长时间拿 mmap_sem ，卡住其他工作进程导致影响服务质量的问题。和 Yang Shi 在 Drop mmap_sem during unmapping large map 提出的内核态实现有异曲同工之妙。 总结从用户的角度来看，一个线程不应该受到另外一个线程的干扰。然而从这次的 case 来看，多个线程对进程级共享的 mmap_sem 的竞争是问题产生的主要原因，换句话说，mmap_sem 管太多，导致干啥都要拿它，从而限制了进程的可扩展性。 能否换用更细粒度的锁？Laurent DUFOUR 在 LPC2019 上提出用 VMA 层级的锁来取代 mmap_sem ，这样在上述的 case 中数据加载线程只会拿住 20G 内存对应的 VMA 锁，而不会影响到早已不使用这块内存的其他工作线程。当然，换用更细粒度的锁带来的就是内核实现上复杂度的提升，比如需要遵循一些规则来避免死锁。 参考：mmap_sem 的可扩展性LWN文章PDF文档","link":"/2020/09/14/%E5%B9%B6%E5%8F%91%E9%97%AE%E9%A2%98/mmap_sem%E5%8F%AF%E6%89%A9%E5%B1%95%E9%97%AE%E9%A2%98/"},{"title":"per-cpu变量引起的思考","text":"疑问 内核为什么要引入per-cpu变量？解决了什么问题？ per-cpu变量如何实现的？ 是否可以用数组实现per-cpu变量？ per-cpu变量访问有什么原则？ 如果一个per-cpu变量既可以在线程上下文中访问，又可以在中断上下文中访问，需要保护吗？ 接下来我们就带着以上的疑问继续往下看 内核为什么要引入per-cpu变量？解决了什么问题？per-cpu变量是Linux内核中的一种同步机制。 当系统中的所有CPU访问并共享变量V时，CPU0修改变量V的值。 CPU1也会同时修改变量V，这将导致变量V的值不正确。如果使用了原子锁，CPU0只能等待修改。 这种方式有两个缺点： （1）原子操作很耗时 （2）现在，所有CPU都具有L1高速缓存，因此许多CPU同时访问变量将导致高速缓存一致性问题。 当CPU修改共享变量V时，其他CPU上的相应缓存行必须无效，这会导致性能损失。 per-cpu变量提供了解决上述问题的有趣功能。 它将变量的副本分配给系统中的每个处理器。 在多处理器系统中，当处理器只能访问其所属变量的副本时，无需考虑与其他处理器的竞争，因此可以充分利用处理器的本地硬件缓存来提高性能。 。 per-cpu变量如何实现的？这里分 静态per-cpu变量 和 动态per-cpu变量首先看一下使用的 API 123456789101112// 静态定义 include/linux/Percpu-def.h#define DECLARE_PER_CPU(type, name) \\ DECLARE_PER_CPU_SECTION(type, name, &quot;&quot;)#define DEFINE_PER_CPU(type, name) \\ DEFINE_PER_CPU_SECTION(type, name, &quot;&quot;)// 动态定义 include/linux/percpu.h#define alloc_percpu(type) \\ (typeof(type) __percpu *)__alloc_percpu(sizeof(type), __alignof__(type))void free_percpu(void __percpu *__pdata);","link":"/2020/09/05/%E5%B9%B6%E5%8F%91%E9%97%AE%E9%A2%98/percpu%E5%8F%98%E9%87%8F%E5%BC%95%E8%B5%B7%E7%9A%84%E6%80%9D%E8%80%83/"},{"title":"softirq何时会被执行","text":"很多人可能都知道中断irq，但是对软中断softfirq却比较陌生，软中断这个概念是纯软件意义上的，与中断依赖于硬件行为不一样。在linux中，软中断主要用于执行irq中没有执行，但又不是很紧急的事情，现在linux内核中为每个CPU都分配了一个线程 [ksoftirqd/n]，用来执行软中断。 12345sh@ubuntu[root]:/sys/kernel/debug/tracing# ps -aux |grep softroot 10 0.0 0.0 0 0 ? S 9月19 0:19 [ksoftirqd/0]root 18 0.0 0.0 0 0 ? S 9月19 0:17 [ksoftirqd/1]root 24 0.0 0.0 0 0 ? S 9月19 0:18 [ksoftirqd/2]root 30 0.0 0.0 0 0 ? S 9月19 0:21 [ksoftirqd/3] 具体软中断分以下几种 12345678910111213enum{ HI_SOFTIRQ=0, TIMER_SOFTIRQ, NET_TX_SOFTIRQ, NET_RX_SOFTIRQ, BLOCK_SOFTIRQ, IRQ_POLL_SOFTIRQ, TASKLET_SOFTIRQ, SCHED_SOFTIRQ, HRTIMER_SOFTIRQ, /* Unused, but kept as tools rely on the numbering. Sigh! */ RCU_SOFTIRQ, /* Preferable RCU should always be the last softirq */ NR_SOFTIRQS}; 通过 open_softirq 初始化 1234567static struct softirq_action softirq_vec[NR_SOFTIRQS] __cacheline_aligned_in_smp;void open_softirq(int nr, void (*action)(struct softirq_action *)){ softirq_vec[nr].action = action;} 触发软中断的时候通过 raise_softirq 来触发 12345678void raise_softirq(unsigned int nr){ unsigned long flags; local_irq_save(flags); raise_softirq_irqoff(nr); local_irq_restore(flags);} 但tasklet机制直接使用了 raise_softirq_irqoff，搜索代码可以发现其他类型的softirq也基本都是用 raise_softirq_irqoff 触发。 123456789101112131415static void __tasklet_schedule_common(struct tasklet_struct *t, struct tasklet_head __percpu *headp, unsigned int softirq_nr){ struct tasklet_head *head; unsigned long flags; local_irq_save(flags); head = this_cpu_ptr(headp); t-&gt;next = NULL; *head-&gt;tail = t; head-&gt;tail = &amp;(t-&gt;next); raise_softirq_irqoff(softirq_nr); local_irq_restore(flags);} 来看下 raise_softirq_irqoff 实现， 123456789101112131415#define local_softirq_pending_ref irq_stat.__softirq_pending#define or_softirq_pending(x) (__this_cpu_or(local_softirq_pending_ref, (x)))void __raise_softirq_irqoff(unsigned int nr){ trace_softirq_raise(nr); // 软中断 raise 的 tracepoint 点 or_softirq_pending(1UL &lt;&lt; nr); // 设置当前cpu的软中断pending状态}inline void raise_softirq_irqoff(unsigned int nr){ __raise_softirq_irqoff(nr); if (!in_interrupt()) wakeup_softirqd(); //如果不是中断上下文，就需要唤醒 ksoftirqd来执行相关软中断，这也保证了在线程上下文中软中断可以得到较快执行} __raise_softirq_irqoff 仅仅是设置 __softirq_pending 标志位，这有两个作用 如果当前是中断irq上下文，在 irq_exit 之后，检查 local_softirq_pending，判断有软中断需要执行 如果当前是线程上下文，在 ksoftirq 线程中检查标志位，最后执行相关的软中断 如果当前在临界区上，在打开中断时，可以检测pending的软中断如何去执行 irq_exit的情况 1234567891011121314151617181920212223static inline void invoke_softirq(void){ //不考虑软中断强制线程化，简化代码 if (ksoftirqd_running(local_softirq_pending())) return; __do_softirq();}void irq_exit(void){#ifndef __ARCH_IRQ_EXIT_IRQS_DISABLED local_irq_disable();#else lockdep_assert_irqs_disabled();#endif account_irq_exit_time(current); preempt_count_sub(HARDIRQ_OFFSET); if (!in_interrupt() &amp;&amp; local_softirq_pending()) invoke_softirq(); //在中断退出之后，如果有 pending的软中断，就需要执行 软中断 tick_irq_exit(); rcu_irq_exit(); trace_hardirq_exit(); /* must be last! */} ksoftirq线程 123456789101112131415static void run_ksoftirqd(unsigned int cpu){ local_irq_disable(); if (local_softirq_pending()) { /* * We can safely run softirq on inline stack, as we are not deep * in the task stack here. */ __do_softirq(); local_irq_enable(); cond_resched(); return; } local_irq_enable();} 开启中断的情况下 123456789101112131415161718void __local_bh_enable_ip(unsigned long ip, unsigned int cnt){ WARN_ON_ONCE(in_irq()); lockdep_assert_irqs_enabled(); preempt_count_sub(cnt - 1); if (unlikely(!in_interrupt() &amp;&amp; local_softirq_pending())) { // 如果不在中断中，且 softirq 有pending的位，就需要执行软中断 do_softirq(); } preempt_count_dec(); preempt_check_resched();}EXPORT_SYMBOL(__local_bh_enable_ip);static inline void local_bh_enable(void){ __local_bh_enable_ip(_THIS_IP_, SOFTIRQ_DISABLE_OFFSET);} __do_softirq不管是退出中断时执行软中断，还是在ksoftirqd中執行软中断，最终都会执行到 __do_softirq 这个函数 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677#define MAX_SOFTIRQ_TIME msecs_to_jiffies(2) //2ms#define MAX_SOFTIRQ_RESTART 10asmlinkage __visible void __softirq_entry __do_softirq(void){ unsigned long end = jiffies + MAX_SOFTIRQ_TIME; unsigned long old_flags = current-&gt;flags; int max_restart = MAX_SOFTIRQ_RESTART; struct softirq_action *h; bool in_hardirq; __u32 pending; int softirq_bit; /* * Mask out PF_MEMALLOC as the current task context is borrowed for the * softirq. A softirq handled, such as network RX, might set PF_MEMALLOC * again if the socket is related to swapping. */ current-&gt;flags &amp;= ~PF_MEMALLOC; pending = local_softirq_pending(); account_irq_enter_time(current); __local_bh_disable_ip(_RET_IP_, SOFTIRQ_OFFSET); in_hardirq = lockdep_softirq_start();restart: /* Reset the pending bitmask before enabling irqs */ set_softirq_pending(0); local_irq_enable(); h = softirq_vec; while ((softirq_bit = ffs(pending))) { //循环执行 pending的软中断 unsigned int vec_nr; int prev_count; h += softirq_bit - 1; vec_nr = h - softirq_vec; prev_count = preempt_count(); kstat_incr_softirqs_this_cpu(vec_nr); trace_softirq_entry(vec_nr); //trace 软中断执行 h-&gt;action(h); trace_softirq_exit(vec_nr); //trace 软中断退出 if (unlikely(prev_count != preempt_count())) { pr_err(&quot;huh, entered softirq %u %s %p with preempt_count %08x, exited with %08x?\\n&quot;, vec_nr, softirq_to_name[vec_nr], h-&gt;action, prev_count, preempt_count()); preempt_count_set(prev_count); } h++; pending &gt;&gt;= softirq_bit; } if (__this_cpu_read(ksoftirqd) == current) rcu_softirq_qs(); local_irq_disable(); pending = local_softirq_pending(); if (pending) { if (time_before(jiffies, end) &amp;&amp; !need_resched() &amp;&amp; --max_restart) //如果又有pending的软中断了，看看是否执行超时了 2ms，且 restart 不能超过10次 goto restart; //未超时 wakeup_softirqd(); //超时 2ms，唤醒softirqd去执行软中断 } lockdep_softirq_end(in_hardirq); account_irq_exit_time(current); __local_bh_enable(SOFTIRQ_OFFSET); WARN_ON_ONCE(in_interrupt()); current_restore_flags(old_flags, PF_MEMALLOC);} do_softirqdo_softirq 和 其他代码路径下执行软中断不一样，最终执行代码的是 do_softirq_own_stack，后续分析 -=-！ 12345678910111213141516171819202122232425void do_softirq_own_stack(void){ struct irq_stack *irqstk; u32 *isp, *prev_esp; irqstk = __this_cpu_read(softirq_stack_ptr); /* build the stack frame on the softirq stack */ isp = (u32 *) ((char *)irqstk + sizeof(*irqstk)); /* Push the previous esp onto the stack */ prev_esp = (u32 *)irqstk; *prev_esp = current_stack_pointer; call_on_stack(__do_softirq, isp);}asmlinkage __visible void do_softirq(void){ unsigned long flags; local_irq_save(flags); if (local_softirq_pending() &amp;&amp; !ksoftirqd_running(pending)) do_softirq_own_stack(); local_irq_restore(flags);} 如何观察softirq在softirq处理过程中（非ksoftirqd线程），在该local cpu上的其他进程是无法进行调度的，不管进程有多高的优先级。因此这点势必对系统实时性造成影响。 通过 /proc/softirqs ，配合 watch 命令来观察 通过一些tracepoint来观察","link":"/2020/09/20/%E5%B9%B6%E5%8F%91%E9%97%AE%E9%A2%98/softirq%E4%BD%95%E6%97%B6%E4%BC%9A%E8%A2%AB%E6%89%A7%E8%A1%8C/"},{"title":"linux内核死锁检测","text":"死锁类型大概可以将linux内核中死锁分为以下两类 121. AA型死锁2. AB-BA型死锁 AA型死锁AA型死锁主要是两个代码路径竞争同一把锁造成的： 进程连续连续两次申请同一把锁就会导致在在第二次申请这把锁的时候死锁 或者是读写锁，同一代码路径先申请了读锁，然后在读锁没有释放的前提下又申请了写者锁，这就会导致在申请写者锁的时候发生死锁 或者在具有抢占关系的代码路径上申请同一把锁，比如进程申请了一把锁A，然后发生了中断或者软中断都会抢占进程，如果在中断或者软中断中又去申请这把锁就会导致死锁的发生 AB-BA型死锁AB-BA型死锁 主要是对两把锁加锁顺序不确定导致的死锁： 假设进程1，2执行过程中都需要持有AB两个锁，进程1线申请到了A锁，进程2申请到了B锁，此时进程1尝试去获取B锁，进程2尝试去获取A锁都无法成功，进程1等待进程2使用完B锁，进程2等待进程1使用完A锁，这就导致了ABBA的死锁情况 还有一种情形是和中断上下文中，进程1需要顺序持有A-B两把锁，进程2会持有B锁，中断会持有A锁。假设T时刻进程1已经持有了A锁，进程2已经持有了B锁，然后进程A去尝试获取B锁，此时中断打断了进程2，试图获取A锁，这样又完美的形成了AB-BA的死锁 想要避免AB-BA死锁的问题，最简单有效的一个办法就是定义获取锁的顺序，不要在不同代码路径下获取不同锁的顺序不一样，破坏死锁的条件。如果大家获取锁的顺序都是一样的，死锁概率会降低很多。 lockdep其实想要在内核成千上百个锁中定义好持有锁的顺序，并且coder严格按照要求来执行的可能性很小，内核同时也提供了死锁检测的工具lockdep。开启这个工具只需要开启下面这两个宏 12CONFIG_PROVE_LOCKING=yCONFIG_DEBUG_LOCK_ALLOC=y 在menuconfig之后会自动打开CONFIG_LOCKDEP这个宏，重新编译替换内核就可以检测kernel中的死锁问题了.. lockdep实现原理开个头，后面好好分析一下-=- 案列分析开个头，后面好好分析一下-=-","link":"/2020/09/11/%E5%B9%B6%E5%8F%91%E9%97%AE%E9%A2%98/%E6%AD%BB%E9%94%81%E9%97%AE%E9%A2%98/"},{"title":"kdump+crash定位稳定性问题","text":"kudmp 原理kdump实际上有两个内核，一个是正常运行的内核，一个我们称为捕获内核/第二内核。 首先需要通过crashkernel=xxx@xxxx来专门为kdump功能专门预留一部分内存，用来放置第二个内核和其他数据信息的，这一部分是给捕获内核使用的。 可以通过kexec系统调用，将第二个内核和initrd，vmcore信息，启动参数等传递给第一个内核。 当系统崩溃时，保存当前寄存器信息，检查是否加载了捕获内核，如果存在则跳转到捕获内核，捕获内核和正常内核一样启动，但是会提供/proc/vmcore接口能够导出内存镜像信息，可以使用dd工具，高级的makedumpfile提供压缩选项。 系统重启，之后就可以使用crash/gdb分析dump文件了 所以可以配置的就本就是 crashkernel的大小，makedumpfile 工具 kudmp 安装使用首先是编译 kernel 配置 12345CONFIG_KEXEC=yCONFIG_SYSFS=yCONFIG_DEBUG_INFO=YCONFIG_CRASH_DUMP=yCONFIG_PROC_VMCORE=y 安装 12sudo apt install kdump-toolssudo apt install crash 如何使用？可以直接主动触发一个崩溃来看看效果 12sudo suecho c &gt; /proc/sysrq-trigger 然后就进入转存储过程，一般会转存储到 /var/crash/$date 目录下，此时应该包含 dump.$date 文件，但是还需要 vmlinux 文件才可以使用 crash 工具分析。vmlinux 是 编译生成的带符号表的可执行文件，只要将它copy到当前目录就可以一起分析了。 一般系统这样就可以搞起来了。 kdump 遇到的问题但是我这边是一直跟踪 mainline kernel, 上述过程都不是很顺利。遇到的问题： makefiledump 转存储不成功1234567[ 4.030842] kdump-tools[301]: Starting kdump-tools:[ 4.032464] kdump-tools[308]: * running makedumpfile -c -d 31 /proc/vmcore /var/crash/202101192137/dump-incomplete[ 24.045632] kdump-tools[380]: check_release: Can't get the kernel version.[ 24.069205] kdump-tools[380]: The kernel version is not supported.[ 24.071343] kdump-tools[380]: The makedumpfile operation may be incomplete.[ 24.077177] kdump-tools[380]: makedumpfile Failed.[ 24.079278] kdump-tools[308]: * kdump-tools: makedumpfile failed, falling back to 'cp' 我们发现是 运行 makedumpfile 过程中，由于 无法 get kernel version 导致无法压缩转存储成功。找到 makedumpfile 源码 添加log 复现。(重新编译 makedumpfile 花费了不少时间) 12345678910[ 4.030842] kdump-tools[301]: Starting kdump-tools:[ 4.032464] kdump-tools[308]: * running makedumpfile -c -d 31 /proc/vmcore /var/crash/202101192137/dump-incomplete[ 4.041333] kdump-tools[380]: create_dumpfile: create_dumpfile![ 24.045632] kdump-tools[380]: check_release: Can't get the kernel version.[ 24.056179] kdump-tools[380]: The kernel version 0:0:0.[ 24.063203] kdump-tools[380]: The kernel version:[0] old_version:[33947663] new_version:[85196807].[ 24.069205] kdump-tools[380]: The kernel version is not supported.[ 24.071343] kdump-tools[380]: The makedumpfile operation may be incomplete.[ 24.077177] kdump-tools[380]: makedumpfile Failed.[ 24.079278] kdump-tools[308]: * kdump-tools: makedumpfile failed, falling back to 'cp' 是这段代码原因 1234567891011121314151617181920#define OLDEST_VERSION KERNEL_VERSION(2, 6, 15) /* linux-2.6.15 */#define LATEST_VERSION KERNEL_VERSION(5, 9, 4) /* linux-5.9.4 */int32_t get_kernel_version(char *release){ version = KERNEL_VERSION(maj, min, rel); if ((version &lt; OLDEST_VERSION) || (LATEST_VERSION &lt; version)) { MSG(&quot;The kernel version is not supported.\\n&quot;); MSG(&quot;The makedumpfile operation may be incomplete.\\n&quot;); }}int check_release(void){ info-&gt;kernel_version = get_kernel_version(info-&gt;system_utsname.release); if (info-&gt;kernel_version == FALSE) { ERRMSG(&quot;Can't get the kernel version.\\n&quot;); return FALSE; }} 第一步先将 LATEST_VERSION 宏定义改掉 1#define LATEST_VERSION KERNEL_VERSION(5, 20, 7) /* linux-5.20.7 */ 还有一个是要看 为什么获得kernel version不对 123456[ 3.917016] kdump-tools[301]: Starting kdump-tools:[ 3.919053] kdump-tools[308]: * running makedumpfile -c -d 31 /proc/vmcore /var/crash/202101192153/dump-incomplete[ 3.930292] kdump-tools[392]: create_dumpfile: create_dumpfile![ 3.943676] kdump-tools[392]: check_release: info-&gt;system_utsname.release: 5.11.0-rc4+.[ 3.955855] kdump-tools[392]: check_release: info-&gt;release: .rc4+.Copying data : [100.0 %] \\ eta: 看代码应该是这段的问题 12345678910111213141516check_release(void){ unsigned long utsname; /* * Get the kernel version. */ if (SYMBOL(system_utsname) != NOT_FOUND_SYMBOL) { utsname = SYMBOL(system_utsname); } else if (SYMBOL(init_uts_ns) != NOT_FOUND_SYMBOL) { utsname = SYMBOL(init_uts_ns) + sizeof(int); //?????? } else { ERRMSG(&quot;Can't get the symbol of system_utsname.\\n&quot;); return FALSE; }} 去掉 + sizeof(int) 即可正常工作 12345[ 3.917016] kdump-tools[3https://github.com/crash-utility/crash01]: Starting kdump-tools:[ 3.919053] kdump-tools[308]: * running makedumpfile -c -d 31 /proc/vmcore /var/crash/202101192153/dump-incomplete[ 3.943676] kdump-tools[392]: check_release: info-&gt;system_utsname.release: 5.11.0-rc4+.[ 3.955855] kdump-tools[392]: check_release: info-&gt;release: 5.11.0-rc4+.Copying data : [100.0 %] \\ eta: 0s crash 版本较低不合适拿到转存储的dump文件之后，就可以用crash 来分析了 ubuntu 20.04 自带的 crash版本是 7.2.8 123456789101112131415161718192021222324252627stable_kernel@kernel: /var/crash/202101192317# sudo crash dump.202101192317 vmlinuxcrash 7.2.8Copyright (C) 2002-2020 Red Hat, Inc.Copyright (C) 2004, 2005, 2006, 2010 IBM CorporationCopyright (C) 1999-2006 Hewlett-Packard CoCopyright (C) 2005, 2006, 2011, 2012 Fujitsu LimitedCopyright (C) 2006, 2007 VA Linux Systems Japan K.K.Copyright (C) 2005, 2011 NEC CorporationCopyright (C) 1999, 2002, 2007 Silicon Graphics, Inc.Copyright (C) 1999, 2000, 2001, 2002 Mission Critical Linux, Inc.This program is free software, covered by the GNU General Public License,and you are welcome to change it and/or distribute copies of it undercertain conditions. Enter &quot;help copying&quot; to see the conditions.This program has absolutely no warranty. Enter &quot;help warranty&quot; for details.GNU gdb (GDB) 7.6Copyright (C) 2013 Free Software Foundation, Inc.License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;This is free software: you are free to change and redistribute it.There is NO WARRANTY, to the extent permitted by law. Type &quot;show copying&quot;and &quot;show warranty&quot; for details.This GDB was configured as &quot;x86_64-unknown-linux-gnu&quot;...WARNING: kernel relocated [702MB]: patching 130299 gdb minimal_symbol valuesplease wait... (patching 130299 gdb minimal_symbol values) [1] 4172 segmentation fault sudo crash dump.202101192317 vmlinux 发现一直是有 segmentfault..怎么也没法解决，才想到可能是 crash 根据版本较低导致的。 找到 crash 源码 找到 7.2.8 的release版本， 发现最新的版本是 7.2.9就下载了7.2.9版本编译安装，其中还需要下载 gdb-7.6,建议直接国内源下载 但是还会出现如下错误 1234567891011121314151617181920stable_kernel@kernel: /var/crash/202101192317# sudo crash dump.202101192317 vmlinuxThis GDB was configured as &quot;x86_64-unknown-linux-gnu&quot;...WARNING: kernel relocated [702MB]: patching 130299 gdb minimal_symbol values KERNEL: vmlinux DUMPFILE: dump.202101192317 [PARTIAL DUMP] CPUS: 4 DATE: Tue Jan 19 23:17:07 CST 2021 UPTIME: 00:03:31LOAD AVERAGE: 0.06, 0.06, 0.01 TASKS: 476 NODENAME: rlk-Standard-PC-i440FX-PIIX-1996 RELEASE: 5.11.0-rc4+ VERSION: #27 SMP Tue Jan 19 13:36:03 CST 2021 MACHINE: x86_64 (3692 Mhz) MEMORY: 2 GB PANIC: &quot;Kernel panic - not syncing: sysrq triggered crash&quot; PID: 3655crash: cannot determine length of symbol: log_end 最后看了看 crash github 的 issue,地一个 open 的问题就是这个…issuse-74 12The crash-7.2.9 doesn't support the 5.10 kernel. It needs these two patchesa5531b2 and 71e159c, so please use the latest master if possible. 果然用最新 master 分支编译安装就 轻松秒杀。。（crash 工具编译安装很省心，没有过多依赖） 1234567891011121314151617181920212223242526stable_kernel@kernel: /var/crash/202101192317# sudo crash dump.202101192317 vmlinuxThis GDB was configured as &quot;x86_64-unknown-linux-gnu&quot;...WARNING: kernel relocated [702MB]: patching 130299 gdb minimal_symbol values KERNEL: vmlinux DUMPFILE: dump.202101192317 [PARTIAL DUMP] CPUS: 4 DATE: Tue Jan 19 23:17:07 CST 2021 UPTIME: 00:03:31LOAD AVERAGE: 0.06, 0.06, 0.01 TASKS: 476 NODENAME: rlk-Standard-PC-i440FX-PIIX-1996 RELEASE: 5.11.0-rc4+ VERSION: #27 SMP Tue Jan 19 13:36:03 CST 2021 MACHINE: x86_64 (3692 Mhz) MEMORY: 2 GB PANIC: &quot;Kernel panic - not syncing: sysrq triggered crash&quot; PID: 3655 COMMAND: &quot;bash&quot; TASK: ffff9aa984598d80 [THREAD_INFO: ffff9aa984598d80] CPU: 1 STATE: TASK_RUNNING (PANIC)crash&gt; exit 这里就不记录具体使用了，在后面具体案例再记录","link":"/2021/01/20/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/kdump+crash%E5%AE%9A%E4%BD%8D%E7%A8%B3%E5%AE%9A%E6%80%A7%E9%97%AE%E9%A2%98/"},{"title":"IDE选择","text":"Vim 还是 Vscode 还是 Source Insight相信很多人一开始都是使用的是SI，还是比较方便的，也比较轻量，运行时占用内存也比较少，一般也能找到破解的方式。我再到公司之后发现我们公司都是使用linux服务器进行开发，使用SI不太方便，但还有方法：比如使用samba服务器，SI通过samba访问服务器代码，也可以建立完整工程但这样有两个致命缺点 121. samba 方式修改代码之后，文件被修改成为了可执行的了，每次提交之前都需要手动修改，或者写个脚本在每次提交之前执行2. samba需要在windows开发机 和 linux服务器之间大量同步，有时候同步很慢 基于上面两个原因，我后面开始使用Vim来进行日常开发。 Vim 使用过的人都知道学习曲线比较陡峭，有个著名的问题：我该怎么退出vim?在使用一段时间Vim之后，插件也越装越多，在远程的时候还是会卡顿，且Vim用起来也不是很方便 12341. Vim 查找替换十分繁琐2. Vim 对于散落在不同文件夹的文件，建立工程十分不友好，也可以做3. 在重构代码的时候，Vim 就不是一个很好的选择，需要经常查找定义等...... 过了一段时间之后，我再也不能忍受Vim了，然后看到了vscode，正巧开放了Remote-ssh插件，插件也都可以离线安装，所以我就切换到了 Vscode。 从我实际使用经验来看，Vscode 除了内存占用大一点之外几乎没有其他缺点，但是优点可以列一堆 12345671. 查找替换十分方便，相比于Vim SI，不要太方便2. 工作区简历十分方便3. Pretty!!4. 各种插件支持，新的语言，只要装一个插件就可以很好支持，且这些插件是社区在维护，基本一段时间就有新的版本（BugFix + New Feature）5. Remote-ssh: 运算，查找这些操作都是在 服务器进行，不会导致本地卡顿，但是对网络要求比较高6. 跟随社区，一直有新版本，新功能可以体验........ So, Vscode Yes!","link":"/2020/09/02/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/vim%20vscode%20si/"},{"title":"vscode 阅读内核源码","text":"相信大多数人使用vscode阅读内核源码跟我一开始的做法是一样的： 121. 下载vscode,安装Remote-ssh,C/C++解析等工具2. 建立 kernel workspace,添加kernel代码 其实这样从我实际的体验来说都要比 SI,Vim 要方便一点了，但是我们还是会遇到 找 12341. 不到清晰的定义，类似于多个slab slub slob的代码，通常都是乱跳转2. 硬件体系结构相关代码和kernel 通用代码之间跳转的时候乱跳转3. kernel 代码中有很多宏定义，不知道实际到底定义了哪些宏，如果看代码的时候需要对比.config文件，那效率会大大降低4. 看一个代码文件，也不知道当前工作会不会用到，可能看到代码，在你工作的环境根本就不会编译 你是不是也有这样的困惑呢？我们接下来一条一条的解决上面这几个问题，让你在看内核代码时畅通无阻 12341. 第一点和第二点可以通过在 settings.json中 设置 search.exclude files.exclude来避免, search.exclude 可以避免你搜索时搜索到他，files.exclude可以直接让你在文件列表视图上看不到他2. 主要原因是 Vscode 默认认为宏是未定义的。可以写一个简单脚本来处理 .config 文件，将其中 =y =m的 CONFIG 项目提取出来，最后在settings.json中 设置 DEFINE 的项，来避免这个问题3. 这个就需要使用 gcc的特性 和 cpp解析的插件了，还需要新版本内核提供的一个脚本 解析config文件脚本下面我重点来讲如何利用 内核编译产生的中间文件，来解析建立你的工程 首先需要下面这个脚本，超过4.19的版本内核应该有这个文件 12sh@ubuntu:~/workspace/linux$ find ./scripts/ -name gen_compile*./scripts/gen_compile_commands.py 先编译bzImage 123mkdir outcp .config ./out/.configmake -j4 bzImage O=./out 生成compile_commands.json 1234# 4.19 - 5.8 之前可以用./scripts/gen_compile_commands.py -d ./out/# 5.9 之后可以用./scripts/clang-tools/gen_compile_commands.py -d ./out/ 配置 .vscode/c_cpp_properties.json 12345678910{# 4.19 - 5.8 之前可以用 ..... &quot;compileCommands&quot;: &quot;${WorkspaceFolder}/out/compile_commands.json&quot; .....# 5.9 之后可以用 ..... &quot;compileCommands&quot;: &quot;${WorkspaceFolder}/compile_commands.json&quot; .....} 到此为止，基本上在浏览内核代码上不会有困难了 贴一下 .vscode/c_cpp_properties.json 配置 1234567891011121314151617181920212223242526{ &quot;env&quot;: { &quot;myDefaultIncludePath&quot;: [&quot;${workspaceFolder}&quot;, &quot;${workspaceFolder}/include&quot;], &quot;myCompilerPath&quot;: &quot;/usr/local/bin/gcc-9&quot; }, &quot;configurations&quot;: [ { &quot;name&quot;: &quot;linux-kernel&quot;, &quot;intelliSenseMode&quot;: &quot;gcc-x64&quot;, &quot;includePath&quot;: [&quot;${myDefaultIncludePath}&quot;], &quot;defines&quot;: [ &quot;__KERNEL__&quot;, &quot;BAR=100&quot;], &quot;compilerPath&quot;: &quot;/usr/bin/clang&quot;, &quot;cStandard&quot;: &quot;c11&quot;, &quot;cppStandard&quot;: &quot;c++17&quot;, &quot;compileCommands&quot;: &quot;${workspaceFolder}/out/compile_commands.json&quot;, &quot;browse&quot;: { &quot;path&quot;: [&quot;${workspaceFolder}&quot;], &quot;limitSymbolsToIncludedHeaders&quot;: true, &quot;databaseFilename&quot;: &quot;&quot; } } ], &quot;version&quot;: 4} 注意”intelliSenseMode”: “gcc-x64” 不同平台配置不一样，我是代码放在ubuntu20上，所以是gcc-x64","link":"/2020/09/03/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/vscode%20%E9%98%85%E8%AF%BB%E5%86%85%E6%A0%B8%E6%BA%90%E4%BB%A3%E7%A0%81/"},{"title":"那些经常忘记的shell命令","text":"总有那么几条经常使用，但有比较难记住的命令，做个记录。 ln 创建连接 ln 创建 软链接 12ln -s 实际文件/目录 软链接文件/目录ln -s /usr/sbin/trace-bpfcc /usr/bin/trace ln 创建 硬链接 12ln 实际文件 硬链接文件 //目录无法使用硬链接ln /usr/sbin/trace-bpfcc /usr/bin/trace tar 解压 tar.gz 文件 // extrat – 提取 1tar -zxvf file.tar.gz 压缩 文件、文件夹 // create – 创建 1tar -zcvf file.tar.gz ./file zip zip 解压 aaa.zip文件 1unzip aaa.zip zip 压缩文件夹 1zip -r aaa.zip ./aaa zip 压缩多个文件 1zip -r aaa.zip ./aaa ./bbb scp 本地 &lt;–&gt; 远程 互传文件 从 ubuntu@xxx:222 获取 /home/ubuntu/.vscode-server/data/Machine/settings.json 文件，且保存为 /tmp/settings.json 1scp -P 222 ubuntu@xxx:/home/ubuntu/.vscode-server/data/Machine/settings.json /tmp/settings.json 将本地的 elfutils-0.144.tar.bz2 文件传输到 ubuntu@xxxx:/home/ubuntu/ 目录下 1scp -P 222 ./elfutils-0.144.tar.bz2 ubuntu@xxxx:/home/ubuntu/ 可以参考scp 命令介绍 这样还是必须输入密码的，可以使用 sshpass 来免密码传输 1sshpass -p hjkl scp -P 222 ./elfutils-0.144.tar.bz2 ubuntu@xxxx:/home/ubuntu/ ubuntu 截图工具 安装 1sudo apt-get install flameshot 使用 1flameshot gui 123Inspiron-5548@ubuntu: ~/workspace/linux-stable# cat ~/.zshrc| tail -n 1alias shot=&quot;flameshot gui&quot;Inspiron-5548@ubuntu: ~/workspace/linux-stable# shot 编译linux kernel 前准备工作 1sudo apt install git fakeroot build-essential ncurses-dev xz-utils libssl-dev bc flex libelf-dev bison clang dd 快速生成大文件12345Inspiron-5548@130ubuntu: ~# dd if=/dev/zero of=haha bs=1M count=100记录了100+0 的读入记录了100+0 的写出104857600 bytes (105 MB, 100 MiB) copied, 0.294862 s, 356 MB/sInspiron-5548@ubuntu: ~# rsync为啥我需要 rsync来同步文件目录呢？ 我hexo_blog是部署在 tencent上的，其实本地公司、家里个人电脑也部署了一份，github gitee上也备份了。但是如果依赖手动去同步： 在哪个 linux机器上修改了之后及时发布到github,且及时备份到gitee 到新的机器上修改之前，需要先 git pull origin master 将本地，或者服务器上 blog更新到最新 开始修改，修改完成 –&gt; 1 然后我发现我这个场景 rsync 并不能很好解决。。如果是 一个服务器 和 备份服务器之间 数据同步 可以使用这样的方式。。突然想到，在新安装一台 linux主机 虚拟机 的时候，可以使用 rsync 来同步，减少 home 目录下文件搞来稿去的 事情。。 远程目录同步到本地 1rsync -avz 49.235.41.28:/home/ubuntu/workspace/hexo_blog /home/ubuntu/workspace/ 本地目录同步到远程 1rsync -avz /home/ubuntu/workspace/ 49.235.41.28:/home/ubuntu/workspace/hexo_blog rsync 命令 samba 相关 1net Z: 192.168.1.103??? wine 使用 工作上可能需要使用通讯会议这样的软件，或者是临时使用source insight，但这些又都没有linux版本，要么使用wine来模拟windows api实现，要么使用windows虚拟机。 这里使用wine:安装 wine 12sudo apt install winesudo apt install winetricks 安装 软件 1sudo apt install winetricks 进去选择资源管理器，初始化一下~/.wine 目录，然后直接安装 使用 软件 1234Inspiron-5548@ubuntu: ~/workspace# tail ~/.zshrc -n 2alias shot=&quot;flameshot gui&quot;alias wemeet=&quot;wine /home/ubuntu/.wine/drive_c/Program\\ Files\\ \\(x86\\)/Tencent/WeMeet/wemeetapp.exe&quot;Inspiron-5548@ubuntu: ~/workspace# wemeet 查询常见函数 apropos 1234567891011121314Inspiron-5548@ubuntu: ~/workspace/kernel_debug_tools# apropos pidacpid (8) - Advanced Configuration and Power Interface event daemonbiosnoop-bpfcc (8) - Trace block device I/O and print details incl. issuing PID.getpid (2) - get process identificationgetppid (2) - get process identificationgit (1) - the stupid content trackerpid_namespaces (7) - overview of Linux PID namespacespidfd_open (2) - obtain a file descriptor that refers to a processpidfd_send_signal (2) - send a signal to a process specified by a file descriptorpidof (8) - find the process ID of a running program.pidpersec-bpfcc (8) - Count new processes (via fork()). Uses Linux eBPF/bcc.pidpersec.bt (8) - Count new processes (via fork()). Uses bpftrace/eBPF.waitpid (2) - wait for process to change stateInspiron-5548@ubuntu: ~/workspace/kernel_debug_tools# 查找常见软件包sudo apt-cache search 123456Inspiron-5548@ubuntu: ~/workspace/kernel_debug_tools# sudo apt-cache search qemu | grep armqemu-efi-arm - UEFI firmware for 32-bit ARM virtual machinesqemu-system-arm - QEMU full system emulation binaries (arm)Inspiron-5548@ubuntu: ~/workspace/kernel_debug_tools# sudo apt-cache search qemu | grep aarch64qemu-efi-aarch64 - UEFI firmware for 64-bit ARM virtual machinesInspiron-5548@ubuntu: ~/workspace/kernel_debug_tools# make 相关指定 gcc version，和 忽略 warning. 1make CC=gcc-7 CFLAGS=&quot;-Wno-error&quot; ubuntu-21 安装低版本 gcc在 ubuntu-21 上需要编译 linux-3.4 版本代码，需要安装 gcc-5但是在ubuntu-21 上，没有提供 gcc-5的官方源，这时候 可以将 ubuntu-16 的源添加到 /etc/apt/source.list 中 123456789101112ubuntu@zeku_server:~/workspace/linux $ sudo apt-cache policy gcc-5gcc-5: 已安装：5.4.0-6ubuntu1~16.04.12 候选： 5.4.0-6ubuntu1~16.04.12 版本列表： *** 5.4.0-6ubuntu1~16.04.12 500 500 https://mirrors.tuna.tsinghua.edu.cn/ubuntu xenial-updates/main amd64 Packages 500 https://mirrors.tuna.tsinghua.edu.cn/ubuntu xenial-security/main amd64 Packages 100 /var/lib/dpkg/status 5.3.1-14ubuntu2 500 500 https://mirrors.tuna.tsinghua.edu.cn/ubuntu xenial/main amd64 Packagesubuntu@zeku_server:~/workspace/linux $ 参考Ubuntu高版本如何安装低版本GCC (以Ubuntu 20安装GCC5为例) sed 命令sed grep awk 被称为linux三剑客，足以说明其强大的字符文本处理能力。a. 删除 ‘str1 str2’ 的特定行(这个在处理 kernel config文件时候很有用) 12345ubuntu@~: $ cat .config | grep &quot;CONFIG_INLINE_SPIN_UNLOCK_IRQ=y&quot;CONFIG_INLINE_SPIN_UNLOCK_IRQ=yubuntu@~: $ubuntu@~: $ sed -i '/CONFIG_INLINE_SPIN_UNLOCK_IRQ=y/d' .configubuntu@~: $ cat .config | grep &quot;CONFIG_INLINE_SPIN_UNLOCK_IRQ=y&quot; b. l grep 命令a. 最常见的1234tencent_clould@1ubuntu: ~/workspace/hexo_blog# grep &quot;asdfasdf&quot; . -nr./source/file.md:224:asdfasdf_./source/file.md:239:asdfasdftencent_clould@ubuntu: ~/workspace/hexo_blog# b. -w 全词匹配 123tencent_clould@ubuntu: ~/workspace/hexo_blog# grep &quot;asdfasdf&quot; . -nr -w./source/file.md:239:asdfasdftencent_clould@ubuntu: ~/workspace/hexo_blog# c. -A 3 -B 3 -C 3显示前后几行：加上查找到的后三行 加上查找前三行 加上查找的前后三行 d. 正则表达式 1grep -E 'migrate.*a.out|sched_switch.*a.out|sugov_next_freq|START|END' awk 命令在 下面这一段字符串总找出 关键数字。12345Big task executing for 3s...Changing to small task...Time incorrectly scheduled on small when task was big: 80561 usec (2% of big task CPU time)Time incorrectly scheduled on big when task was small, after downmigration: 53 usec (0% of small task CPU time)Downmigration latency: 72232 usec awk 将shell中参数传递到 awk表达式中 1234567891011FILE_NAME=/tmp/1234.txtlines=`awk '{print NR}' $FILE_NAME | tail -n1`echo &quot;lines : $lines&quot;# 循环遍历for i in $(seq 1 $lines);do val=`awk -v i=&quot;$i&quot; 'NR==i {print $1}' $FILE_NAME` # awk -v i=&quot;$i&quot; 'NR==i {print $1}' $FILE_NAME echo &quot;val= $val&quot;done 可以使用 awk -v i=&quot;$i&quot; 来传参。 awk 传参到shell数组 123456789101112131415161718#!/bin/bashFILE_NAME=/tmp/1234.txtlines=`awk '{print NR}' $FILE_NAME | tail -n1`echo &quot;lines : $lines&quot;# 循环遍历for i in $(seq 1 $lines);do val[$i]=`awk -v i=&quot;$i&quot; 'NR==i {print $1}' $FILE_NAME` # awk -v i=&quot;$i&quot; 'NR==i {print $1}' $FILE_NAME # echo &quot;val= $val&quot;donefor i in $(seq 1 $lines);do echo &quot;val[$i]= ${val[$i]}&quot;done inlinelinux 代码中会经常使用到 inline的技巧，使得少一次函数调用，代码原地展开，但是有时候 gcc 编译器会 ‘智能’的给我们 inline一些我们不想 inline的函数，所以有什么部分可以让编译器知道我们不想 inline 某些函数呢？ 添加 noinline 关键字 1234static void noinline create_oops(void){ *(int *)0 = 0;} mount重新挂载某一个目录，并改变大小 1mount -o 128M -o remount /tmp ubuntu shell 切换 wifi切换wifi12sudo nmcli connection up Susudo nmcli connection up ChinaNet-5mrv 连接wifi 1sudo nmcli dev wifi connect ChinaNet-5mrv password ejgaefpq ubuntu cmd 测试网络速度 123456789101112amd_server@130ubuntu: ~/workspace# sudo apt install speedtest-cliamd_server@ubuntu: ~/workspace#amd_server@ubuntu: ~/workspace# speedtest-cliRetrieving speedtest.net configuration...Testing from China Telecom Shanghai (101.228.28.85)...Retrieving speedtest.net server list...Selecting best server based on ping...Hosted by China Telecom (Shanghai) [21.59 km]: 8.567 msTesting download speed................................................................................Download: 19.08 Mbit/sTesting upload speed......................................................................................................Upload: 22.78 Mbit/s 局域网内部两台ubuntu 之间测网速两台机器都需要安装iperf 1sudo apt install iperf 测速server 12345amd_server@ubuntu: ~/workspace# iperf -s------------------------------------------------------------Server listening on TCP port 5001TCP window size: 128 KByte (default)------------------------------------------------------------ clientudp测速 123456789101112h@ubuntu:~/rlk$ iperf -u -c 192.168.1.17 -b 1000M------------------------------------------------------------Client connecting to 192.168.1.17, UDP port 5001Sending 1470 byte datagrams, IPG target: 11.22 us (kalman adjust)UDP buffer size: 208 KByte (default)------------------------------------------------------------[ 3] local 192.168.47.128 port 42228 connected with 192.168.1.17 port 5001[ 3] WARNING: did not receive ack of last datagram after 10 tries.[ ID] Interval Transfer Bandwidth[ 3] 0.0-10.0 sec 584 MBytes 490 Mbits/sec[ 3] Sent 416552 datagramssh@ubuntu:~/rlk$ tcp测速 12345678sh@ubuntu:~/rlk$ iperf -c 192.168.1.17 -b 1000M------------------------------------------------------------Client connecting to 192.168.1.17, TCP port 5001TCP window size: 204 KByte (default)------------------------------------------------------------[ 3] local 192.168.47.128 port 46822 connected with 192.168.1.17 port 5001[ ID] Interval Transfer Bandwidth[ 3] 0.0-10.2 sec 22.8 MBytes 18.7 Mbits/sec 明显tcp 测速结果比 udp测速的慢了好多 shell 命令连续执行 &amp;&amp; 连续执行多条命令，前面执行成功，后面才能继续执行 123456amd_server@127ubuntu: ~/workspace# ls &amp;&amp; echobenos data jenkins_workspace just_for_git kernel_debug_tools linux-stable mails nps share tmp vscodeamd_server@ubuntu: ~/workspace# ls asdasd &amp;&amp; echols: cannot access 'asdasd': No such file or directoryamd_server@2ubuntu: ~/workspace# || 前一条命令执行失败，后面命令才能继续执行 12345678amd_server@ubuntu: ~/workspace# ls || echo &quot;hjkl&quot;benos data jenkins_workspace just_for_git kernel_debug_tools linux-stable mails nps share tmp vscodeamd_server@ubuntu: ~/workspace#amd_server@ubuntu: ~/workspace#amd_server@ubuntu: ~/workspace# ls asdasd || echo &quot;hjkl&quot;ls: cannot access 'asdasd': No such file or directoryhjklamd_server@ubuntu: ~/workspace# ;顺序执行一系列命令，后一个命令不管前一条命令执行是否成功都需要执行 12345678amd_server@2ubuntu: ~/workspace# ls asdasd &amp;&amp; echo &quot;hjkl&quot;ls: cannot access 'asdasd': No such file or directoryamd_server@2ubuntu: ~/workspace#amd_server@2ubuntu: ~/workspace#amd_server@2ubuntu: ~/workspace# ls asdasd ; echo &quot;hjkl&quot;ls: cannot access 'asdasd': No such file or directoryhjklamd_server@ubuntu: ~/workspace# &amp;后台执行 123456amd_server@ubuntu: ~/workspace# ls asdasd &amp;[1] 75103amd_server@ubuntu: ~/workspace# ls: cannot access 'asdasd': No such file or directory[1] + 75103 exit 2 ls --color=tty asdasdamd_server@ubuntu: ~/workspace# 切换 shell 为 zsh123456789101112131415sh@amd_server:~/workspace/diagnose-tools $ cat /etc/shells# /etc/shells: valid login shells/bin/sh/bin/bash/usr/bin/bash/bin/rbash/usr/bin/rbash/bin/dash/usr/bin/dash/usr/bin/tmux/bin/zsh/usr/bin/zshsh@amd_server:~/workspace/diagnose-tools $ chsh -s /bin/zshPassword:sh@amd_server:~/workspace/diagnose-tools $ 安装 o-my-zsh 1sh -c &quot;$(curl -fsSL https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh)&quot; 生成、应用patch12git diff &gt; /tmp/123.patchgit apply /tmp/123.patch 12git format-patch HEAD^git am 001-xxx.patch 解压 vmlinuz 到 vmlinux 1234567891011121314Inspiron-5548@1ubuntu: /boot# sudo touch vmlinuxInspiron-5548@ubuntu: /boot# sudo chmod 777 vmlinuxInspiron-5548@ubuntu: /boot# sudo od -t x1 -A d vmlinuz | grep &quot;1f 8b 08 00&quot;0016800 8d 80 00 02 00 00 ff e0 1f 8b 08 00 00 00 00 00^CInspiron-5548@130ubuntu: /boot# 0016800+8zsh: command not found: 0016800+8Inspiron-5548@127ubuntu: /boot# sudo dd if=vmlinuz bs=1 skip=0016808 | zcat &gt; vmlinux记录了9699832+0 的读入记录了9699832+0 的写出9699832 bytes (9.7 MB, 9.3 MiB) copied, 19.7819 s, 490 kB/sgzip: stdin: decompression OK, trailing garbage ignoredInspiron-5548@2ubuntu: /boot# insmod failed 12345678stable_kernel@1kernel: /tmp/share/test_modules/resource_leak/kmemleak# sudo insmod kmemleak.ko[ 42.674738] kmemleak: module is already loadedinsmod: ERROR: could not insert module kmemleak.ko: Invalid parametersstable_kernel@1kernel: /tmp/share/test_modules/resource_leak/kmemleak#stable_kernel@1kernel: /tmp/share/test_modules/resource_leak/kmemleak# dmesg | tail -n 3[ 33.897197] rfkill: input handler disabled[ 42.674217] kmemleak: loading out-of-tree module taints kernel.[ 42.674738] kmemleak: module is already loaded 原因是 kernel 已经安装了一个 built-in 的 kmemleak 模块。 adb 脚本相关等待 adb 设备上线1adb wait-for-device bat脚本相关延时5s 1timeout 5 stress 使用stress --help 123456789101112131415161718192021222324amd_server@ubuntu: ~/workspace/linux-stable# stress --help`stress' imposes certain types of compute stress on your systemUsage: stress [OPTION [ARG]] ... -?, --help show this help statement --version show version statement -v, --verbose be verbose -q, --quiet be quiet -n, --dry-run show what would have been done -t, --timeout N timeout after N seconds --backoff N wait factor of N microseconds before work starts -c, --cpu N spawn N workers spinning on sqrt() -i, --io N spawn N workers spinning on sync() -m, --vm N spawn N workers spinning on malloc()/free() --vm-bytes B malloc B bytes per vm worker (default is 256MB) --vm-stride B touch a byte every B bytes (default is 4096) --vm-hang N sleep N secs before free (default none, 0 is inf) --vm-keep redirty memory instead of freeing and reallocating -d, --hdd N spawn N workers spinning on write()/unlink() --hdd-bytes B write B bytes per hdd worker (default is 1GB)Example: stress --cpu 8 --io 4 --vm 2 --vm-bytes 128M --timeout 10sNote: Numbers may be suffixed with s,m,h,d,y (time) or B,K,M,G (size). stress -c 4 – 对 cpu 进行压力测试stress -m 2 – 对 memory 进行压力测试，两个进程，每个进程默认占用 256MB 内存，一直进行malloc free操作，所以cpu也很高stress -m 3 --vm-bytes 300M – 对 memory 进行压力测试，三个进程，每个进程占用 300MB 内存，一直进行malloc free操作，所以cpu也很高stress -m 3 --vm-bytes 300M --vm-hang 3 – 对 memory 进行压力测试，三个进程，每个进程占用 300MB 内存，因为hang 住，就是不会一直分配内存，所以cpu占用很少 扩大swap分区主要是笔记本只有4G内存，开两个vscode之后一直内存回收，swap设置小了之后会触发剧烈的pagein pageout，所以扩大一点。 1234567891011121314151617root@ubuntu-Inspiron-5548:/# swapoff -aroot@ubuntu-Inspiron-5548:/#root@ubuntu-Inspiron-5548:/# sudo dd if=/dev/zero of=swapfile bs=4M count=1024记录了1024+0 的读入记录了1024+0 的写出4294967296 bytes (4.3 GB, 4.0 GiB) copied, 10.1046 s, 425 MB/sroot@ubuntu-Inspiron-5548:/#root@ubuntu-Inspiron-5548:/# sudo mkswap -f swapfile正在设置交换空间版本 1，大小 = 4 GiB (4294963200 个字节)无标签， UUID=78564906-a685-4965-a8b8-e967feb23a0froot@ubuntu-Inspiron-5548:/# swapswapin.bt swaplabel swapoff swaponroot@ubuntu-Inspiron-5548:/# swaposwapoff swaponroot@ubuntu-Inspiron-5548:/# swaponroot@ubuntu-Inspiron-5548:/# swapon -aroot@ubuntu-Inspiron-5548:/# 查看 线程优先级 123456789amd_server@130ubuntu: ~/workspace/share/test_modules/scheduler/userspace api/nice# ./a.out 20now nice is 20, ret = 19^Camd_server@130ubuntu: ~/workspace/share/test_modules/scheduler/userspace api/nice# ./a.out 0now nice is 0, ret = 0amd_server@130ubuntu: ~/workspace/share/test_modules/scheduler/userspace api/nice# sudo ./a.out -12now nice is -12, ret = -12^Camd_server@130ubuntu: ~/workspace/share/test_modules/scheduler/userspace api/nice# 1234567amd_server@ubuntu: ~/workspace# ps -el | grep a.out0 S 1000 1760035 1690956 0 99 19 - 625 hrtime pts/4 00:00:00 a.outamd_server@ubuntu: ~/workspace# ps -el | grep a.out0 S 1000 1760082 1690956 0 80 0 - 625 hrtime pts/4 00:00:00 a.outamd_server@ubuntu: ~/workspace# ps -el | grep a.out4 S 0 1761864 1761863 0 68 -12 - 624 - pts/4 00:00:00 a.outamd_server@ubuntu: ~/workspace# 只有 root 用户才可以使用 -x 的 nice值。参考Linux C语言库函数参考 — nice参考a.out 代码 改变运行中线程的优先级先 sleep12amd_server@130ubuntu: ~/workspace/share/test_modules/scheduler/userspace api# sleep 100amd_server@ubuntu: ~/workspace/share/test_modules/scheduler/userspace api# 使用 renice 改变线程优先级 123456789101112131415161718192021222324amd_server@ubuntu: ~/workspace# ps -el | grep sleep0 S 1000 1763468 1206623 0 80 0 - 2790 hrtime ? 00:00:00 sleep0 S 1000 1763647 1690956 0 80 0 - 2791 hrtime pts/4 00:00:00 sleepamd_server@1ubuntu: ~/workspace# renice -n 13 -p 2791amd_server@130ubuntu: ~/workspace# ps -aux | grep sleepubuntu 1763468 0.0 0.0 11160 520 ? S 18:11 0:00 sleep 180ubuntu 1763647 0.0 0.0 11164 588 pts/4 S+ 18:12 0:00 sleep 100ubuntu 1763805 0.0 0.0 12120 728 pts/5 S+ 18:13 0:00 grep --color=auto --exclude-dir=.bzr --exclude-dir=CVS --exclude-dir=.git --exclude-dir=.hg --exclude-dir=.svn --exclude-dir=.idea --exclude-dir=.tox sleepamd_server@ubuntu: ~/workspace#amd_server@ubuntu: ~/workspace#amd_server@ubuntu: ~/workspace# renice -n 13 -p 17636471763647 (process ID) old priority 0, new priority 13amd_server@ubuntu: ~/workspace# ps -aux | grep sleepubuntu 1763468 0.0 0.0 11160 520 ? S 18:11 0:00 sleep 180ubuntu 1763647 0.0 0.0 11164 588 pts/4 SN+ 18:12 0:00 sleep 100ubuntu 1763883 0.0 0.0 12120 660 pts/5 S+ 18:14 0:00 grep --color=auto --exclude-dir=.bzr --exclude-dir=CVS --exclude-dir=.git --exclude-dir=.hg --exclude-dir=.svn --exclude-dir=.idea --exclude-dir=.tox sleepamd_server@ubuntu: ~/workspace#amd_server@ubuntu: ~/workspace#amd_server@ubuntu: ~/workspace#amd_server@ubuntu: ~/workspace# ps -el | grep sleep0 S 1000 1763468 1206623 0 80 0 - 2790 hrtime ? 00:00:00 sleep0 S 1000 1763647 1690956 0 93 13 - 2791 hrtime pts/4 00:00:00 sleepamd_server@ubuntu: ~/workspace# 一行循环执行某个命令 1for i in $(seq 1 200); do echo $i; done 快速git clone 大项目 1git clone git://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git --depth 1 1for i in $(seq 1 200); do echo $i; done 然后 git fetch --unshallow 整个项目即可 tuna 开源 mirror的linux仓库地址123mainline: git clone https://mirrors.tuna.tsinghua.edu.cn/git/linux.gitstable: git clone https://mirrors.tuna.tsinghua.edu.cn/git/linux-stable.gitnext: git clone https://mirrors.tuna.tsinghua.edu.cn/git/linux-next.git kernel.org官方源 12345amd_server@ubuntu: ~/workspace/stable# git remote -vvkernel git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git (fetch)kernel git://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git (push)stable git://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git (fetch)stable git://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git (push) 如何快速 clone 一个较大仓库 12git clone --depth=1 xxxgit fetch --unshallow vim 安装插件 1:PlugInstall 查看线程调度策略 12ps -eLfc | grep FFps -eLfc | grep RR 给cpu定频率 1234567891011121314151617cpufreq-set:DESCRIPTIONcpufreq-set allows you to modify cpufreq settings without having to type e.g. &quot;/sys/devices/system/cpu/cpu0/cpufreq/scaling_set_speed&quot; all the time.OPTIONS-c --cpu &lt;CPU&gt;number of CPU where cpufreq settings shall be modified.-d --min &lt;FREQ&gt;new minimum CPU frequency the governor may select.-u --max &lt;FREQ&gt;new maximum CPU frequency the governor may select.-g --governor &lt;GOV&gt;new cpufreq governor.-f --freq &lt;FREQ&gt;specific frequency to be set. Requires userspace governor to be available and loaded.-h --helpPrints out the help screen. git loggit log 十分强大，目前一个工作是查看 Q &amp; M的 kernel代码改了那些东西，用起来这个更是十分有必要 12345678910111. git log -S2. git log -L :func_name:filegit log -L :select_task_rq:kernel/sched/core.c3. git log -Lline_num:filegit log -L2825,+2:kernel/sched/core.c4. git log master..feature : 包含在 feature分支，但是不在master分支上的patch5. git log --no-merges : 过滤掉merge的patch6. git log commit1 commit2 : commit1--commit2之间的patch 我需要监控 两个commit之间的patch，但是我只关注 kernel/sched/ 目录下文件的修改，改如何将这些commits找出来？这里我写了一个python脚本来实现。 git brancha. 检查哪些分支包含某个 commit id123456789101112131415161718192021222324252627ubuntu@zeku_server:~/workspace/msm-5.4 $ git branch -a --contains f556cd74a7ea038c67812b0d9b5d971d03da5372 msm-5.4.r5 remotes/origin/LA.AU.0.2.0.r4.1 remotes/origin/LV.AU.0.2.0.r1 remotes/origin/kernel.lnx.5.4.r1-rel remotes/origin/kernel.lnx.5.4.r3-rel remotes/origin/kernel.lnx.5.4.r6-rel remotes/origin/kernel.lnx.5.4.r7-rel remotes/origin/kernel.lnx.5.4.r9-rel remotes/origin/msm-5.4.r2 remotes/origin/msm-5.4.r5ubuntu@zeku_server:~/workspace/msm-5.4 $ git branch -r --contains f556cd74a7ea038c67812b0d9b5d971d03da5372 origin/LA.AU.0.2.0.r4.1 origin/LV.AU.0.2.0.r1 origin/kernel.lnx.5.4.r1-rel origin/kernel.lnx.5.4.r3-rel origin/kernel.lnx.5.4.r6-rel origin/kernel.lnx.5.4.r7-rel origin/kernel.lnx.5.4.r9-rel origin/msm-5.4.r2 origin/msm-5.4.r5ubuntu@zeku_server:~/workspace/msm-5.4 $ git branch --contains f556cd74a7ea038c67812b0d9b5d971d03da5372 msm-5.4.r5ubuntu@zeku_server:~/workspace/msm-5.4 $ git branch msm-5.4.r5 --contains f556cd74a7ea038c67812b0d9b5d971d03da5372 msm-5.4.r5ubuntu@zeku_server:~/workspace/msm-5.4 $ git branch android11-5.4 --contains f556cd74a7ea038c67812b0d9b5d971d03da5372ubuntu@zeku_server:~/workspace/msm-5.4 $ git showa. 查看某个 commit是哪个版本引入的 123456ubuntu@zeku_server:~/workspace/linux $ git show e6223a3b19421e3a8df1352d21fd0d71093f44ae:MakefileVERSION = 2PATCHLEVEL = 6SUBLEVEL = 36EXTRAVERSION =NAME = Flesh-Eating Bats with Fangs 随机提取一个commit来生成patch 12git format-patch HEAD^ : 生成patchgit format-patch -1 commit-id : 生成这个 commit-id 的patch。 gcc -E预编译 .c文件 gcc 生成汇编 文件 1gcc -S 123.c gcc 生成 汇编和 C文件 对照 1gcc -Wa,-adlhn 123.c 检查commits是否有Functional change 12345678910111213141516171819add helper func for util_avg and runnable_avg calc when entityenqueue and dequeue. No functional change.without this change:size vmlinux text data bss dec hex filename19889268 6632812 2429160 28951240 1b9c2c8 vmlinuxsize kernel/sched/fair.o text data bss dec hex filename 40044 1569 96 41709 a2ed kernel/sched/fair.oubuntu@zeku_server:~/workspace/linux-stable $with this change:size vmlinux text data bss dec hex filename19889268 6632812 2429160 28951240 1b9c2c8 vmlinuxsize kernel/sched/fair.o text data bss dec hex filename 40044 1569 96 41709 a2ed kernel/sched/fair.o 参考lkml 邮件 编译出现error, forbidden warning: 编译错误 1234两种解决办法这是因为编译的时候,由于你的代码不符合标准,比如类型转换的时候,你没有强制转化(比如将int型赋值给char型,需要强制转换)或者定义了某些变量或者函数却没有使用.这些都会出现警告,而警告将会被看做错误来处理.1. 修改自己的代码,将出现的警告全部解决掉.该强制转换的强制转化,该删掉定义了未使用的变量函数删掉或者注释掉.2. 修改scripts/gcc-wrapper.py将interpret_warning(line)注释掉,这样它就不会将警告当成错误处理了 qemu 邮件列表 qemu-discuss@nongnu.org 12345678910KVM开发者邮件列表是：kvm@vger.kernel.orgKVM内核部分以及QEMU中与KVM相关部分的讨论都会在这里讨论。订阅方法：向majordomo@vger.kernel.org发送一封以”subscribe kvm”为正文的邮件。QEMU开发者邮件列表是：qemu-devel@nongnu.orgQEMU普通用户讨论的邮件列表是：qemu-discuss@nongnu.orgKVM的IRC频道：在irc.freenode.net服务器上的#kvm频道QEMU的IRC频道：在irc.oftc.net服务器上的#qemu频道 android ltp 编译 test eas. 动态编译 123./configure CC=aarch64-linux-gnu-gcc --build=x86_64-pc-linux-gnu --target=aarch64-linux --host=aarch64-linuxmake -j10 然后静态编译 1234aarch64-linux-gnu-gcc -static -g -O2 -fno-strict-aliasing -pipe -Wall -W -Wold-style-definition -I../../../../include -I../../../../include -I../../../../include/old/ -c -o eas_big_to_small.o eas_big_to_small.caarch64-linux-gnu-gcc -static -L../../../../lib eas_big_to_small.o trace_parse.o util.o -lltp -lpthread -o eas_big_to_small sched_feat 怎么动态修改？a. 直接在板子上修改文件节点，只对当前测试有效 /sys/kernel/debug/sched_features，12345678910111213venus:/data/local/tmp/eas # cat /sys/kernel/debug/sched_featuresGENTLE_FAIR_SLEEPERS START_DEBIT NO_NEXT_BUDDY LAST_BUDDY CACHE_HOT_BUDDY WAKEUP_PREEMPTION NO_HRTICK NO_DOUBLE_TICK NONTASK_CAPACITY NO_TTWU_QUEUE NO_SIS_AVG_CPU SIS_PROP NO_WARN_DOUBLE_CLOCK RT_PUSH_IPI RT_RUNTIME_SHARE NO_LB_MIN ATTACH_AGE_LOAD WA_IDLE WA_WEIGHT WA_BIAS NO_UTIL_EST NO_SUGOV_RT_MAX_FREQvenus:/data/local/tmp/eas #venus:/data/local/tmp/eas #venus:/data/local/tmp/eas # echo UTIL_EST &gt; /sys/kernel/debug/sched_featuresvenus:/data/local/tmp/eas #venus:/data/local/tmp/eas #venus:/data/local/tmp/eas # cat /sys/kernel/debug/sched_featuresGENTLE_FAIR_SLEEPERS START_DEBIT NO_NEXT_BUDDY LAST_BUDDY CACHE_HOT_BUDDY WAKEUP_PREEMPTION NO_HRTICK NO_DOUBLE_TICK NONTASK_CAPACITY NO_TTWU_QUEUE NO_SIS_AVG_CPU SIS_PROP NO_WARN_DOUBLE_CLOCK RT_PUSH_IPI RT_RUNTIME_SHARE NO_LB_MIN ATTACH_AGE_LOAD WA_IDLE WA_WEIGHT WA_BIAS UTIL_EST NO_SUGOV_RT_MAX_FREQvenus:/data/local/tmp/eas #venus:/data/local/tmp/eas # b. 直接修改源文件，这样编译生效直接修改 kernel/sched/features.h 文件中的 true or false。 123456789101112SCHED_FEAT(RT_RUNTIME_SHARE, true)SCHED_FEAT(LB_MIN, false)SCHED_FEAT(ATTACH_AGE_LOAD, true)SCHED_FEAT(WA_IDLE, true)SCHED_FEAT(WA_WEIGHT, true)SCHED_FEAT(WA_BIAS, true)/* * UtilEstimation. Use estimated CPU utilization. */SCHED_FEAT(UTIL_EST, true) shell 参数个数 1234567if [ $# -eq 0 ];then echo &quot;zero param...&quot;elif [ $# -eq 1 ];then echo &quot;one param...&quot;else echo &quot;more than one param...&quot;fi shell 参数值比较 1234567if [ &quot;$1&quot; == &quot;start&quot; ];then echo &quot;do start&quot;elif [ &quot;$1&quot; == &quot;stop&quot; ];then echo &quot;do stop&quot;else echo &quot;Please make sure the positon variable is start or stop.&quot;fi shell 捕获 Ctrl + C123456789101112#!/bin/bashtrap 'onCtrlC' INTfunction onCtrlC () { echo 'Ctrl+C is captured' exit 0}while true; do echo 'I am working!' sleep 10done systrace 支持抓取到 systrace之后，还需要直观的显示出来，可以在 https://ui.perfetto.dev/#!/ 看到，点击跳转 offline 某个 cpu 1234567891011121314151617ubuntu@ubuntu:/sys/devices/system/cpu/cpu0$ cat /sys/devices/system/cpu/cpu0/online1ubuntu@ubuntu:/sys/devices/system/cpu/cpu0$ cat /sys/devices/system/cpu/online0-3ubuntu@ubuntu:/sys/devices/system/cpu/cpu0$ cat /sys/devices/system/cpu/offlineubuntu@ubuntu:/sys/devices/system/cpu/cpu0$ sudo suroot@ubuntu:/sys/devices/system/cpu/cpu0# echo 0 &gt;&gt; /sys/devices/system/cpu/cpu0/onlineroot@ubuntu:/sys/devices/system/cpu/cpu0# cat /sys/devices/system/cpu/cpu0/online0root@ubuntu:/sys/devices/system/cpu/cpu0# cat echo 0 &gt;&gt; /sys/devices/system/cpu/onlinebash: /sys/devices/system/cpu/online: Permission deniedroot@ubuntu:/sys/devices/system/cpu/cpu0# cat /sys/devices/system/cpu/online1-3root@ubuntu:/sys/devices/system/cpu/cpu0# cat /sys/devices/system/cpu/offline0root@ubuntu:/sys/devices/system/cpu/cpu0# wsl2 限制内存, cpu使用文件位置 D:\\Users\\50001309 1234567ubuntu@wsl:/mnt/d/Users/50001309 $ cat .wslconfig[wsl2]processors=3memory=3GBswap=3GBlocalhostForwarding=trueubuntu@wsl:/mnt/d/Users/50001309 $ vscode remote-ssh 频繁断开原因win10 自带 openssh 与 vscode不兼容，需要修改环境变量，直接使用 git自带的ssh. 1https://blog.csdn.net/jyhongjax/article/details/106075493#fromHistory 解压 patch.gza. 删除源文件，得到解压后的文件1234567ubuntu@zeku_server:~/workspace/linux/tmp_out $ubuntu@zeku_server:~/workspace/linux/tmp_out $ lsinclude Makefile patch-5.12-rc3-rt3.patch.gz scripts sourceubuntu@zeku_server:~/workspace/linux/tmp_out $ gunzip -d patch-5.12-rc3-rt3.patch.gzubuntu@zeku_server:~/workspace/linux/tmp_out $ lsinclude Makefile patch-5.12-rc3-rt3.patch scripts sourceubuntu@zeku_server:~/workspace/linux/tmp_out $ b. 保留源文件，得到解压后的文件 12ubuntu@zeku_server:~/workspace/linux/tmp_out $ gunzip -cd patch-5.12-rc3-rt3.patch.gz &gt; patch.txtubuntu@zeku_server:~/workspace/linux/tmp_out $ 解压 patches.xx.tar.gza. 解压1234567891011ubuntu@zeku_server:~/workspace/linux/tmp_out $ lsinclude Makefile patches-5.12-rc3-rt3.tar.gz scripts sourceubuntu@zeku_server:~/workspace/linux/tmp_out $ tar -zxvf patches-5.12-rc3-rt3.tar.gzpatches/patches/0001-kthread-Move-prio-affinite-change-into-the-newly-cre.patchpatches/0001-locking-rtmutex-Remove-cruft.patchpatches/0001-mm-sl-au-b-Change-list_lock-to-raw_spinlock_t.patch...ubuntu@zeku_server:~/workspace/linux/tmp_out $ ls patches/ 0001-kthread-Move-prio-affinite-change-into-the-newly-cre.patch 0019-tick-sched-Prevent-false-positive-softirq-pending-wa.patch mm-memcontrol-Disable-preemption-in-__mod_memcg_lruv.patch 0001-locking-rtmutex-Remove-cruft.patch 0020-kdb-only-use-atomic-consoles-for-output-mirroring.patch mm-memcontrol-do_not_disable_irq.patch b. 使用 1 给 kernel 打上 rt patch 1234567891011121314151617181920212223242526272829303132333435363738ubuntu@zeku_server:~/workspace/linux/tmp_out $ wget http://cdn.kernel.org/pub/linux/kernel/projects/rt/5.12/patch-5.12-rc3-rt3.patch.gz--2021-05-25 10:24:37-- http://cdn.kernel.org/pub/linux/kernel/projects/rt/5.12/patch-5.12-rc3-rt3.patch.gzResolving cdn.kernel.org (cdn.kernel.org)... 151.101.1.176, 151.101.129.176, 151.101.65.176, ...Connecting to cdn.kernel.org (cdn.kernel.org)|151.101.1.176|:80... connected.HTTP request sent, awaiting response... 200 OKLength: 135397 (132K) [application/x-gzip]Saving to: ‘patch-5.12-rc3-rt3.patch.gz’patch-5.12-rc3-rt3.patch.gz 100%[===============================================================================================================&gt;] 132.22K 384KB/s in 0.3s2021-05-25 10:24:38 (384 KB/s) - ‘patch-5.12-rc3-rt3.patch.gz’ saved [135397/135397]ubuntu@zeku_server:~/workspace/linux/tmp_out $ gunzip -d patch-5.12-rc3-rt3.patch.gzubuntu@zeku_server:~/workspace/linux/tmp_out $ lsinclude Makefile patch-5.12-rc3-rt3.patch scripts sourceubuntu@zeku_server:~/workspace/linux/tmp_out $ubuntu@zeku_server:~/workspace/linux/tmp_out $ cd ..ubuntu@zeku_server:~/workspace/linux $ patch -p1 &lt; ./tmp_out/patch-5.12-rc3-rt3.patchpatching file arch/alpha/include/asm/spinlock_types.hpatching file arch/arm/Kconfigpatching file arch/arm/include/asm/spinlock_types.hpatching file arch/arm/include/asm/thread_info.hpatching file arch/arm/kernel/asm-offsets.cpatching file arch/arm/kernel/entry-armv.Spatching file arch/arm/kernel/signal.c......ubuntu@zeku_server:~/workspace/linux $ git stOn branch 5.12-rc3Changes not staged for commit: (use &quot;git add/rm &lt;file&gt;...&quot; to update what will be committed) (use &quot;git restore &lt;file&gt;...&quot; to discard changes in working directory) modified: arch/alpha/include/asm/spinlock_types.h modified: arch/arm/Kconfig modified: arch/arm/include/asm/spinlock_types.h modified: arch/arm/include/asm/thread_info.h modified: arch/arm/kernel/asm-offsets.c modified: arch/arm/kernel/entry-armv.S modified: arch/arm/kernel/signal.c MIPS DMIPS MFLOPSMIPS: Million Instructions Per Second，每秒处理的百万级的机器语言指令数。DMIPS：Dhrystone Million Instructions Per Second，整数运算测试程序 每秒处理的百万级的机器语言指令数MFLOPS：A benchmark which attemps to estimate a system's floating-point &quot;MFLOPS&quot; rating for specific FADD, FSUB, FMUL and FDIV instruction mixes.，是一种基于浮点运算的CPU测试程序，当然，这种测试的结果也以 MFLOPS来加以表示，代表了CPU处理浮点运算的能力。 jenkins 升级tuna mirror1https://mirrors.tuna.tsinghua.edu.cn/jenkins/war-stable/2.277.4/jenkins.war 如何升级 1234567下载提示的 jenkins.war 包，上传到服务器查看 jenkins.war 的目录，比如是：/usr/share/jenkins/jenkins.war，可用如下命令查看：ps aux | grep jenkins备份初始的 jenkins.war 包：cp /usr/share/jenkins/jenkins.war jenkins.war.bak停止服务：/etc/init.d/jenkins stop替换新的 war 包后，启动服务：/etc/init.d/jenkins start 各种版本 gcc 下载wget 相应地址 1https://ftp.gnu.org/gnu/gcc/gcc-4.8.5/ linux 源文件编译成为 汇编文件 1make kernel/sched/core.lst V=1 12(base) ubuntu@zeku_server:~/workspace/linux $ ls kernel/sched/core.lstkernel/sched/core.lst 公司服务器安装 软件一般公司服务器都会预装很多软件，需要使用的时候只需要 module load即可 123module load vs_codemodule unload vs_codemodule av Ubuntu启用Cgroups V2首先确定 当前系统是默认启用的是 cgroup v1 还是 v2，可以通过 /sys/fs/cgroup/cgroup.controllers 文件确认，12ubuntu@zeku_server:~ $ cat /sys/fs/cgroup/cgroup.controllerscat: /sys/fs/cgroup/cgroup.controllers: 没有那个文件或目录 说明 此时还是 cgroup v1 可以更新 grub文件，在启动的时候 给 kernel 传参，使得默认使用 cgroup v2。在/etc/default/grub 文件中，在 GRUB_CMDLINE_LINUX 中添加 systemd.unified_cgroup_hierarchy=1；然后sudo update-grub 更新 grub文件，reboot之后，系统将默认使用 cgroup v2. 12ubuntu@zeku_server:~ $ cat /sys/fs/cgroup/cgroup.controllerscpuset cpu io memory pids rdma 这是系统默认使用 cgroup v2的样子 也可以通过mount 看： 123ubuntu@zeku_server:~ $ mount | grep cgroupcgroup2 on /sys/fs/cgroup type cgroup2 (rw,nosuid,nodev,noexec,relatime,nsdelegate)ubuntu@zeku_server:~ $ 其中GRUB_CMDLINE_LINUX_DEFAULT 仅在正常引导时才有效（恢复模式不适用）GRUB_CMDLINE_LINUX 总是有效的 ubuntu 升级查看当前版本123456789101112ubuntu@zeku_server:/etc $ lsb_release -aNo LSB modules are available.Distributor ID: UbuntuDescription: Ubuntu 20.10Release: 20.10Codename: groovyubuntu@zeku_server:/etc $ uname -aLinux ubuntu-HP-ProDesk-680-G4-MT 5.8.0-59-generic #66-Ubuntu SMP Thu Jun 17 00:46:01 UTC 2021 x86_64 x86_64 x86_64 GNU/Linuxubuntu@zeku_server:/etc $ sudo do-release-upgrade正在检查新版 Ubuntu请在升级前安装您的发行版所有可用更新。ubuntu@zeku_server:/etc $ 如果想升级到非lts版本，需要修改 lts 到 normal 1sudo sed -i 's/lts/normal/g' /etc/update-manager/release-upgrades 最后执行升级即可 1sudo do-release-upgrade 升级成功之后查看 123456789101112131415ubuntu@zeku_server:~ $ lsb_release -aNo LSB modules are available.Distributor ID: UbuntuDescription: Ubuntu 21.04Release: 21.04Codename: hirsuteubuntu@zeku_server:~ $ uname -aLinux ubuntu-HP-ProDesk-680-G4-MT 5.11.0-22-generic #23-Ubuntu SMP Thu Jun 17 00:34:23 UTC 2021 x86_64 x86_64 x86_64 GNU/Linuxubuntu@zeku_server:~ $ubuntu@zeku_server:~ $ sudo do-release-upgrade正在检查新版 UbuntuThere is no development version of an LTS available.To upgrade to the latest non-LTS development releaseset Prompt=normal in /etc/update-manager/release-upgrades.ubuntu@zeku_server:~ $ 删除多余 ubuntu old kernel image 12345671973 sudo dpkg --get-selections |grep linux1974 sudo dpkg --purge linux-image-5.4.0-77-generic1975 sudo dpkg --purge linux-modules-extra-5.4.0-77-generic1976 sudo dpkg --purge linux-image-5.4.0-77-generic1977 sudo dpkg --purge linux-image-5.8.0-59-generic1978 sudo dpkg --purge linux-modules-extra-5.8.0-59-generic1979 sudo dpkg --purge linux-image-5.8.0-59-generic linux selftests怎么运行？编译 1make -C tools/testing/selftests 运行所有测试项 1make -C tools/testing/selftests run_tests 运行单个测试项 1make -C tools/testing/selftests TARGETS=ptrace run_test 参考Linux Kernel Selftests参考Doc文档 ubuntu bootchart 安装使用安装1sudo apt install systemd-bootchart 使用 需要在 kernel 启动参数加上 init=/lib/systemd/systemd-bootchart，然后在/run/log 目录下，每次启动会生成一张 svg 的矢量图，可以分析出 boot阶段 cpu, io 的使用情况，酌情优化。 12345stable_kernel@kernel: /run/log# pwd/run/logstable_kernel@kernel: /run/log# lsbootchart-20210721-1917.svg journalstable_kernel@kernel: /run/log# 参考ARCH-LINUX-WIKI-Improving performance/Boot process ubuntu 配置 smba 服务器 server安装软件包 1sudo apt-get install samba smbclient cifs-utils sudo vim /etc/samba/smb.conf在global下的workgroup=WORKGROUP下面添加一句：security = user最后一行添加： 1234567891011[zeku_smba] comment = sharefolder path = /tmp/server valid users = zeku_smba browseable = yes read only = yes create mask = 0777 directory mask = 0777 public = yes writable = yes available = yes clientwsl 上 安装 软件包 1sudo apt-get install smbclient cifs-utils 挂载 目录 1sudo mount -t cifs -o username=zeku_smba //10.122.7.199/zeku_smba /tmp/server/ _GNU_SOURCE_GNU_SOURCE 有两种使用方法 在包含头文件前 加上 #define _GNU_SOURCE 在链接编译的时候，加上-D_GNU_SOURCE 选项这样会降低可移植性 参考_GNU_SOURCE有啥用 ubuntu sudo 免密码 sudo chmod 744 /etc/sudoers sudo vi /etc/sudoers, 最后一行添加 rlk ALL=(ALL) NOPASSWD:ALL sudo chmod 400 /etc/sudoers, 然后即刻生效 a a a a vmbox 虚拟机？ linux发行版本如何选择？123456789个人使用的话，有信仰选 Debian，追求新特性选 Fedora，爱好瞎折腾 arch 、gentoo，新手 Ubuntu，想用 Windows 软件 deepin，喜欢 kde 界面、或想在命令行下用 gui 配置选 suse，安全 Linux 小白选 kali，你问我 CentOS 怎样？它早该进坟墓了，倒是 CentOS stream 还凑合 参考V2EX安装 fedora 33发现已经是 5.10.22了，果然是与mainline最近的版本。 手动编译安装qemu 发现卸载qemu-system-aarch64，然后重新安装，就已经支持了 raspi4了，可能是 ubuntu官方源自己适配的吧。。ubuntu 安装 qemu-system 1sudo apt install qemu-system ubuntu 卸载 qemu-system 1sudo apt auto-remove qemu-system 手动编译安装 qemu 123451. sudo apt-get install git libglib2.0-dev libfdt-dev libpixman-1-dev zlib1g-dev2. sudo apt install ninja-build3. ./configure --target-list=aarch64-softmmu --enable-debug-info --enable-trace-backends=simple --prefix=/usr/local4. make -j45. make install linux下pdf阅读器12341. chorme 直接打开2. ubuntu自带的3. FoxitReader4. wps ubuntu cmd 更新 vscode方法一:123wget https://vscode-update.azurewebsites.net/latest/linux-deb-x64/stable -O /tmp/code_latest_amd64.debsudo dpkg -i /tmp/code_latest_amd64.deb 方法二: 12sudo apt updatesudo apt install code 会自动下载最新版本 code，然后解压覆盖掉老版本，避免每次手动从网页端下载。 shell 脚本检查 安装 1sudo apt install shellcheck 检查 1shellcheck yourscript 代码参考github使用参考博客 控制 /var/log/journal 目录大小 1journalctl --vacuum-size=100M /var/log/btmp 文件太大参考btmpThis means people are trying to brute-force your passwords (common on any public-facing server).就是有人在尝试暴力破解你的机器密码，Linux用户登录信息放在三个文件中： /var/run/utmp：记录当前正在登录系统的用户信息，默认由who和w记录当前登录用户的信息，uptime记录系统启动时间； /var/log/wtmp：记录当前正在登录和历史登录系统的用户信息，默认由last命令查看； /var/log/btmp：记录失败的登录尝试信息，默认由lastb命令查看。 jenkins 启动失败jenkins 自身运行的log存放在 /var/log/jenkins 中可以查看其中log，来确定具体什么原因其中有个原因是 8080 端口被其他进程占用了1234567891011121314151617181920212223242526272022-01-07 08:37:26.217+0000 [id=1] SEVERE winstone.Logger#logInternal: Container startup failedjava.net.BindException: Address already in use at sun.nio.ch.Net.bind0(Native Method) at sun.nio.ch.Net.bind(Net.java:461) at sun.nio.ch.Net.bind(Net.java:453) at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:222) at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:85) at org.eclipse.jetty.server.ServerConnector.openAcceptChannel(ServerConnector.java:345)Caused: java.io.IOException: Failed to bind to 0.0.0.0/0.0.0.0:8080 at org.eclipse.jetty.server.ServerConnector.openAcceptChannel(ServerConnector.java:349) at org.eclipse.jetty.server.ServerConnector.open(ServerConnector.java:310) at org.eclipse.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:80) at org.eclipse.jetty.server.ServerConnector.doStart(ServerConnector.java:234) at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:72) at org.eclipse.jetty.server.Server.doStart(Server.java:386) at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:72) at winstone.Launcher.&lt;init&gt;(Launcher.java:182)Caused: java.io.IOException: Failed to start Jetty at winstone.Launcher.&lt;init&gt;(Launcher.java:184) at winstone.Launcher.main(Launcher.java:355) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at Main._main(Main.java:375) at Main.main(Main.java:151) 查看是哪个进程将 端口占用了 12345678910111213141516171819202122VM-0-11-ubuntu# netstat -tnlpActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program nametcp 0 0 127.0.0.1:43265 0.0.0.0:* LISTEN 2595/nodetcp 0 0 0.0.0.0:111 0.0.0.0:* LISTEN 1/inittcp 0 0 0.0.0.0:80 0.0.0.0:* LISTEN 985/nginx: master ptcp 0 0 0.0.0.0:6000 0.0.0.0:* LISTEN 944/sshd: /usr/sbintcp 0 0 192.168.122.1:53 0.0.0.0:* LISTEN 1712/dnsmasqtcp 0 0 10.0.3.1:53 0.0.0.0:* LISTEN 1111/dnsmasqtcp 0 0 0.0.0.0:21 0.0.0.0:* LISTEN 862/vsftpdtcp 0 0 127.0.0.53:53 0.0.0.0:* LISTEN 781/systemd-resolvetcp 0 0 127.0.0.1:38551 0.0.0.0:* LISTEN 860/containerdtcp 0 0 0.0.0.0:25 0.0.0.0:* LISTEN 2367/mastertcp 0 0 127.0.0.1:6010 0.0.0.0:* LISTEN 2974/sshd: ubuntu@ptcp6 0 0 :::12865 :::* LISTEN 955/netservertcp6 0 0 :::111 :::* LISTEN 1/inittcp6 0 0 :::8080 :::* LISTEN 10483/javatcp6 0 0 :::80 :::* LISTEN 985/nginx: master ptcp6 0 0 :::25 :::* LISTEN 2367/masterVM-0-11-ubuntu# ps -aux | grep 10483tomcat 10483 20.1 6.1 2411244 125152 ? Ssl 16:44 0:08 /usr/lib/jvm/java-8-openjdk-amd64/bin/java -Djava.util.logging.config.file=/var/lib/tomcat9/conf/logging.properties -Djava.util.logging.manager=org.apache.juli.ClassLoaderLogManager -Djava.awt.headless=true -Djdk.tls.ephemeralDHKeySize=2048 -Djava.protocol.handler.pkgs=org.apache.catalina.webresources -Dorg.apache.catalina.security.SecurityListener.UMASK=0027 -Dignore.endorsed.dirs= -classpath /usr/share/tomcat9/bin/bootstrap.jar:/usr/share/tomcat9/bin/tomcat-juli.jar -Dcatalina.base=/var/lib/tomcat9 -Dcatalina.home=/usr/share/tomcat9 -Djava.io.tmpdir=/tmp org.apache.catalina.startup.Bootstrap startroot 10690 0.0 0.0 6300 672 pts/3 S+ 16:44 0:00 grep 10483 最后将他干掉就好了 1233045 2022-01-07 16:37:22 sudo service jenkins restart3046 2022-01-07 16:37:26 sudo service jenkins status3062 2022-01-07 16:42:38 sudo service tomcat stop EBLEBUWTMOXNVAMC 单独编译内核某个文件1make -j12 ARCH=arm64 CROSS_COMPILE=/usr/bin/aarch64-linux-gnu- kernel/sched/core_ctl.s O=./out/ 各个手机厂商开源的手机镜像 12345678910111213141516171819三星：https://opensource.samsung.com/projects小米:device tree: https://github.com/MiCode/kernel_devicetree/tree/umi-q-osskernel: https://github.com/MiCode/Xiaomi_Kernel_OpenSource华为:https://consumer.huawei.com/en/opensource/detail/?siteCode=worldwide&amp;productCode=Smartphones&amp;fileType=openSourceSoftware&amp;pageSize=10&amp;curPage=1oppo:https://github.com/oppo-source/FindX2vivo:暂无Qcom codeaurora:https://www.codeaurora.org/https://source.codeaurora.org/quic/la/kernel/common/ 博客 1https://xinqiu.gitbooks.io/linux-inside-zh/content/SyncPrim/linux-sync-6.html ubuntu删除旧版本内核image查看机器安装过哪些版本image 123456789101112ubuntu@zeku_server:/usr/sbin $ dpkg -l | grep linux-image-ii linux-image-5.11.0-22-generic 5.11.0-22.23 amd64 Signed kernel image genericii linux-image-5.11.0-25-generic 5.11.0-25.27 amd64 Signed kernel image genericii linux-image-5.11.0-38-generic 5.11.0-38.42 amd64 Signed kernel image genericii linux-image-5.13.0-20-generic 5.13.0-20.20 amd64 Signed kernel image genericii linux-image-5.13.0-21-generic 5.13.0-21.21 amd64 Signed kernel image genericii linux-image-5.13.0-22-generic 5.13.0-22.22 amd64 Signed kernel image genericiU linux-image-5.13.0-27-generic 5.13.0-27.29 amd64 Signed kernel image genericrc linux-image-5.4.0-71-generic 5.4.0-71.79 amd64 Signed kernel image genericrc linux-image-5.4.0-72-generic 5.4.0-72.80 amd64 Signed kernel image genericiU linux-image-generic 5.13.0.27.37 amd64 Generic Linux kernel imageubuntu@zeku_server:/usr/sbin $ rc表示软件包已经删除（Removed），但配置文件（Config-files）还在．ii表示已经安装（Installed）．可以用 1sudo apt-get purge linux-image-5.13.0-21-generic 删除已经安装的image linux &amp;&amp; windows拆分合并大文件windows需要 1split trace.html -b 40m &amp;&amp; tail -10 trace.html &gt; tail.html &amp;&amp; cat xaa tail.html &gt; new.html 安卓过滤前台进程 1logcat -b events | grep on_resume 安卓过滤前台进程之后，logcat退出，不阻塞当前shell 1logcat -d -b events | grep on_resume 安卓logcat 过滤某个tag的log 1logcat *:S powerhal-libperfmgr 安卓批量安装apk 12345678910111213141516# 使用应用宝下载apk，它可以保存apk文件/sdcard/Android/data/com.tencent.android.qqdownloader/files/tassistant/apk/# 先压缩再pulladb shell &quot;tar -jcvf /sdcard/Android/data/com.tencent.android.qqdownloader/files/tassistant/apk.tar.gz /sdcard/Android/data/com.tencent.android.qqdownloader/files/tassistant/apk&quot;adb pull //sdcard/Android/data/com.tencent.android.qqdownloader/files/tassistant/apk.tar.gz ./# 直接 pulladb shell &quot;tar -jcvf /sdcard/Android/data/com.tencent.android.qqdownloader/files/tassistant/apk# 遍历目录安装for i in `ls ./`; do adb install -r &quot;${i}&quot;done echo 不换行 echo -n &quot;abc&quot; &amp;&amp; echo adf 如何在 hexo markdown 文件中插入图片 方法 使用MarkDown中![](/images/image.jpg)的方式访问使用 CONFIG_COMPAT 含义 1CONFIG_COMPAT －－－－－是否支持64 bit kernel上运行32bit 的application System V 信号量 和 POSIX 信号量区别 System V它最初由AT&amp;T开发，曾经也被称为AT&amp;T System V，是Unix操作系统众多版本中的一支。在1983年第一次发布，一共发行了4个System V的主要版本，System V Release4，或者称为SVR4，是最成功的版本，该版本有些风格成为一些UNIX共同特性的源头，如下表格的初始化脚本/etc/init.d。用来控制系统的启动和关闭。System v信号量测试基于内核的，它放在内核里面，要使用System V信号量需要进入内核态，所以在多线程编程中一般不建议使用System V信号量，因为线程相对于进程是轻量级的，从操作系统的调度开销角度看，如果使用System V信号量会使得每次调用都要进入内核态，丧失了线程的轻量优势。当我们讨论“System v信号量”时，所指的是计数信号量集。 Posix是Portable Operating System Interface(可移植性操作系统接口)的简称，是一个电气与电子工程学会即IEEE开发的一系列标准，目的是为运行在不同操作系统的应用程序提供统一的接口，实现者是不同的操作系统内核。Posix信号量是基于内存的，即信号量值是放在共享内存中的，它使与文件系统中的路径名对应的名字来标识。当我们谈论“Posix 信号量”时，所指的是单个计数信号量。在Linux操作系统中，Posix信号量(共享内存、消息队列)可以通过ipcs命令查看。Posix信号量多用于进程间通信。 参考Unix/Linux的System V、BSD、Posix概念 代码覆盖率统计方法1231. 在gcc编译代码时增加参数gcov2. 使用gtest写ut用例，构造不同的参数，争取跑到函数中每个分支3. 使用gcov还原代码执行过程，生成代码执行分析文件.html 下载 pixel6 kernel 代码a. https://source.android.com/setup/build/building-kernels#fromHistory按照这个源来，但是 实际需要 切换源。 b. https://mirrors.tuna.tsinghua.edu.cn/help/AOSP/ 这是清华的 源 12345注意: 本镜像是 AOSP 镜像，Android SDK因版权原因，我们不能提供镜像服务。可访问 https://cs.android.com 或 https://github.com/aosp-mirror 在线搜索及浏览 AOSP 源码。参考 Google 教程 https://source.android.com/setup/build/downloading， 将 https://android.googlesource.com/ 全部使用 https://mirrors.tuna.tsinghua.edu.cn/git/AOSP/ 代替即可。 实际命令 1repo init -u https://mirrors.tuna.tsinghua.edu.cn/git/AOSP/kernel/manifest -b android-gs-raviole-5.10-android12L 需要注意环境变量 .repo 文件非常大 在 .repo 目录下执行 12345先查看find . -name &quot;*pack&quot; -type f |xargs -I {} du -s {}|sort -k 1 -n|tail -10|awk '{print $2}'| xargs rm再删除find . -name &quot;*pack&quot; -type f |xargs -I {} du -s {}|sort -k 1 -n|tail -10|awk '{print $2}'| xargs rm 参考删除repo中大文件 arm training 培训地址 1https://arm.fuseuniversal.com/learning/plans docker root权限 docker 1docker run -it --name rambunctious_whereas ubuntu:18.04 减少 aarch64 ubuntu image大小","link":"/2020/09/12/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/%E7%BB%8F%E5%B8%B8%E5%BF%98%E8%AE%B0%E7%9A%84shell%E5%91%BD%E4%BB%A4/"},{"title":"欢迎来到liulangren的读核之旅","text":"不忘初心我是博主 liulangren, 从2019年毕业开始，到目前一直在一家无人机(DJI)公司 从事linux 内核性能、稳定性相关的工作。 建立这个博客的初心是想记录在工作中的一些发现，读代码的一些心得。从毕业开始到现在一年多时间不长也不短，看了很多代码，也解决了很多系统中的问题，但是一直没有系统的去记录一下，导致现在回想起来一时竟然不知道如何总结描述过去这一年多得工作。linux 正所谓好记性不如烂笔头，之后在工作之余，一定多多记录总结。我的博客应该会包含以下几个部分 1231. linux 内核某些子系统的阅读分享2. linux 内核性能稳定性、性能调优实践案列分享3. 个人年度总结、月度总结等 回首过去回想大学生活，还是很精彩的。 大一，懵懵懂懂，充满了对大学生活的期待，从军训到刚入班级时的自我介绍，一切都是那么美好。上学期和大多数人一样，参加社团，努力学习公共课，泡图书馆，成绩还不错拿到了一等奖学金。大一下学期开始，开始思考之后的职业路线，慢慢接触到学院的一些实验室，也认识了很多厉害的学长。也是从大一下学期，在学校的某个神奇的实验室，参加了第一个校电子设计大赛，当时好像是做的一个888的光立方，当然最后也没拿到奖，不过融入了实验室的大佬，后面跟着他们开始做大学生电子设计大赛。 还记得那个炎热的暑假，大家暑假之后陆陆续续离校了，我和同队的小伙伴还继续坚守在实验室的一线。也是在这个暑假，苏北的我第一次在宿舍见到拇指大的蟑螂，还记得因为蟑螂在实验室睡了好几天…。 白天大家在实验室画板子，晚上熬夜写代码，那时候都是玩MCU,代码也是操作寄存器加上一些 if else，虽然很简单，但是配合几个电机传感器，确实让我着迷的很；有时候干活干累了，几个好友聚在一起吹一吹牛皮，探讨一下今年可能的赛题，几个人再去学校对面的香江不夜街点几个菜，好好犒劳一下自己。工作之后想起来，尤其怀念那种无忧无虑，没有KPI压力，有大把时间做自己喜欢的事情的生活。 这样从暑假开始到八月中，几乎每天都是早上十点多到实验室，晚上十一点回去，最后也取得了不错的成绩，在当时看最重要的是学会了 AD，STM32等一系列MCU、传感器的使用，可以自己单独干一些小项目了，实际上后来我也靠着画板子，写MCU裸机代码赚了一些钱，后面再说。 大二，说起大二我都有些模糊了，因为我的大二和大三是十分相似的：逃课，泡实验室，跟老师做项目，参加比赛。先说说逃课吧，其实从大一下学期接触到实验室开始，我就开始经常逃课，不过到了大二大三逃课更多，可能一周上个两节课这种吧，实验室的几个老师都知道我逃课，也经常说我不要逃课，但是并不会阻止我去实验室。其次，泡实验室，那时候H老师从一些当地企业接到了不少的外包项目，H老师自己只做硬件，然后MCU的简单代码 都是让我搞定的，在大二大三这两年应该给他做了四五个项目，H老师也很厚道，每个月按本科标准都会给一点劳务费，一直给到我大四毕业。期间我也自己从外面接到了一些项目，自己画原理图，画PCB，做板子，焊接，debug代码，简直一条龙服务啊，也赚到了一点钱补贴生活费。从大二下学期我就开始搬出去住了，主要是晚上经常11点会宿舍，怕打扰宿管阿姨。大二暑假和大一暑假并没有什么不同，也是留校画板子，写代码，不过开始从打酱油的角色变到了主力队员，比赛过程不表，参加过电赛的都知道很辛苦，结果当然也还可以。 大三，基本和大二类似，业余时间接触了linux，从安装虚拟机，使用cat ls等命令做起，C语言也重新学习了一遍，画板子，debug裸机代码也比较熟练了。大三上学期还参加了一个农业机械的比赛，是去的重庆的西南大学，在高铁上 和 路上学会了王者荣耀（挺后悔的，到现在王者荣耀应该打了2500场以上了，浪费了比较多的时间），也是第一次去西部，在去往重庆的高铁上一路山见识了祖国的大好河山，着实震撼（我自己出生在江苏，大学也在江苏，没咋见过大山大河）。大三下学期开始面临读研和工作的选择，由于我自己呆实验室时间比较长，对学术研究不太感兴趣，当时感觉也可以找到一个不错的工作，就没有准备考研（PS：由于大二大三基本没咋上课，早就没有保研名额了，我们学校电赛一等奖也不能保研…）。从春天开始着手准备，自己在实验室干了两年MCU裸机相关的事情，感觉技术壁垒不高（当时原子、野火、安富莱很火），可替代性太强。然后学习了UCOSII的代码，对RTOS有了一定了解，当时裸机开发实习那种就没考虑，其中过程应该比较艰辛的，忘得差不多了。一开始一个学长S给我内推了百度的一份实习，一直没面试，然后遇到著名的ODM企业（华勤）招实习生就去了，基本还是比较简单的，很快就拿到了实习offer（主要考察C语言）。又过了很久百度开始面试，和面试官说打算写一个RTOS，然后面试官问我栈相关问题，没回答上来，被无情嘲讽了一下，但是最后拿到了百度的 offer，华勤就被备胎了。。。当时拿到offer有点飘，有一门课就没复习，想着补考再说了（一点课都没去过。。）后面再说。大三暑假就去上海百度实习了，过程比较辛苦，还记得实习工资150/天，当时对 RTOS也还不太了解，去了之后发现是用乐鑫的ESP32跑RTOS做智能小度音响，不知道为啥一直没给我分配活。我就开始学习RTOS，每天看代码，看博客，写博客（之前是在CSDN上）准备秋招，学习了一个学长自己写的 一个RTOS–SMC_RTOS，真的是麻雀虽小，五脏俱全，学习了RTOS 调度原理，mutex、simapore 实现原理，对比UCOSII、UCOSIII、FREERTOS也看了看，写了一些博客，后面在秋招的帮助很大。期间我也还参加了电赛，当然只有比赛那几天回去了，也是辛苦，但是收获满满，其中暑假还给老师做了一个项目，每个周五从公司出发去虹桥火车站到学校，周日再从学校出发到上海，那个暑假坐高铁花了不少钱….期间虽然没给我安排啥活，但是从正好碰到了部门年中大会，居然在长白山举行，人生第一次坐飞机，第一次去东北，第一次泡温泉等等….现在回想起来，互联网公司真的有钱，之后还是想到互联网公司去发展（DJI年会就在附近一个宾馆随便举行了，也没有年中大会这种）很多公司秋招在大三暑假前就开始了，DJI我记得还没放暑假就开始了，但是真正面试却到了八月，当时正看SMC_RTOS、FREERTOS、UCOSII，和面试官吹了一波，就过了；暑假过程中也面了一些其他公司，华勤（自然被备胎了），CVTE(挂)，OVHM(简历没过)，这时候才意识到学校不是211带来的伤害，也比较晚了。到九月份拿到了DJI的正式电话、邮件 offer，开出得到薪资很高（至少在当时的我看来），然后十月份又面了海康拿到了offer没去。学校一发三方我就和DJI签了… 大四，和前两年相比轻松了些许，主要是之后工作已经确定，但是奇葩的专业大四还有很多课，比大三还多，大四下也补考了大三由于逃课落下的课。到了寒假，我就立刻去DJi实习了，工作也都是linux下的相关开发，去了之后发现 现实 和 想象中的 DJI还是有很大不一样的，没有外界的那种无人机的高大上，只有每天10115低头打代码，更可恶的是公司严格的保密制度，导致研发不能上外网，遇到问题基本手机百度…实习快结束的时候，去深圳出差了两周，第一次出差，算是第二次坐飞机，五星级酒店，体验还不错。。。 步入职场毕业之后回家呆了两周，然后就去公司正式入职了。 DJI 确实会给年轻人很多机会，刚正式入职就直接跟项目（也说明没有成熟的培训体系），一个team几个人开始一个模块重构，重构过程中，写了很多代码（BUG），对个人能力也得到了一些锻炼。。。也会经常出差… 重新出发后面会 尽力业余时间更新本博客","link":"/2020/09/01/%E7%94%9F%E6%B4%BB%E6%84%9F%E6%82%9F/%E4%B8%8D%E5%BF%98%E5%88%9D%E5%BF%83/"},{"title":"我应该如何看代码","text":"每次看一个模块源码都比较痛苦，在 kernel 的代码海洋里面像一只无头苍蝇一样，看来半天却不知所云，记录一下之后在看某个模块源码时 应该注意什么，比较快的熟悉，警醒自己！ 庞大的linux内核现在是 2021-01-11号，上个月linus发布了 Linux-5.10 LTS版本，往往每年的最后一个 kernel 版本就是作为LTS的版本，今年是12月13号。 linus 会在每个大版本发布之后都会 开启 一个merge window的时间，这个时间大家都可以尽量的合入之前早就开发好的已经在next tree的 patch，大概有一两周？今年这时候恰好是圣诞节，老外是基本周末都不会工作的，更何况是 圣诞+元旦 这种节日了，看了基本 2020-12-23 – 2021-01-04 都是没有daily next tree 版本更新的。 A total of 1,971 developers contributed to 5.10 — again, just short of the record set by 5.8. Of those developers, 252 (just under 13%) made their first contribution in 5.10; that is the lowest number seen since 5.6. The most active 5.10 developers were: Linux 5.10代码统计 新的模块应该如何涉及首先要高明白他是干什么的 结合注释去看，谷歌翻译，vscode在线、离线插件都很好用 结合 git log 看： git log -S’xxx’ file_name 根据Makefile看：如果这个目录下Makefile 都没直接包含这个文件.o需要使能之后才会编译到内核中的话，结合是否工作实际需要再看 谷歌、百度：中文 英文都可以作为关键次搜索 LWN 找到Archives，看看有没有介绍的文章 看看 Documents 目录下有没有介绍的文档 看这个模块 关键数据结构 和关键数据结构数据成员 如 struct_task struct_mmaddress_space等 新的子系统 知道模块子系统作用 找个方法实践一下，比如 docker 可以很好实践 cgroups 关键数据结构 导出到用户空间的接口，看他在 kernel中实现比如123456789101112tencent_clould@127ubuntu: /proc/sys/fs# lsaio-max-nr dir-notify-enable inode-nr leases-enable overflowgid pipe-user-pages-soft protected_symlinksaio-nr epoll inode-state mount-max overflowuid protected_fifos quotabinfmt_misc file-max inotify mqueue pipe-max-size protected_hardlinks suid_dumpabledentry-state file-nr lease-break-time nr_open pipe-user-pages-hard protected_regular veritytencent_clould@ubuntu: /proc/sys/fs#tencent_clould@ubuntu: /proc/sys/fs#tencent_clould@ubuntu: /proc/sys/fs# cat file-nr3200 0 9223372036854775807tencent_clould@ubuntu: /proc/sys/fs# cat file-max9223372036854775807tencent_clould@ubuntu: /proc/sys/fs# 12345678910111213141516171819202122static struct ctl_table fs_table[] = { { .procname = &quot;inode-nr&quot;, .data = &amp;inodes_stat, .maxlen = 2*sizeof(long), .mode = 0444, .proc_handler = proc_nr_inodes, }, { .procname = &quot;inode-state&quot;, .data = &amp;inodes_stat, .maxlen = 7*sizeof(long), .mode = 0444, .proc_handler = proc_nr_inodes, }, { .procname = &quot;file-nr&quot;, .data = &amp;files_stat, .maxlen = sizeof(files_stat), .mode = 0444, .proc_handler = proc_nr_files, }, 如何深入比如我知道 page_cache 的大概了，也知道他的作用，但是看代码的时候比较迷茫？ 看看 page_cache 的API 12345678amd_server@ubuntu: ~/workspace/linux-stable/mm# grep &quot;EXPORT_SYMBOL&quot; ./filemap.c -nr279:EXPORT_SYMBOL(delete_from_page_cache);377:EXPORT_SYMBOL(filemap_check_errors);437:EXPORT_SYMBOL(filemap_fdatawrite);444:EXPORT_SYMBOL(filemap_fdatawrite_range);459:EXPORT_SYMBOL(filemap_flush);502:EXPORT_SYMBOL(filemap_range_has_page);557:EXPORT_SYMBOL(filemap_fdatawait_range); 在.c 文件中搜索trace_ 看看，找到他的静态追踪点 tracepoints 1234567amd_server@130ubuntu: ~/workspace/linux-stable/mm# grep &quot;trace_&quot; ./filemap.c -nr236: trace_mm_filemap_delete_from_page_cache(page);354: trace_mm_filemap_delete_from_page_cache(pvec-&gt;pages[i]);683: trace_filemap_set_wb_err(mapping, eseq);724: trace_file_check_and_advance_wb_err(file, old);902: trace_mm_filemap_add_to_page_cache(page);amd_server@ubuntu: ~/workspace/linux-stable/mm# 直接使用 bpftrace -l 这个根据搜索你的模块，看看有那些比较重要的函数，这些重要函数往往是掌握这个模块的关键，也是经常出现问题需要debug 或者观测的地方，所以 kernel 才会为他设置一个静态追踪点 1234amd_server@ubuntu: ~/workspace/linux-stable/mm# sudo bpftrace -l | grep tracepoint: | grep page_cachetracepoint:filemap:mm_filemap_delete_from_page_cachetracepoint:filemap:mm_filemap_add_to_page_cacheamd_server@ubuntu: ~/workspace/linux-stable/mm# stat 统计数据入手类似于 slub 这种内存分配器，内核提供了很多统计数据，可以利用这些事件统计 来跟踪关键代码路径 如 12345678910111213enum stat_item { ALLOC_FASTPATH, /* Allocation from cpu slab */ ALLOC_SLOWPATH, /* Allocation by getting a new cpu slab */ FREE_FASTPATH, /* Free to cpu slab */ FREE_SLOWPATH, /* Freeing not to cpu slab */amd_server@ubuntu: ~/workspace/linux-stable/mm# grep ALLOC_FASTPATH . -nr./slub.c:2883: stat(s, ALLOC_FASTPATH);./slub.c:5390:STAT_ATTR(ALLOC_FASTPATH, alloc_fastpath);amd_server@ubuntu: ~/workspace/linux-stable/mm# grep ALLOC_SLOWPATH . -nr./slub.c:2666: stat(s, ALLOC_SLOWPATH);./slub.c:5391:STAT_ATTR(ALLOC_SLOWPATH, alloc_slowpath);amd_server@ubuntu: ~/workspace/linux-stable/mm# struct_task 的 context switch 事件计数 12345678910111213struct task_struct {...... unsigned long nvcsw; unsigned long nivcsw;......}amd_server@ubuntu: ~/workspace/linux-stable/kernel/sched# grep &quot;nivcsw&quot; . -nr./debug.c:497: (long long)(p-&gt;nvcsw + p-&gt;nivcsw),./debug.c:934: nr_switches = p-&gt;nvcsw + p-&gt;nivcsw;./debug.c:989: __PS(&quot;nr_involuntary_switches&quot;, p-&gt;nivcsw);./core.c:4988: switch_count = &amp;prev-&gt;nivcsw;amd_server@ubuntu: ~/workspace/linux-stable/kernel/sched# 看到静态追踪点 或者stat统计数据 之后，看看附近的函数，更上层函数等","link":"/2021/01/11/%E7%94%9F%E6%B4%BB%E6%84%9F%E6%82%9F/%E6%88%91%E5%BA%94%E8%AF%A5%E5%A6%82%E4%BD%95%E7%9C%8B%E4%BB%A3%E7%A0%81/"},{"title":"记录第一个kernel patch","text":"之前也提过两个patch给社区不过都被否了，很多大佬第一次提patch也是从改改标点开始的。最近在看文件系统相关的代码，看到有个地方定义和注释不一致，可以给社区贡献一下，也顺便记录一下，搭建环境的过程（之前用的ubuntu18虚拟机被我删了…现在用ubuntu20重新搞一下） 修改代码，形成patch其实我这个patch很简单，只是修改一个注释而已： 1234567891011121314151617sh@ubuntu:~/workspace/linux$ git diffdiff --git a/include/linux/jbd2.h b/include/linux/jbd2.hindex 08f904943ab2..a1ef05412acf 100644--- a/include/linux/jbd2.h+++ b/include/linux/jbd2.h@@ -452,8 +452,8 @@ struct jbd2_inode { struct jbd2_revoke_table_s; /**- * struct handle_s - The handle_s type is the concrete type associated with- * handle_t.+ * struct jbd2_journal_handle - The jbd2_journal_handle type is the concrete+ * type associated with handle_t. * @h_transaction: Which compound transaction is this update a part of? * @h_journal: Which journal handle belongs to - used iff h_reserved set. * @h_rsv_handle: Handle reserved for finishing the logical operation.sh@ubuntu:~/workspace/linux$ 12345678910111213141516sh@ubuntu:~/workspace/linux$ git log -1commit f8db1794205285c44d4b0b7d83f0fda1b9adec00 (HEAD -&gt; master)Author: Hui Su &lt;sh_def@163.com&gt;Date: Tue Sep 22 23:28:05 2020 +0800 FIX the comment of struct jbd2_journal_handle the struct name was modified long ago, but the comment still use struct handle_s. Signed-off-by: Hui Su &lt;sh_def@163.com&gt;sh@ubuntu:~/workspace/linux$sh@ubuntu:~/workspace/linux$sh@ubuntu:~/workspace/linux$ git format-patch HEAD^0001-FIX-the-comment-of-struct-jbd2_journal_handle.patchsh@ubuntu:~/workspace/linux$ 如果第一个版本有问题则需要重新提第二个版本patch，与第一个版本差别就是加上 ‘ -v2’ ‘ -v3’ 等， 12sh@ubuntu:~/workspace/linux-stable$ git format-patch HEAD^ -v2v2-0001-tools-time-access-sys-kernel-debug-udelay_test-be.patch 在邮件发出之后会自动变成[PATCH v2] tools/time: access /sys/kernel/debug/udelay_test before test 如果不是一个patch，而是一系列patch怎么办呢？ 123肯定是有办法的，目前我还没这个需求，留白，后续补充lkml中的邮件格式：[PATCH v6 00/25] Add support for Clang LTO git format-patch 找到修改代码部分的maintainer 先用 perl scripts/checkpatch.pl 检查patch有无语法错误 再用 perl scripts/get_maintainer.pl 获取patch修改的代码的 maintainer1234567891011sh@ubuntu:~/workspace/linux$ perl scripts/checkpatch.pl 0001-FIX-the-comment-of-struct-jbd2_journal_handle.patchtotal: 0 errors, 0 warnings, 10 lines checked0001-FIX-the-comment-of-struct-jbd2_journal_handle.patch has no obvious style problems and is ready for submission.sh@ubuntu:~/workspace/linux$sh@ubuntu:~/workspace/linux$ perl scripts/get_maintainer.pl 0001-FIX-the-comment-of-struct-jbd2_journal_handle.patch&quot;Theodore Ts'o&quot; &lt;tytso@mit.edu&gt; (maintainer:JOURNALLING LAYER FOR BLOCK DEVICES (JBD2))Jan Kara &lt;jack@suse.com&gt; (maintainer:JOURNALLING LAYER FOR BLOCK DEVICES (JBD2))linux-ext4@vger.kernel.org (open list:JOURNALLING LAYER FOR BLOCK DEVICES (JBD2))linux-kernel@vger.kernel.org (open list)sh@ubuntu:~/workspace/linux$ 安装配置邮件客户端安装下面工具 123sudo apt-get install sendmailsudo apt install muttsudo apt install smtp 配置一下 1234567891011121314151617181920sh@ubuntu:~$ cat .muttrcset sendmail=&quot;/usr/bin/msmtp&quot;set envelope_from=yesset realname=&quot;Hui Su&quot;set from=sh_def@163.comset use_from=yesset envelope_from=yessh@ubuntu:~$sh@ubuntu:~$sh@ubuntu:~$sh@ubuntu:~$ cat .msmtprcaccount defaulthost smtp.163.comport 25from sh_def@163.comauth plaintls offuser sh_def@163.compassword #这里填授权码sh@ubuntu:~$ 测试一下 12sh@ubuntu:~$ echo &quot;test&quot; |mutt -s &quot;my_first_test&quot; sh_def@163.comsh@ubuntu:~$ echo &quot;test&quot; |mutt -s &quot;my_first_test&quot; xxxx@qq.com 吐槽一下，如果是第一次配置还是比较难搞的参考参考 git 配置这个应该在 git commit 阶段就已经配置好了，不赘述 推送patch只需要在各个邮箱之间加上,即可，不赘述 回复邮件一般情况下社区大佬都会在几天到一周内回复你的patch邮件，如果你的patch需要修改，然后你需要重新回复他们的邮件，这应该如何做呢 第一步配置好fetchmail 1234567891011121314151617181920212223sh@ubuntu:~$ sudo apt install fetchmail[sudo] password for rlk:Reading package lists... DoneBuilding dependency treeReading state information... Donefetchmail is already the newest version (6.4.2-2).0 upgraded, 0 newly installed, 0 to remove and 369 not upgraded.sh@ubuntu:~$ cat ~/.fetchmailrcpoll pop3.163.comprotocol POP3user &quot;xxx@163.com&quot;password &quot;客户端授权码&quot;sh@ubuntu:~$ ll | grep rc | grep fetch-rwx------ 1 rlk rlk 83 10月 8 01:38 .fetchmailrc*# 接收邮件sh@ubuntu:~$ fetchmail -vfetchmail: 6.4.2 querying pop3.163.com (protocol POP3) at 2020年10月08日 星期四 01时38分51秒: poll startedTrying to connect to 220.181.12.110/110...connected.fetchmail: POP3&lt; +OK Welcome to coremail Mail Pop3 Server (163coms[10774b260cc7a37d26d71b52404dcf5cs])fetchmail: POP3&gt; CAPAfetchmail: POP3&lt; +OK Capability list follows............... 查看接收的邮件然后直接 mutt 就可以看到接收到的邮件了 123456789101112131415161718192021222324252627282930q:Quit d:Del u:Undel s:Save m:Mail r:Reply g:Group ?:Help 1 O + 9月 09 iovisor-dev@lis ( 378) [iovisor-dev] iovisor-dev@lists.iovisor.org Daily Summary 2 + 9月 22 Mail Delivery S ( 50) Returned mail: see transcript for details 3 + 9月 22 Mail Delivery S ( 50) Returned mail: see transcript for details 4 O F 9月 23 To sh_def@163.c ( 75) [PATCH] mm,page_alloc: fix the count of free pages 5 O T 9月 23 Jan Kara ( 38) ┬─&gt;Re: [PATCH] FIX the comment of struct jbd2_journal_handle 6 O T 10月 03 Theodore Y. Ts' ( 11) └─&gt;Re: [PATCH] FIX the comment of struct jbd2_journal_handle 7 O + 9月 25 Josh Poimboeuf ( 3) ┬─&gt;Re: [PATCH] mm,kmemleak-test.c: move kmemleak-test.c to samples dir 8 O T 10月 05 Catalin Marinas ( 12) └─&gt;Re: [PATCH] mm,kmemleak-test.c: move kmemleak-test.c to samples dir 9 O T 9月 25 akpm@linux-foun ( 313) + mmkmemleak-testc-move-kmemleak-testc-to-samples-dir.patch added to -mm tree 10 O T 9月 25 akpm@linux-foun ( 92) + mm-fix-some-comments-in-page_allocc-and-mempolicyc.patch added to -mm tree 11 O T 9月 28 Ira Weiny ( 57) Re: [PATCH] mm/vmalloc.c: check the addr first 12 O T 9月 28 akpm@linux-foun ( 56) + mm-vmscan-fix-comments-for-isolate_lru_page.patch added to -mm tree 13 O T 9月 28 akpm@linux-foun ( 59) + mm-vmallocc-update-the-comment-in-__vmalloc_area_node.patch added to -mm tree 14 O T 9月 28 akpm@linux-foun ( 66) + mm-vmallocc-fix-the-comment-of-find_vm_area.patch added to -mm tree 15 O T 9月 28 akpm@linux-foun ( 65) + mmz3fold-use-xx_zalloc-instead-xx_alloc-and-memset.patch added to -mm tree 16 O + 9月 29 haifei.wang@car ( 312) 阿里云/今日头条Linux内核研发专家职位推荐 17 O T 9月 29 Dietmar Eggeman ( 59) Re: [PATCH] sched,fair: use list_for_each_entry() in print_cfs_stats() 18 O + 9月 29 iovisor-dev@lis ( 378) [iovisor-dev] iovisor-dev@lists.iovisor.org Daily Summary 19 O T 9月 29 Steven Rostedt ( 61) Re: [PATCH] sched/rt.c remove unnecessary parameter in pick_next_rt_entity 20 O T 9月 30 boris.ostrovsky ( 13) Re: [PATCH] arch/x86: fix some typos in xen_pagetable_p2m_free() 21 O T 10月 01 David Hildenbra ( 57) Re: [PATCH] mm: fix some comments in page_alloc.c and mempolicy.c 22 O T 10月 01 Joe Perches ( 14) └─&gt; 23 O + 10月 01 iovisor-dev@lis ( 378) [iovisor-dev] iovisor-dev@lists.iovisor.org Daily Summary 24 O 10月 02 George Worden ( 1) 25 O T 10月 02 akpm@linux-foun ( 81) [to-be-updated] mm-fix-some-comments-in-page_allocc-and-mempolicyc.patch removed from -mm tree 26 O + 10月 08 流浪人 ( 75) Re:[PATCH] mm/hugetlb.c: just use put_page_testzero() instead of page_count()---Mutt: /var/mail/rlk [Msgs:26 Old:24 Post:8 174K]---(threads/date)---------------------------------------------------------------------------------------------------------------------------------(all)-- 回复邮件我的第一个patch 使用 git send-amail 发送邮件 安装 1apt-get install git-email 首先需要先配置 ~/.gitconfig 12345678910111213141516[user] name = Hui Su email = suhui@xxx.com[http] sslverify = false[https] sslverify = false[sendemail] smtpserver = smtp.office365.com smtpserverport = 587 smtpuser = suhui@xxx.com smtpencryption = tls smtppass = xxx 直接发送1git send-email /tmp/0001-test.patch --to suhui@xxx.com --cc suhui199712@gmail.com","link":"/2020/09/22/%E7%94%9F%E6%B4%BB%E6%84%9F%E6%82%9F/%E8%AE%B0%E5%BD%95%E7%AC%AC%E4%B8%80%E4%B8%AAkernel%20patch/"},{"title":"valgrind 定位用户空间内存泄漏","text":"WHY在实际开发中，某个应用程序如果存在内存泄露，且是长时间运行的程序，就会导致比较严重的后果。在一些业务场景的长跑测试中，这些问题往往会充分暴露出来。 一般情况下这种内存泄漏都是以 OOM Kill 而结尾的。 kmsg log 往往只有 这个进程确实消耗了大量进程的证据，但是无法确切知道是哪里的内存泄漏，这里就需要一个工具来帮助检测，如果是发生内存泄漏之后仅仅通过人肉去分析代码，往往很困难。 valgrind 就是这样一款 强大的工具： 检查用户空间内存泄漏 检查 valdrind 原理valdrind 安装，使用直接安装， ubuntu 官方软件源已经包含了，其他平台可以通过源码编译安装 1234567891011121314151617tencent_clould@100ubuntu: ~/workspace/test_modules/resource_leak/user_space_memleak# sudo apt install valgrindReading package lists... DoneBuilding dependency treeReading state information... DoneThe following packages were automatically installed and are no longer required: openbsd-inetd openjdk-11-jdk-headless tcpd update-inetdUse 'sudo apt autoremove' to remove them.Suggested packages: valgrind-dbg valgrind-mpi kcachegrind alleyoop valkyrieThe following NEW packages will be installed: valgrind0 upgraded, 1 newly installed, 0 to remove and 231 not upgraded.Need to get 20.3 MB of archives.After this operation, 90.0 MB of additional disk space will be used.Get:1 https://mirrors.tuna.tsinghua.edu.cn/ubuntu focal-updates/main amd64 valgrind amd64 1:3.15.0-1ubuntu9.1 [20.3 MB]48% [1 valgrind 12.2 MB/20.3 MB 60%]tencent_clould@ubuntu: ~/workspace/test_modules/resource_leak/user_space_memleak# ls 使用如下代码检测 123456789101112131415161718#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;unistd.h&gt;void main(void){ int i = 3; char *p = NULL; p = malloc(1024 * 1024); if (!p) printf(&quot;malloc failed,just wait!!\\n&quot;); printf(&quot;malloc sucess,just wait!!\\n&quot;); while(i--) { usleep(1000 * 1000); }} 对于 valgrind 不好的地方在于： 对于想使用 valgrind来检查内存泄漏的业务来说，必须从开始 用valgrind 启动，意味着对于业务需要重启。还有比如很难复现的内存泄漏，等你重启业务用 valgrind来启动，说不定又不复现了。。 对于 valgrind 启动的业务来说，会比直接启动有一些性能损失。 userspace memleak demo使用上面代码 直接用 valgrind 启动 12345678910111213141516171819202122tencent_clould@ubuntu: ~/workspace/test_modules/resource_leak/user_space_memleak# valgrind ./a.out==2813281== Memcheck, a memory error detector==2813281== Copyright (C) 2002-2017, and GNU GPL'd, by Julian Seward et al.==2813281== Using Valgrind-3.15.0 and LibVEX; rerun with -h for copyright info==2813281== Command: ./a.out==2813281==malloc sucess,just wait!!==2813281====2813281== HEAP SUMMARY:==2813281== in use at exit: 1,048,576 bytes in 1 blocks==2813281== total heap usage: 2 allocs, 1 frees, 1,049,600 bytes allocated==2813281====2813281== LEAK SUMMARY:==2813281== definitely lost: 1,048,576 bytes in 1 blocks==2813281== indirectly lost: 0 bytes in 0 blocks==2813281== possibly lost: 0 bytes in 0 blocks==2813281== still reachable: 0 bytes in 0 blocks==2813281== suppressed: 0 bytes in 0 blocks==2813281== Rerun with --leak-check=full to see details of leaked memory==2813281====2813281== For lists of detected and suppressed errors, rerun with: -s==2813281== ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 0 from 0) 可以看到 HEAP SUMMARY: 中写了 12==2813281== in use at exit: 1,048,576 bytes in 1 blocks==2813281== total heap usage: 2 allocs, 1 frees, 1,049,600 bytes allocated 在进程退出时，仍然有 1048576 bytes 内存在使用中，这部分就是泄漏的内存。但是我们仍然不能确定到底是哪里泄漏的内存，按照他的建议 加上 -leak-check=full 1234567891011121314151617181920212223242526tencent_clould@ubuntu: ~/workspace/test_modules/resource_leak/user_space_memleak# valgrind --leak-check=full ./a.out==2813434== Memcheck, a memory error detector==2813434== Copyright (C) 2002-2017, and GNU GPL'd, by Julian Seward et al.==2813434== Using Valgrind-3.15.0 and LibVEX; rerun with -h for copyright info==2813434== Command: ./a.out==2813434==malloc sucess,just wait!!==2813434====2813434== HEAP SUMMARY:==2813434== in use at exit: 1,048,576 bytes in 1 blocks==2813434== total heap usage: 2 allocs, 1 frees, 1,049,600 bytes allocated==2813434====2813434= 1,048,576 bytes in 1 blocks are definitely lost in loss record 1 of 1==2813434== at 0x483B7F3: malloc (in /usr/lib/x86_64-linux-gnu/valgrind/vgpreload_memcheck-amd64-linux.so)==2813434== by 0x1091AD: main (in /home/ubuntu/workspace/test_modules/resource_leak/user_space_memleak/a.out)==2813434====2813434== LEAK SUMMARY:==2813434== definitely lost: 1,048,576 bytes in 1 blocks==2813434== indirectly lost: 0 bytes in 0 blocks==2813434== possibly lost: 0 bytes in 0 blocks==2813434== still reachable: 0 bytes in 0 blocks==2813434== suppressed: 0 bytes in 0 blocks==2813434====2813434== For lists of detected and suppressed errors, rerun with: -s==2813434== ERROR SUMMARY: 1 errors from 1 contexts (suppressed: 0 from 0)tencent_clould@ubuntu: ~/workspace/test_modules/resource_leak/user_space_memleak# 可以看到这次， valgrind 已经将泄漏的具体位置打印了出来 123==2813434= 1,048,576 bytes in 1 blocks are definitely lost in loss record 1 of 1==2813434== at 0x483B7F3: malloc (in /usr/lib/x86_64-linux-gnu/valgrind/vgpreload_memcheck-amd64-linux.so)==2813434== by 0x1091AD: main (in /home/ubuntu/workspace/test_modules/resource_leak/user_space_memleak/a.out) 可以用 gcc -g 重新编译一下带上符号表信息，再次用 valgrind 定位，可以得到更详细信息。 12==2822695== at 0x483B7F3: malloc (in /usr/lib/x86_64-linux-gnu/valgrind/vgpreload_memcheck-amd64-linux.so)==2822695== by 0x1091AD: main (user_space_memleak.c:9) 这次直接将 在 哪个文件 哪一行都直接打印出来了。 out of bounds access demo代码： 12345678910111213141516171819202122#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;unistd.h&gt;void main(void){ int i = 3; char *p = NULL; p = malloc(1024); if (!p) printf(&quot;malloc failed,just wait!!\\n&quot;); printf(&quot;malloc sucess,just wait!!*(p + 10) = %d\\n&quot;, *(p + 10)); *(p + 1023) = *(p + 1024); *(p + 1024) = 1; while(i--) { usleep(1000 * 1000); } free(p); free(p + 1);} gcc -g 编译之偶 用 valgrind ./a.out 跑一下 123456789101112131415161718192021222324252627282930313233343536373839404142tencent_clould@ubuntu: ~/workspace/test_modules/resource_leak/valgrind# valgrind ./a.out==2826928== Memcheck, a memory error detector==2826928== Copyright (C) 2002-2017, and GNU GPL'd, by Julian Seward et al.==2826928== Using Valgrind-3.15.0 and LibVEX; rerun with -h for copyright info==2826928== Command: ./a.out==2826928====2833797== Use of uninitialised value of size 8==2833797== at 0x48B681B: _itoa_word (_itoa.c:179)==2833797== by 0x48D26F4: __vfprintf_internal (vfprintf-internal.c:1687)==2833797== by 0x48BCEBE: printf (printf.c:33)==2833797== by 0x109225: main (out_of_bounds_access.c:13)malloc sucess,just wait!!==2826928== Invalid read of size 1==2826928== at 0x109200: main (out_of_bounds_access.c:14)==2826928== Address 0x4a4d440 is 0 bytes after a block of size 1,024 alloc'd==2826928== at 0x483B7F3: malloc (in /usr/lib/x86_64-linux-gnu/valgrind/vgpreload_memcheck-amd64-linux.so)==2826928== by 0x1091CD: main (out_of_bounds_access.c:9)==2826928====2826928== Invalid write of size 1==2826928== at 0x109213: main (out_of_bounds_access.c:15)==2826928== Address 0x4a4d440 is 0 bytes after a block of size 1,024 alloc'd==2826928== at 0x483B7F3: malloc (in /usr/lib/x86_64-linux-gnu/valgrind/vgpreload_memcheck-amd64-linux.so)==2826928== by 0x1091CD: main (out_of_bounds_access.c:9)==2826928====2845980== Invalid free() / delete / delete[] / realloc()==2845980== at 0x483CA3F: free (in /usr/lib/x86_64-linux-gnu/valgrind/vgpreload_memcheck-amd64-linux.so)==2845980== by 0x10927F: main (out_of_bounds_access.c:21)==2845980== Address 0x4a4d041 is 1 bytes inside a block of size 1,024 free'd==2845980== at 0x483CA3F: free (in /usr/lib/x86_64-linux-gnu/valgrind/vgpreload_memcheck-amd64-linux.so)==2845980== by 0x10926F: main (out_of_bounds_access.c:20)==2845980== Block was alloc'd at==2845980== at 0x483B7F3: malloc (in /usr/lib/x86_64-linux-gnu/valgrind/vgpreload_memcheck-amd64-linux.so)==2845980== by 0x1091ED: main (out_of_bounds_access.c:9)==2826928====2826928== HEAP SUMMARY:==2826928== in use at exit: 0 bytes in 0 blocks==2826928== total heap usage: 2 allocs, 3 frees, 2,048 bytes allocated==2826928====2826928== All heap blocks were freed -- no leaks are possible==2826928====2826928== For lists of detected and suppressed errors, rerun with: -s==2826928== ERROR SUMMARY: 2 errors from 2 contexts (suppressed: 0 from 0) 也是直接指出了 一次引用未初始化的内存, 两次越界访问, 一次非法 free： 一次引用为初始化的内存 12345==2833797== Use of uninitialised value of size 8==2833797== at 0x48B681B: _itoa_word (_itoa.c:179)==2833797== by 0x48D26F4: __vfprintf_internal (vfprintf-internal.c:1687)==2833797== by 0x48BCEBE: printf (printf.c:33)==2833797== by 0x109225: main (out_of_bounds_access.c:13) 一次越界读访问： 12345==2826928== Invalid read of size 1==2826928== at 0x109200: main (out_of_bounds_access.c:14)==2826928== Address 0x4a4d440 is 0 bytes after a block of size 1,024 alloc'd==2826928== at 0x483B7F3: malloc (in /usr/lib/x86_64-linux-gnu/valgrind/vgpreload_memcheck-amd64-linux.so)==2826928== by 0x1091CD: main (out_of_bounds_access.c:9) 一次越界写访问 12345==2826928== Invalid write of size 1==2826928== at 0x109213: main (out_of_bounds_access.c:15)==2826928== Address 0x4a4d440 is 0 bytes after a block of size 1,024 alloc'd==2826928== at 0x483B7F3: malloc (in /usr/lib/x86_64-linux-gnu/valgrind/vgpreload_memcheck-amd64-linux.so)==2826928== by 0x1091CD: main (out_of_bounds_access.c:9) 一次 invaild free 123456789==2845980== Invalid free() / delete / delete[] / realloc()==2845980== at 0x483CA3F: free (in /usr/lib/x86_64-linux-gnu/valgrind/vgpreload_memcheck-amd64-linux.so)==2845980== by 0x10927F: main (out_of_bounds_access.c:21)==2845980== Address 0x4a4d041 is 1 bytes inside a block of size 1,024 free'd==2845980== at 0x483CA3F: free (in /usr/lib/x86_64-linux-gnu/valgrind/vgpreload_memcheck-amd64-linux.so)==2845980== by 0x10926F: main (out_of_bounds_access.c:20)==2845980== Block was alloc'd at==2845980== at 0x483B7F3: malloc (in /usr/lib/x86_64-linux-gnu/valgrind/vgpreload_memcheck-amd64-linux.so)==2845980== by 0x1091ED: main (out_of_bounds_access.c:9) 总结实际项目中应用的内存泄漏往往比这要复杂很多，可能代码考虑了正常情况下的 free，异常情况下没有free等， valgrind 是很强大，有很多参数，但也只能帮助我们在出现问题之后定位，还是需要养成良好的编码习惯，减少杜绝这类问题 参考文章1参考文章2","link":"/2021/01/17/%E7%94%A8%E6%88%B7%E7%A9%BA%E9%97%B4/valgrind%20%E5%AE%9A%E4%BD%8D%E7%94%A8%E6%88%B7%E7%A9%BA%E9%97%B4%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F/"},{"title":"KASAN 定位越界访问","text":"KASAN 配置首先想使用这类根据肯定是需要重新编译内核的。看下.config的改变 123456789101112131415if [ $debug_KASAN == 1 ]then## KASAN detect and panic start #---- add echo &quot;CONFIG_CONSTRUCTORS=y&quot; &gt;&gt; /tmp/.config echo &quot;CONFIG_KASAN_SHADOW_OFFSET=0xdffffc0000000000&quot; &gt;&gt; /tmp/.config echo &quot;CONFIG_STACKDEPOT=y&quot; &gt;&gt; /tmp/.config echo &quot;CONFIG_KASAN=y&quot; &gt;&gt; /tmp/.config echo &quot;CONFIG_KASAN_GENERIC=y&quot; &gt;&gt; /tmp/.config echo &quot;CONFIG_KASAN_OUTLINE=y&quot; &gt;&gt; /tmp/.config echo &quot;CONFIG_KASAN_STACK=1&quot; &gt;&gt; /tmp/.config #---- delect sed -i '/CONFIG_VMAP_STACK=y/d' /tmp/.config## KASAN detect and panic endfi 当然，我在别的地方将$debug_slub_debug也已经打开了. KASAN 使用写了一个test case，参考代码 oob检测编译安装模块之后，直接panic了(我设置了panic_on_warn。。)，只能通过 crash 根据查看 现场的 dmesg log 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869crash&gt; log | tail -n 70==================================================================[ 114.816452] BUG: KASAN: slab-out-of-bounds in kasan_debug_oob+0x35/0x70 [kasan_debug][ 114.816452] Write of size 1 at addr ffff8880076053c0 by task kasan_debug/3229[ 114.816452][ 114.816452] CPU: 1 PID: 3229 Comm: kasan_debug Kdump: loaded Tainted: G O 5.11.0-rc5+ #6[ 114.816452] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS 1.13.0-1ubuntu1.1 04/01/2014[ 114.816452] Call Trace:[ 114.816452] dump_stack+0x9a/0xcc[ 114.816452] ? kasan_debug_oob+0x35/0x70 [kasan_debug][ 114.816452] print_address_description.constprop.0+0x1a/0x140[ 114.816452] ? kasan_debug_oob+0x35/0x70 [kasan_debug][ 114.816452] ? kasan_debug_oob+0x35/0x70 [kasan_debug][ 114.816452] kasan_report.cold+0x7f/0x10e[ 114.816452] ? ____kasan_kmalloc.constprop.0+0x1/0xa0[ 114.816452] ? kasan_debug_oob+0x35/0x70 [kasan_debug][ 114.816452] ? 0xffffffffc0880000[ 114.816452] kasan_debug_oob+0x35/0x70 [kasan_debug][ 114.816452] kthread+0x1aa/0x200[ 114.816452] ? kthread_create_on_node+0xd0/0xd0[ 114.816452] ret_from_fork+0x22/0x30[ 114.816452][ 114.816452] Allocated by task 3229:[ 114.816452] kasan_save_stack+0x1b/0x40[ 114.816452] ____kasan_kmalloc.constprop.0+0x84/0xa0[ 114.816452] kasan_debug_oob+0x29/0x70 [kasan_debug][ 114.816452] kthread+0x1aa/0x200[ 114.816452] ret_from_fork+0x22/0x30[ 114.816452][ 114.816452] The buggy address belongs to the object at ffff8880076053a0 which belongs to the cache kmalloc-32 of size 32[ 114.816452] The buggy address is located 0 bytes to the right of 32-byte region [ffff8880076053a0, ffff8880076053c0)[ 114.816452] The buggy address belongs to the page:[ 114.816452] page:00000000352cff66 refcount:1 mapcount:0 mapping:0000000000000000 index:0x0 pfn:0x7604[ 114.816452] head:00000000352cff66 order:1 compound_mapcount:0[ 114.816452] flags: 0x100000000010200(slab|head)[ 114.816452] raw: 0100000000010200 ffffea0000441688 ffffea0000466d88 ffff888001042540[ 114.816452] raw: 0000000000000000 0000000000130013 00000001ffffffff 0000000000000000[ 114.816452] page dumped because: kasan: bad access detected[ 114.816452][ 114.816452] Memory state around the buggy address:[ 114.816452] ffff888007605280: fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc[ 114.816452] ffff888007605300: fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc[ 114.816452] &gt;ffff888007605380: fc fc fc fc 00 00 00 00 fc fc fc fc fc fc fc fc[ 114.816452] ^[ 114.816452] ffff888007605400: fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc[ 114.816452] ffff888007605480: fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc[ 114.816452] ==================================================================[ 114.816452] Disabling lock debugging due to kernel taint[ 114.833983] Kernel panic - not syncing: panic_on_warn set ...[ 114.834833] CPU: 1 PID: 3229 Comm: kasan_debug Kdump: loaded Tainted: G B O 5.11.0-rc5+ #6[ 114.835925] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS 1.13.0-1ubuntu1.1 04/01/2014[ 114.836979] Call Trace:[ 114.836979] dump_stack+0x9a/0xcc[ 114.836979] ? 0xffffffffc0880000[ 114.836979] panic+0x1ae/0x3c3[ 114.836979] ? print_oops_end_marker.cold+0x10/0x10[ 114.839656] ? kasan_debug_oob+0x35/0x70 [kasan_debug][ 114.840336] ? kasan_debug_oob+0x35/0x70 [kasan_debug][ 114.841598] end_report+0x58/0x5e[ 114.841598] kasan_report.cold+0x9d/0x10e[ 114.841598] ? ____kasan_kmalloc.constprop.0+0x1/0xa0[ 114.841598] ? kasan_debug_oob+0x35/0x70 [kasan_debug][ 114.841598] ? 0xffffffffc0880000[ 114.841598] kasan_debug_oob+0x35/0x70 [kasan_debug][ 114.841598] kthread+0x1aa/0x200[ 114.841598] ? kthread_create_on_node+0xd0/0xd0[ 114.841598] ret_from_fork+0x22/0x30 可以看到报错信息很明显 123456[ 114.816452] BUG: KASAN: slab-out-of-bounds in kasan_debug_oob+0x35/0x70 [kasan_debug][ 114.816452] Write of size 1 at addr ffff8880076053c0 by task kasan_debug/3229[ 114.816452] The buggy address belongs to the object at ffff8880076053a0 which belongs to the cache kmalloc-32 of size 32[ 114.816452] The buggy address is located 0 bytes to the right of 32-byte region [ffff8880076053a0, ffff8880076053c0) 基本dis反汇编一下就能看到出错逻辑 use after free检测使用kasan_debug_use_after_free（）重新编译安装（这次我关掉了panic_on_warn）,报错信息如下 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263[ 98.719080] ==================================================================[ 98.719793] BUG: KASAN: use-after-free in kasan_debug_use_after_free+0x109/0x180 [kasan_debug][ 98.719793] Write of size 1 at addr ffff8880432ce85e by task kasan_debug/3203[ 98.719793] CPU: 0 PID: 3203 Comm: kasan_debug Kdump: loaded Tainted: G O 5.11.0-rc5+ #6[ 98.719793] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS 1.13.0-1ubuntu1.1 04/01/2014[ 98.719793] Call Trace:[ 98.719793] dump_stack+0x9a/0xcc[ 98.719793] ? kasan_debug_use_after_free+0x109/0x180 [kasan_debug][ 98.719793] print_address_description.constprop.0+0x1a/0x140[ 98.719793] ? kasan_debug_use_after_free+0x109/0x180 [kasan_debug][ 98.719793] ? kasan_debug_use_after_free+0x109/0x180 [kasan_debug][ 98.719793] kasan_report.cold+0x7f/0x10e[ 98.719793] ? kasan_debug_use_after_free+0x109/0x180 [kasan_debug][ 98.719793] kasan_debug_use_after_free+0x109/0x180 [kasan_debug][ 98.719793] ? 0xffffffffc07b8000[ 98.719793] ? mark_held_locks+0x24/0x90[ 98.719793] ? lockdep_hardirqs_on_prepare+0x12e/0x1f0[ 98.719793] ? _raw_spin_unlock_irqrestore+0x34/0x40[ 98.719793] ? trace_hardirqs_on+0x1c/0x100[ 98.719793] ? 0xffffffffc07b8000[ 98.719793] kthread+0x1aa/0x200[ 98.719793] ? kthread_create_on_node+0xd0/0xd0[ 98.719793] ret_from_fork+0x22/0x30[ 98.719793] Allocated by task 3203:[ 98.719793] kasan_save_stack+0x1b/0x40[ 98.719793] ____kasan_kmalloc.constprop.0+0x84/0xa0[ 98.719793] kasan_debug_use_after_free+0xb8/0x180 [kasan_debug][ 98.719793] kthread+0x1aa/0x200[ 98.719793] ret_from_fork+0x22/0x30[ 98.719793] Freed by task 3203:[ 98.719793] kasan_save_stack+0x1b/0x40[ 98.719793] kasan_set_track+0x1c/0x30[ 98.719793] kasan_set_free_info+0x20/0x30[ 98.719793] ____kasan_slab_free+0xec/0x120[ 98.719793] kfree+0xcc/0x2e0[ 98.719793] kasan_debug_use_after_free+0xf6/0x180 [kasan_debug][ 98.719793] kthread+0x1aa/0x200[ 98.719793] ret_from_fork+0x22/0x30[ 98.719793] The buggy address belongs to the object at ffff8880432ce840 which belongs to the cache kmalloc-32 of size 32[ 98.719793] The buggy address is located 30 bytes inside of 32-byte region [ffff8880432ce840, ffff8880432ce860)[ 98.719793] The buggy address belongs to the page:[ 98.719793] page:00000000d04b265b refcount:1 mapcount:0 mapping:0000000000000000 index:0xffff8880432ce1c0 pfn:0x432ce[ 98.719793] head:00000000d04b265b order:1 compound_mapcount:0[ 98.719793] flags: 0x100000000010200(slab|head)[ 98.719793] raw: 0100000000010200 ffffea0000720f08 ffff888001041b88 ffff888001042540[ 98.719793] raw: ffff8880432ce1c0 0000000000130011 00000001ffffffff 0000000000000000[ 98.719793] page dumped because: kasan: bad access detected[ 98.719793] Memory state around the buggy address:[ 98.719793] ffff8880432ce700: fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc[ 98.719793] ffff8880432ce780: fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc[ 98.719793] &gt;ffff8880432ce800: fc fc fc fc fc fc fc fc fa fb fb fb fc fc fc fc[ 98.719793] ^[ 98.719793] ffff8880432ce880: fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc[ 98.719793] ffff8880432ce900: fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc[ 98.719793] ==================================================================[ 98.719793] Disabling lock debugging due to kernel taint 可以看到报错信息 1234567[ 98.719793] BUG: KASAN: use-after-free in kasan_debug_use_after_free+0x109/0x180 [kasan_debug][ 98.719793] Allocated by task 3203:[ 98.719793] Freed by task 3203:[ 98.719793] The buggy address belongs to the object at ffff8880432ce840 which belongs to the cache kmalloc-32 of size 32[ 98.719793] The buggy address is located 30 bytes inside of 32-byte region [ffff8880432ce840, ffff8880432ce860) 直接反汇编，基本直接可以找到问题 double free检测使用kasan_debug_double_free重新编译安装（这次我关掉了panic_on_warn）,报错信息如下 123456789101112131415161718192021222324252627[ 80.381499] ==================================================================[ 80.508872] ==================================================================[ 80.509767] BUG: KASAN: double-free or invalid-free in kasan_debug_double_free+0x119/0x180 [kasan_debug][ 80.509767] CPU: 3 PID: 3407 Comm: kasan_debug Kdump: loaded Tainted: G B O 5.11.0-rc5+ #6[ 80.509767] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS 1.13.0-1ubuntu1.1 04/01/2014[ 80.509767] Call Trace:......[ 80.509767] The buggy address belongs to the object at ffff888010e3b540 which belongs to the cache kmalloc-32 of size 32[ 80.509767] The buggy address is located 0 bytes inside of 32-byte region [ffff888010e3b540, ffff888010e3b560)[ 80.509767] The buggy address belongs to the page:[ 80.509767] page:000000001edff74e refcount:1 mapcount:0 mapping:0000000000000000 index:0x0 pfn:0x10e3a[ 80.509767] head:000000001edff74e order:1 compound_mapcount:0[ 80.509767] flags: 0x100000000010200(slab|head)[ 80.509767] raw: 0100000000010200 ffffea0000955888 ffff888001041ba8 ffff888001042540[ 80.509767] raw: 0000000000000000 0000000000130013 00000001ffffffff 0000000000000000[ 80.509767] page dumped because: kasan: bad access detected[ 80.509767] Memory state around the buggy address:[ 80.509767] ffff888010e3b400: fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc[ 80.509767] ffff888010e3b480: fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc[ 80.509767] &gt;ffff888010e3b500: fc fc fc fc fc fc fc fc fa fb fb fb fc fc fc fc[ 80.509767] ^[ 80.509767] ffff888010e3b580: fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc[ 80.509767] ffff888010e3b600: fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc fc 可以看到报错信息 12345[ 80.509767] BUG: KASAN: double-free or invalid-free in kasan_debug_double_free+0x119/0x180 [kasan_debug][ 80.509767] The buggy address belongs to the object at ffff888010e3b540 which belongs to the cache kmalloc-32 of size 32[ 80.509767] The buggy address is located 0 bytes inside of 32-byte region [ffff888010e3b540, ffff888010e3b560) 根据这提示信息和 backtrace，反汇编一下基本就可以找到问题所在 vmalloc 内存检测由于 vmalloc存在的 guard page的原因，基本上只要 vmalloc内存存在越界的问题，就会立即报出来后面加上 guard page分析。 KASAN overhead当然假设我们场景允许 使能KASAN之后，重新编译 kernel，也要关系 KASAN 所带来的overhead是否允许。 关于使能 KASAN的 overhead: image 变大on1234amd_server@ubuntu: ~/workspace/share/debug# ls -alhtotal 1.1G-rw-r--r-- 1 root root 17M 1月 26 21:56 bzImage-rwxr-xr-x 1 root root 1.1G 1月 26 21:56 vmlinux off 1234amd_server@ubuntu: ~/workspace/share/stable# ls -alhtotal 970M-rw-r--r-- 1 root root 9.6M 1月 26 11:05 bzImage-rwxr-xr-x 1 root root 967M 1月 26 11:05 vmlinux 运行速度原来我自己qemu虚拟机开机12s，开启 kasan 之后开机时间是 20s. cpu使用率 1on off 我的qemu虚拟机看起来好像区别不大，后面需要用公司机器看一下 内存使用我的qemu上（2G）on 1234stable_kernel@kernel: /tmp/share/test_modules/memory/kasan_debug# free -m total used free shared buff/cache availableMem: 1419 783 44 6 591 592Swap: 1497 0 1496 总结KASAN 和 slub_debug一样也可以使由于内存越界的检查。但是原理上截然不同，后续等搞明白原理再分析一波。","link":"/2021/01/27/%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86/KASAN%20%E4%BD%BF%E7%94%A8/"},{"title":"kmemleak 定位内存泄露","text":"kmemleak 原理Basic AlgorithmThe memory allocations via kmalloc(), vmalloc(), kmem_cache_alloc() and friends are traced and the pointers, together with additional information like size and stack trace, are stored in a rbtree. The corresponding freeing function calls are tracked and the pointers removed from the kmemleak data structures. An allocated block of memory is considered orphan if no pointer to its start address or to any location inside the block can be found by scanning the memory (including saved registers). This means that there might be no way for the kernel to pass the address of the allocated block to a freeing function and therefore the block is considered a memory leak. The scanning algorithm steps: mark all objects as white (remaining white objects will later be considered orphan)scan the memory starting with the data section and stacks, checking the values against the addresses stored in the rbtree. If a pointer to a white object is found, the object is added to the gray listscan the gray objects for matching addresses (some white objects can become gray and added at the end of the gray list) until the gray set is finishedthe remaining white objects are considered orphan and reported via /sys/kernel/debug/kmemleakSome allocated memory blocks have pointers stored in the kernel’s internal data structures and they cannot be detected as orphans. To avoid this, kmemleak can also store the number of values pointing to an address inside the block address range that need to be found so that the block is not considered a leak. One example is __vmalloc(). kmemleak 使用kmemleak 功能一开始默认是不开启的，需要配置如下选项 123456789if [ $debug_kmemleak == 1 ]then## kmemleak detect and panic start echo &quot;CONFIG_DEBUG_KMEMLEAK=y&quot; &gt;&gt; /tmp/.config echo &quot;CONFIG_DEBUG_KMEMLEAK_MEM_POOL_SIZE=16000&quot; &gt;&gt; /tmp/.config echo &quot;CONFIG_DEBUG_KMEMLEAK_AUTO_SCAN=y&quot; &gt;&gt; /tmp/.config echo &quot;CONFIG_BOOTPARAM_HUNG_TASK_PANIC_VALUE=1&quot; &gt;&gt; /tmp/.config## kmemleak detect and panic endfi 参考test code 一开始遇到了无法安装的问题，后来发现是因为模块名 一样导致的问题。。参考bug 触发开始扫描，这是个同步过程，内存比较大的话可能比较耗时 12root@rlk-Standard-PC-i440FX-PIIX-1996:/sys/kernel/debug# echo scan &gt; /sys/kernel/debug/kmemleakroot@rlk-Standard-PC-i440FX-PIIX-1996:/sys/kernel/debug# 查看扫描结果 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556root@rlk-Standard-PC-i440FX-PIIX-1996:/sys/kernel/debug# cat kmemleakunreferenced object 0xffff8da0cac1c500 (size 32): comm &quot;insmod&quot;, pid 3529, jiffies 4294873584 (age 562.798s) hex dump (first 32 bytes): 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b kkkkkkkkkkkkkkkk 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b a5 kkkkkkkkkkkkkkk. backtrace: [&lt;00000000ae5c9724&gt;] 0xffffffffc030702d [&lt;00000000afc3b54e&gt;] do_one_initcall+0x56/0x2b0 [&lt;000000000724192e&gt;] do_init_module+0x56/0x200 [&lt;00000000996ecfff&gt;] load_module+0x2348/0x26e0 [&lt;000000004fa63e1a&gt;] __do_sys_finit_module+0xa0/0xe0 [&lt;00000000cddcb6e5&gt;] do_syscall_64+0x33/0x40 [&lt;00000000a0266b85&gt;] entry_SYSCALL_64_after_hwframe+0x44/0xa9unreferenced object 0xffff8da0d2371000 (size 1024): comm &quot;insmod&quot;, pid 3529, jiffies 4294873584 (age 562.798s) hex dump (first 32 bytes): 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b kkkkkkkkkkkkkkkk 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b kkkkkkkkkkkkkkkk backtrace: [&lt;0000000079fd2c9c&gt;] 0xffffffffc030709c [&lt;00000000afc3b54e&gt;] do_one_initcall+0x56/0x2b0 [&lt;000000000724192e&gt;] do_init_module+0x56/0x200 [&lt;00000000996ecfff&gt;] load_module+0x2348/0x26e0 [&lt;000000004fa63e1a&gt;] __do_sys_finit_module+0xa0/0xe0 [&lt;00000000cddcb6e5&gt;] do_syscall_64+0x33/0x40 [&lt;00000000a0266b85&gt;] entry_SYSCALL_64_after_hwframe+0x44/0xa9unreferenced object 0xffff8da0ca534000 (size 4096): comm &quot;insmod&quot;, pid 3529, jiffies 4294873584 (age 562.862s) hex dump (first 32 bytes): 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b kkkkkkkkkkkkkkkk 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b kkkkkkkkkkkkkkkk backtrace: [&lt;000000000526120c&gt;] 0xffffffffc0307130 [&lt;00000000afc3b54e&gt;] do_one_initcall+0x56/0x2b0 [&lt;000000000724192e&gt;] do_init_module+0x56/0x200 [&lt;00000000996ecfff&gt;] load_module+0x2348/0x26e0 [&lt;000000004fa63e1a&gt;] __do_sys_finit_module+0xa0/0xe0 [&lt;00000000cddcb6e5&gt;] do_syscall_64+0x33/0x40 [&lt;00000000a0266b85&gt;] entry_SYSCALL_64_after_hwframe+0x44/0xa9unreferenced object 0xffffaa50801fb000 (size 4096): comm &quot;insmod&quot;, pid 3529, jiffies 4294873584 (age 562.862s) hex dump (first 32 bytes): 27 94 40 81 50 aa ff ff 27 04 00 00 00 00 00 00 '.@.P...'....... 7c 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 |............... backtrace: [&lt;00000000854ee6a4&gt;] __vmalloc_node_range+0x236/0x270 [&lt;00000000a355227b&gt;] __vmalloc_node+0x3f/0x60 [&lt;0000000039b16a7e&gt;] 0xffffffffc0307149 [&lt;00000000afc3b54e&gt;] do_one_initcall+0x56/0x2b0 [&lt;000000000724192e&gt;] do_init_module+0x56/0x200 [&lt;00000000996ecfff&gt;] load_module+0x2348/0x26e0 [&lt;000000004fa63e1a&gt;] __do_sys_finit_module+0xa0/0xe0 [&lt;00000000cddcb6e5&gt;] do_syscall_64+0x33/0x40 [&lt;00000000a0266b85&gt;] entry_SYSCALL_64_after_hwframe+0x44/0xa9root@rlk-Standard-PC-i440FX-PIIX-1996:/sys/kernel/debug# 可以比较清楚的看到 可能leak的 object点，且有详细调用栈，排查起来十分方便 清除之前扫描结果 123root@rlk-Standard-PC-i440FX-PIIX-1996:/sys/kernel/debug# echo clear &gt; /sys/kernel/debug/kmemleakroot@rlk-Standard-PC-i440FX-PIIX-1996:/sys/kernel/debug# cat kmemleakroot@rlk-Standard-PC-i440FX-PIIX-1996:/sys/kernel/debug# kmemleak 代码后续填坑 kmemleak overhead当然假设我们场景允许 使能kmemleak之后，重新编译 kernel，也要关心 kmemleak 所带来的overhead是否允许，如果你的场景本来就是一个高负载的已经80%多的CPU使用率了，然后开启kmemleak,是很可能出问题的。 在 公司 一款三核心的 Cortex-A7 的产品中测试结果： disabled enabled Per Core增加% 换算成单核CPU%User: 3.54% 7.63% 4.09% 12.27%Sys: 10.68% 23.76% 13.08% 38.7%Idle: 84.6% 67.68% 17.02% -50.76% 在我自己 qemu 虚拟机中测试结果是： disabled enabled Per Core增加% 换算成单核CPU% User: 3.54% 7.63% 12.27%Sys: 10.68% 23.76% 38.7%Idle: 84.6% 67.68% -50.76% 参考内核文档","link":"/2021/01/16/%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86/kmemleak%20%E5%AE%9A%E4%BD%8D%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F/"},{"title":"page_owner 定位 buddy 内存泄漏","text":"page_owner 原理主要是通过给 每个分配出去的page 记录调用栈 page_owner 使用从mm/makefile 看，需要如下配置 12345678if [ $debug_page_owner == 1 ]then## page_owner detect and panic start echo &quot;CONFIG_STACKDEPOT=y&quot; &gt;&gt; /tmp/.config echo &quot;CONFIG_PAGE_EXTENSION=y&quot; &gt;&gt; /tmp/.config echo &quot;CONFIG_PAGE_OWNER=y&quot; &gt;&gt; /tmp/.config## page_owner detect and panic endfi 重新编译之后，启动qemu, 需要加上 page_owner=on: 123456789101112sudo qemu-system-x86_64 \\ -kernel /tmp/bzImage \\ -hda /home/ubuntu/myspace/qemu_build/stable_ubuntu.img \\ -append &quot;root=/dev/sda5 console=ttyS0 crashkernel=256M page_owner=on&quot; \\ -smp 4 \\ -m 2048 \\ --enable-kvm \\ -net nic \\ -net user,hostfwd=tcp::2222-:22 \\ --nographic \\ -fsdev local,id=fs1,path=/home/ubuntu/workspace/share,security_model=none \\ -device virtio-9p-pci,fsdev=fs1,mount_tag=host_share 查看 /sys/kernel/debug/page_owner 123root@rlk-Standard-PC-i440FX-PIIX-1996:/sys/kernel/debug# cat page_owner | wc -l4363829root@rlk-Standard-PC-i440FX-PIIX-1996:/sys/kernel/debug# 记录的信息非常多，然后需要一些工具来帮助检查 1234567891011root@rlk-Standard-PC-i440FX-PIIX-1996:/sys/kernel/debug# cat page_owner &gt; /home/rlk/page_owner.txtroot@rlk-Standard-PC-i440FX-PIIX-1996:/sys/kernel/debug# cp /tmp/share/test_modules/resource_leak/page_alloc_leak/page_owner_sort /usr/bin/stable_kernel@kernel: ~# page_owner_sort ./page_owner.txt sorted_page_owner.txtloaded 398724sorting ....cullingstable_kernel@kernel: ~# cat sorted_page_owner.txt| wc -l5470295stable_kernel@kernel: ~# cat page_owner.txt| wc -l5472954stable_kernel@kernel: ~# 看一下详细信息，order mask详细信息都有，对于debug 页面内存泄露用处很大。 123456789101112131415161718192021222324252627282930313233stable_kernel@kernel: ~# cat page_owner.txt | head -n 80Page allocated via order 2, mask 0xd20c1(GFP_DMA|__GFP_IO|__GFP_FS|__GFP_NOWARN|__GFP_NORETRY|__GFP_COMP|__GFP_NOMEMALLOC), pid 62, ts 121463164sPFN 1024 type Unmovable Block 2 type Unmovable Flags 0x10200(slab|head) prep_new_page+0xcf/0xf0 get_page_from_freelist+0xd8a/0x1230 __alloc_pages_nodemask+0x170/0x330 allocate_slab+0x24f/0x2f0 ___slab_alloc+0x480/0x6b0 __slab_alloc+0x50/0x60 kmem_cache_alloc_trace+0x1fb/0x230 sr_probe+0x213/0x5e0 really_probe+0xd6/0x2e0 driver_probe_device+0x4a/0xa0 bus_for_each_drv+0x7c/0xc0 __device_attach+0xe8/0x150 bus_probe_device+0x9a/0xb0 device_add+0x39b/0x850 scsi_sysfs_add_sdev+0x89/0x280 scsi_probe_and_add_lun+0x81e/0xb90Page allocated via order 0, mask 0x0(), pid 1, ts 285099088 nsPFN 4096 type Unmovable Block 8 type Unmovable Flags 0x100000000000000() register_early_stack+0x23/0x60 init_page_owner+0x27/0x290 kernel_init_freeable+0x158/0x273 kernel_init+0x5/0x101Page allocated via order 0, mask 0x0(), pid 1, ts 285099128 nsPFN 4097 type Unmovable Block 8 type Unmovable Flags 0x100000000000000() register_early_stack+0x23/0x60 init_page_owner+0x27/0x290 kernel_init_freeable+0x158/0x273 kernel_init+0x5/0x101 page_owner 代码总结这个工具 运行 overhead较大，在线上可能不能开启，只能在debug时开启。 参考内核文档参考LWN 文章","link":"/2021/01/16/%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86/page_owner%20%E5%AE%9A%E4%BD%8D%20buddy%20%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F/"},{"title":"slub_debug 定位内存越界","text":"slub_debug 概述首先需要使能，一般情况下我们系统都是使用 slub的。所以只需要打开debug选项即可 123456if [ $debug_slubdebug == 1 ]then## slubdebug detect start echo &quot;CONFIG_SLUB_DEBUG_ON=y&quot; &gt;&gt; /tmp/.config## slubdebug detect endfi 写了一个test case,参考代码 与 KASAN 可以自动检测出 内存越界，然后报错不同， slub_debug 需要主动触发检测，这里需要使用到slabinfo 这个工具（如果是很快释放的slab,其实不需要 slabinfo），源码在 ./tools/vm/ 目录下，可以直接编译得到： 123456789101112131415161718192021222324252627jenkins@server:~/workspace/linux_kernel_ltp/tools/vm$ makemake -C ../lib/apimake[1]: Entering directory '/var/lib/jenkins/workspace/linux_kernel_ltp/tools/lib/api'make -C /var/lib/jenkins/workspace/linux_kernel_ltp/tools/build CFLAGS= LDFLAGS= fixdep HOSTCC fixdep.o HOSTLD fixdep-in.o LINK fixdep CC fd/array.o LD fd/libapi-in.o CC fs/fs.o CC fs/tracing_path.o CC fs/cgroup.o LD fs/libapi-in.o CC cpu.o CC debug.o CC str_error_r.o LD libapi-in.o AR libapi.amake[1]: Leaving directory '/var/lib/jenkins/workspace/linux_kernel_ltp/tools/lib/api'gcc -Wall -Wextra -I../lib/ -o page-types page-types.c ../lib/api/libapi.agcc -Wall -Wextra -I../lib/ -o slabinfo slabinfo.c ../lib/api/libapi.agcc -Wall -Wextra -I../lib/ -o page_owner_sort page_owner_sort.c ../lib/api/libapi.ajenkins@server:~/workspace/linux_kernel_ltp/tools/vm$ ls -al | grep slab-rwxrwxr-x 1 jenkins jenkins 50416 1月 26 13:12 slabinfo-rw-rw-r-- 1 jenkins jenkins 37868 12月 20 22:58 slabinfo.c-rw-rw-r-- 1 jenkins jenkins 4817 12月 20 22:58 slabinfo-gnuplot.shjenkins@server:~/workspace/linux_kernel_ltp/tools/vm$ slabinfo 也有多个功能，主动触发 合法性检查slabinfo -v 只是一项，还有比如 123456789101112131415161718192021222324252627stable_kernel@kernel: /tmp/share/test_modules/memory/slub_debug# ./slabinfo --helpslabinfo 4/15/2011. (c) 2007 sgi/(c) 2011 Linux Foundation.slabinfo [-aABDefhilLnoPrsStTUvXz1] [N=K] [-dafzput] [slab-regexp]-a|--aliases Show aliases-A|--activity Most active slabs first-B|--Bytes Show size in bytes-D|--display-active Switch line format to activity-e|--empty Show empty slabs-f|--first-alias Show first alias-h|--help Show usage information-i|--inverted Inverted list-l|--slabs Show slabs-L|--Loss Sort by loss-n|--numa Show NUMA information-N|--lines=K Show the first K slabs-o|--ops Show kmem_cache_ops-P|--partial Sort by number of partial slabs-r|--report Detailed report on single slabs-s|--shrink Shrink slabs-S|--Size Sort by size-t|--tracking Show alloc/free information-T|--Totals Show summary information-U|--Unreclaim Show unreclaimable slabs only-v|--validate Validate slabs-X|--Xtotals Show extended summary information...... slab debug使用right redzone oob(out of bounds)使用这个函数 kslub_debug_right()编译安装 上面test case之后，主动 sudo ./slabinfo -v 触发合法性检查。可以看到如下输出 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364stable_kernel@130kernel: /tmp/share/test_modules/memory/slub_debug# sudo ./slabinfo -v[ 53.423227] =============================================================================[ 53.424083] BUG kmalloc-32 (Tainted: G O ): Redzone overwritten[ 53.424083] -----------------------------------------------------------------------------[ 53.424083][ 53.424083] INFO: 0x00000000de38e5d6-0x00000000de38e5d6 @offset=64. First byte 0x61 instead of 0xcc[ 53.424083] INFO: Allocated in kslub_debug_right+0x1c/0x50 [slub_debug] age=18928 cpu=2 pid=3159[ 53.424083] __slab_alloc+0x50/0x60[ 53.424083] kmem_cache_alloc_trace+0x1fb/0x230[ 53.424083] kslub_debug_right+0x1c/0x50 [slub_debug][ 53.424083] kthread+0x10a/0x140[ 53.424083] ret_from_fork+0x22/0x30[ 53.424083] INFO: Freed in security_cred_free+0x37/0x50 age=18938 cpu=1 pid=0[ 53.424083] security_cred_free+0x37/0x50[ 53.424083] put_cred_rcu+0x22/0x80[ 53.424083] rcu_core+0x25a/0xaa0[ 53.424083] __do_softirq+0xc7/0x419[ 53.424083] asm_call_irq_on_stack+0x12/0x20[ 53.424083] do_softirq_own_stack+0x56/0x60[ 53.424083] irq_exit_rcu+0xa9/0xb0[ 53.424083] sysvec_apic_timer_interrupt+0x43/0xa0[ 53.424083] asm_sysvec_apic_timer_interrupt+0x12/0x20[ 53.424083] default_idle+0xe/0x10[ 53.424083] default_idle_call+0x63/0x1e0[ 53.424083] do_idle+0x1e5/0x240[ 53.424083] cpu_startup_entry+0x14/0x20[ 53.424083] secondary_startup_64_no_verify+0xc2/0xcb[ 53.424083] INFO: Slab 0x00000000831bf412 objects=19 used=18 fp=0x0000000083bc545d flags=0x100000000010201[ 53.424083] INFO: Object 0x00000000f71a6b67 @offset=32 fp=0x000000000629ace8[ 53.424083][ 53.424083] Redzone 0000000033d4cff0: cc cc cc cc cc cc cc cc cc cc cc cc cc cc cc cc ................[ 53.424083] Redzone 00000000bf62d009: cc cc cc cc cc cc cc cc cc cc cc cc cc cc cc cc ................[ 53.424083] Object 00000000f71a6b67: 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b kkkkkkkkkkkkkkkk[ 53.424083] Object 00000000e9b4a4ba: 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b a5 kkkkkkkkkkkkkkk.[ 53.424083] Redzone 00000000de38e5d6: 61 cc cc cc cc cc cc cc a.......[ 53.424083] Padding 000000002dddb492: 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a ZZZZZZZZZZZZZZZZ[ 53.424083] Padding 0000000010d75c56: 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a ZZZZZZZZZZZZZZZZ[ 53.424083] CPU: 1 PID: 3198 Comm: slabinfo Kdump: loaded Tainted: G B O 5.11.0-rc4+ #5[ 53.424083] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS 1.13.0-1ubuntu1.1 04/01/2014[ 53.424083] Call Trace:[ 53.424083] dump_stack+0x77/0x97[ 53.424083] check_bytes_and_report.cold+0x77/0xb4[ 53.424083] check_object+0x193/0x250[ 53.424083] validate_slab+0x127/0x170[ 53.424083] validate_store+0xac/0x160[ 53.424083] slab_attr_store+0x1b/0x30[ 53.424083] kernfs_fop_write+0xca/0x1b0[ 53.424083] vfs_write+0xc6/0x360[ 53.424083] ksys_write+0x63/0xe0[ 53.424083] do_syscall_64+0x33/0x40[ 53.424083] entry_SYSCALL_64_after_hwframe+0x44/0xa9[ 53.424083] RIP: 0033:0x7facb81181e7[ 53.424083] Code: 64 89 02 48 c7 c0 ff ff ff ff eb bb 0f 1f 80 00 00 00 00 f3 0f 1e fa 64 8b 04 25 18 00 00 00 85 c0 75 10 b8 01 00 00 04[ 53.424083] RSP: 002b:00007ffe1b3ef6c8 EFLAGS: 00000246 ORIG_RAX: 0000000000000001[ 53.424083] RAX: ffffffffffffffda RBX: 0000000000000002 RCX: 00007facb81181e7[ 53.424083] RDX: 0000000000000002 RSI: 00005641aa82cb40 RDI: 0000000000000003[ 53.424083] RBP: 00005641aa82cb40 R08: 0000000000000000 R09: 0000000000000002[ 53.424083] R10: 00005641a899065b R11: 0000000000000246 R12: 0000000000000002[ 53.424083] R13: 00005641aa834b80 R14: 00007facb81f44a0 R15: 00007facb81f38a0[ 53.424083] FIX kmalloc-32: Restoring 0x00000000de38e5d6-0x00000000de38e5d6=0xcc[ 53.424083][ 53.424083] =============================================================================[ 53.424083] BUG kmalloc-32 (Tainted: G B O ): Redzone overwritten[ 53.424083] ----------------------------------------------------------------------------- 可以看到开启 slub_debug 之后内存布局是 1234567[ 53.424083] Redzone 0000000033d4cff0: cc cc cc cc cc cc cc cc cc cc cc cc cc cc cc cc ................[ 53.424083] Redzone 00000000bf62d009: cc cc cc cc cc cc cc cc cc cc cc cc cc cc cc cc ................[ 53.424083] Object 00000000f71a6b67: 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b kkkkkkkkkkkkkkkk[ 53.424083] Object 00000000e9b4a4ba: 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b a5 kkkkkkkkkkkkkkk.[ 53.424083] Redzone 00000000de38e5d6: 61 cc cc cc cc cc cc cc a.......[ 53.424083] Padding 000000002dddb492: 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a ZZZZZZZZZZZZZZZZ[ 53.424083] Padding 0000000010d75c56: 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a ZZZZZZZZZZZZZZZZ 在分配之后（未使用GFP_ZREO mask），object的内容都是 6b, 最后一个字节是a5，其余 redzone 的地方都是填充了cc，最后padding的位置填充了 5a，这些magic num 其实都是在分配的时候assign的。先看各个magic num定义，在 include/linux/poison.h 中 123456789101112131415/********** mm/slab.c **********//* * Magic nums for obj red zoning. * Placed in the first word before and the first word after an obj. */#define RED_INACTIVE 0x09F911029D74E35BULL /* when obj is inactive */#define RED_ACTIVE 0xD84156C5635688C0ULL /* when obj is active */#define SLUB_RED_INACTIVE 0xbb#define SLUB_RED_ACTIVE 0xcc/* ...and for poisoning */#define POISON_INUSE 0x5a /* for use-uninitialised poisoning */#define POISON_FREE 0x6b /* for use-after-free poisoning */#define POISON_END 0xa5 /* end-byte of poisoning */ 实现主要参考 mm/slub.c 中代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162static int check_slab(struct kmem_cache *s, struct page *page){ int maxobj; VM_BUG_ON(!irqs_disabled()); if (!PageSlab(page)) { slab_err(s, page, &quot;Not a valid slab page&quot;); return 0; } maxobj = order_objects(compound_order(page), s-&gt;size); if (page-&gt;objects &gt; maxobj) { slab_err(s, page, &quot;objects %u &gt; max %u&quot;, page-&gt;objects, maxobj); return 0; } if (page-&gt;inuse &gt; page-&gt;objects) { slab_err(s, page, &quot;inuse %u &gt; max %u&quot;, page-&gt;inuse, page-&gt;objects); return 0; } /* Slab_pad_check fixes things up after itself */ slab_pad_check(s, page); return 1;}static void init_object(struct kmem_cache *s, void *object, u8 val){ u8 *p = kasan_reset_tag(object); if (s-&gt;flags &amp; SLAB_RED_ZONE) memset(p - s-&gt;red_left_pad, val, s-&gt;red_left_pad); if (s-&gt;flags &amp; __OBJECT_POISON) { memset(p, POISON_FREE, s-&gt;object_size - 1); p[s-&gt;object_size - 1] = POISON_END; } if (s-&gt;flags &amp; SLAB_RED_ZONE) memset(p + s-&gt;object_size, val, s-&gt;inuse - s-&gt;object_size);}static noinline int alloc_debug_processing(struct kmem_cache *s, struct page *page, void *object, unsigned long addr){ init_object(s, object, SLUB_RED_ACTIVE); return 1;}static noinline int free_debug_processing( struct kmem_cache *s, struct page *page, void *head, void *tail, int bulk_cnt, unsigned long addr){ if (s-&gt;flags &amp; SLAB_CONSISTENCY_CHECKS) { if (!check_slab(s, page)) goto out; } init_object(s, object, SLUB_RED_INACTIVE);} check_slab() 主要用来检查slab合法性init_object() 主要用来填充magic numalloc_debug_processing() 和 free_debug_processing() 会hook在slab分配与释放的路径上，达到对所有 slab 对象分配和释放过程中检查是否越界 和 填充magic num的目的。 但是有时候有些slab申请之后，很久之后才会释放，甚至在整个内核生命周期内都不会释放，那通过 free_debug_processing 这个代码路径就无法执行，就没法检查是否越界了吗？显然不是，这就是slabinfo -v的作用，他最后会调用到内核函数validate_slab() 12345678910111213#ifdef CONFIG_SLUB_DEBUGstatic void validate_slab(struct kmem_cache *s, struct page *page){ void *p; void *addr = page_address(page); unsigned long *map; slab_lock(page); if (!check_slab(s, page) || !on_freelist(s, page, NULL)) goto unlock; ......} left redzone oob(out of bounds)使用 kslub_debug_left()同样编译安装之后，可以 使用slabinfo -v触发检查 12345678910111213141516171819stable_kernel@kernel: /tmp/share/test_modules/memory/slub_debug# sudo slabinfo -v[ 88.049885] =============================================================================[ 88.050661] BUG kmalloc-32 (Tainted: G O ): Redzone overwritten[ 88.050661] -----------------------------------------------------------------------------[ 88.050661][ 88.050661] INFO: 0x00000000f7a64828-0x00000000f7a64828 @offset=30. First byte 0x61 instead of 0xcc[ 88.050661] INFO: Allocated in kslub_debug_left+0x1c/0x50 [slub_debug] age=8012 cpu=2 pid=3207......[ 88.050661] INFO: Slab 0x00000000736d0bf0 objects=19 used=13 fp=0x0000000001a78134 flags=0x100000000010201[ 88.050661] INFO: Object 0x00000000a9f17fea @offset=32 fp=0x00000000ec20872a[ 88.050661][ 88.050661] Redzone 000000003a38622f: cc cc cc cc cc cc cc cc cc cc cc cc cc cc cc cc ................[ 88.050661] Redzone 000000006a2f8e0a: cc cc cc cc cc cc cc cc cc cc cc cc cc cc 61 cc ..............a.[ 88.050661] Object 00000000a9f17fea: 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b kkkkkkkkkkkkkkkk[ 88.050661] Object 00000000cc601ad9: 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b a5 kkkkkkkkkkkkkkk.[ 88.050661] Redzone 0000000050c2e0d4: cc cc cc cc cc cc cc cc ........[ 88.050661] Padding 000000009487d314: 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a ZZZZZZZZZZZZZZZZ[ 88.050661] Padding 000000008d823823: 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a ZZZZZZZZZZZZZZZZ[ 88.050661] CPU: 3 PID: 3212 Comm: slabinfo Kdump: loaded Tainted: G B O 5.11.0-rc4+ #5 可以从输出看到原因 12[ 88.050661] BUG kmalloc-32 (Tainted: G O ): Redzone overwritten[ 88.050661] INFO: 0x00000000f7a64828-0x00000000f7a64828 @offset=30. First byte 0x61 instead of 0xcc use after free使用 kslub_debug_use_after_free()同样编译安装之后，可以 使用slabinfo -v触发检查 12345678910111213141516171819202122232425262728[ 43.801344] =============================================================================[ 43.802065] BUG kmalloc-32 (Tainted: G O ): Poison overwritten[ 43.802065] -----------------------------------------------------------------------------[ 43.802065][ 43.802065] INFO: 0x0000000065d12e59-0x0000000065d12e59 @offset=3806. First byte 0x61 instead of 0x6b[ 43.802065] INFO: Allocated in kslub_debug_use_after_free+0x4e/0xc0 [slub_debug] age=8 cpu=1 pid=3163[ 43.802065] __slab_alloc+0x50/0x60[ 43.802065] kmem_cache_alloc_trace+0x1fb/0x230[ 43.802065] kslub_debug_use_after_free+0x4e/0xc0 [slub_debug][ 43.802065] kthread+0x10a/0x140[ 43.802065] ret_from_fork+0x22/0x30[ 43.802065] INFO: Freed in kslub_debug_use_after_free+0x6a/0xc0 [slub_debug] age=8 cpu=1 pid=3163[ 43.802065] kslub_debug_use_after_free+0x6a/0xc0 [slub_debug][ 43.802065] kthread+0x10a/0x140[ 43.802065] ret_from_fork+0x22/0x30[ 43.802065] INFO: Slab 0x0000000091c90915 objects=19 used=19 fp=0x0000000000000000 flags=0x100000000010200[ 43.802065] INFO: Object 0x000000002c0fb46b @offset=3776 fp=0x00000000d32afce7[ 43.802065][ 43.802065] Redzone 000000007de0fb38: bb bb bb bb bb bb bb bb bb bb bb bb bb bb bb bb ................[ 43.802065] Redzone 0000000010d06595: bb bb bb bb bb bb bb bb bb bb bb bb bb bb bb bb ................[ 43.802065] Object 000000002c0fb46b: 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b kkkkkkkkkkkkkkkk[ 43.802065] Object 000000008ffb8d24: 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 61 a5 kkkkkkkkkkkkkka.[ 43.802065] Redzone 00000000f5b8c2fa: bb bb bb bb bb bb bb bb ........[ 43.802065] Padding 000000007af7da7c: 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a ZZZZZZZZZZZZZZZZ[ 43.802065] Padding 000000002d26610c: 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a ZZZZZZZZZZZZZZZZ[ 43.802065] FIX kmalloc-32: Restoring 0x0000000065d12e59-0x0000000065d12e59=0x6b[ 43.802065][ 43.802065] FIX kmalloc-32: Marking all objects used 可以从输出看到原因 12[ 43.802065] BUG kmalloc-32 (Tainted: G O ): Poison overwritten[ 43.802065] INFO: 0x0000000065d12e59-0x0000000065d12e59 @offset=3806. First byte 0x61 instead of 0x6b 可以发现 free 状态的 slab object 的 redzone 填充的是 bb 与使用中的cc不一样，其次 padding还是5a。 double free使用 kslub_debug_double_free()同样编译安装之后 1234567891011121314151617181920212223242526272829303132333435363738[ 99.126747] =============================================================================[ 99.128946] BUG kmalloc-32 (Tainted: G B O ): Object already free[ 99.128946] -----------------------------------------------------------------------------[ 99.128946][ 99.128946] INFO: Allocated in kslub_debug_double_free+0x4e/0xd0 [slub_debug] age=1517 cpu=1 pid=3402[ 99.128946] __slab_alloc+0x50/0x60[ 99.128946] kmem_cache_alloc_trace+0x1fb/0x230[ 99.128946] kslub_debug_double_free+0x4e/0xd0 [slub_debug][ 99.128946] kthread+0x10a/0x140[ 99.128946] ret_from_fork+0x22/0x30[ 99.128946] INFO: Freed in kslub_debug_double_free+0x6b/0xd0 [slub_debug] age=1207 cpu=1 pid=3402[ 99.128946] kslub_debug_double_free+0x6b/0xd0 [slub_debug][ 99.128946] kthread+0x10a/0x140[ 99.128946] ret_from_fork+0x22/0x30[ 99.128946] INFO: Slab 0x000000004358aee6 objects=19 used=16 fp=0x000000007cc439c4 flags=0x100000000010201[ 99.128946] INFO: Object 0x000000007cc439c4 @offset=5856 fp=0x0000000000000000[ 99.128946][ 99.128946] Redzone 000000008f8c9766: bb bb bb bb bb bb bb bb bb bb bb bb bb bb bb bb ................[ 99.128946] Redzone 00000000ad0c3fcc: bb bb bb bb bb bb bb bb bb bb bb bb bb bb bb bb ................[ 99.128946] Object 000000007cc439c4: 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b kkkkkkkkkkkkkkkk[ 99.128946] Object 00000000b15b51a4: 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b a5 kkkkkkkkkkkkkkk.[ 99.128946] Redzone 000000009f473013: bb bb bb bb bb bb bb bb ........[ 99.128946] Padding 00000000cb92dccb: 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a ZZZZZZZZZZZZZZZZ[ 99.128946] Padding 000000006bc8c5e0: 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a 5a ZZZZZZZZZZZZZZZZ[ 99.128946] CPU: 1 PID: 3402 Comm: kslub_debug Kdump: loaded Tainted: G B O 5.11.0-rc4+ #5[ 99.128946] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS 1.13.0-1ubuntu1.1 04/01/2014[ 99.128946] Call Trace:[ 99.128946] dump_stack+0x77/0x97[ 99.128946] free_debug_processing.cold+0x10c/0x14f[ 99.128946] __slab_free+0x286/0x490[ 99.128946] ? schedule_timeout+0x1bf/0x370[ 99.128946] ? trace_hardirqs_on+0x1b/0xd0[ 99.128946] kslub_debug_double_free+0x86/0xd0 [slub_debug][ 99.128946] ? kslub_debug_double_free.part.0+0x20/0x20 [slub_debug][ 99.128946] kthread+0x10a/0x140[ 99.128946] ? kthread_park+0x80/0x80[ 99.128946] ret_from_fork+0x22/0x30[ 99.160092] FIX kmalloc-32: Object at 0x000000007cc439c4 not freed 可以从输出看到原因 1[ 99.128946] BUG kmalloc-32 (Tainted: G B O ): Object already free double free的问题会直接在 第二次 free的时候打印出日志，不需要slabinfo -v去触发 slubinfo 其他用处slabinfo -t 可以跟踪 slab 内存分配释放slabinfo -A 显示 alias 信息slabinfo -T 显示 total 信息 123456789101112131415161718192021222324stable_kernel@kernel: /var/crash# sudo slabinfo -TSlabcache Totals----------------Slabcaches : 220 Aliases : 0-&gt;0 Active: 117Memory used: 211.2M # Loss : 112.7M MRatio: 114%# Objects : 265.6K # PartObj: 17.7K ORatio: 6%Per Cache Average Min Max Total----------------------------------------------------------------------------#Objects 2.2K 1 39.8K 265.6K#Slabs 185 1 3.3K 21.7K#PartSlab 16 1 347 1.9K%PartSlab 56% 0% 100% 9%PartObjs 6 0 3.8K 17.7K% PartObj 53% 0% 100% 6%Memory 1.8M 4.0K 27.1M 211.2MUsed 841.8K 32 22.7M 98.4MLoss 964.0K 4.0K 14.7M 112.7MPer Object Average Min Max-----------------------------------------------------------Memory 742 344 24.5KUser 370 8 8.1KLoss 372 336 16.3K slub_debug overhead从 slub debug 原理上看，主要有以下几点 overhead: 需要额外空间去放 redzone等为了debug而额外添加的一些字段 需要在 kmalloc kfree 等关键路径上hook 设置、检查 magic num的代码其他倒没有很大的overhead 使用公司开发板做一下对比（单核 cortex-A7芯片） 12345678910111213memory on offfree 105224K 57992Kcpu on offusr 4.1 5.5sys 19.8 11.1idle 75.3 82.2loadavg on off1 2.91 1.145 1.13 0.2915 0.41 0.10 可以看到 slub_debug 特性大概占用了7% CPU。free内存少了一半，当然，这是个小内存系统，内存只有211Mb。loadavg 也上升了许多，没有去看 sar -B 1 相关指标，应该也出现了不少直接内存回收。 总结从overall的角度来说，slub debug 通过在 kmalloc kfree的路径上hook了设置和检查 magic num的方式，可以很有效的排查： 内存使用越界 内存use after free double free这三类问题 参考 wowo 文章1参考 wowo 文章2","link":"/2021/01/26/%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86/slub%20debug%20%E5%AE%9A%E4%BD%8D%E5%86%85%E5%AD%98%E8%B6%8A%E7%95%8C/"},{"title":"使能lock_dep解决死锁问题","text":"lockdep 就是 lock dependencies 缩写，翻译是 锁依赖。 如何使能 lockdep在 make menuconfig 使能 lockdep 之后，会自动增加如下配置 12345678echo &quot;CONFIG_LOCKUP_DETECTOR=y&quot; &gt;&gt; /tmp/.configecho &quot;CONFIG_SOFTLOCKUP_DETECTOR=y&quot; &gt;&gt; /tmp/.configecho &quot;CONFIG_BOOTPARAM_SOFTLOCKUP_PANIC=y&quot; &gt;&gt; /tmp/.configecho &quot;CONFIG_BOOTPARAM_SOFTLOCKUP_PANIC_VALUE=1&quot; &gt;&gt; /tmp/.configecho &quot;CONFIG_HARDLOCKUP_DETECTOR_PERF=y&quot; &gt;&gt; /tmp/.configecho &quot;CONFIG_HARDLOCKUP_DETECTOR=y&quot; &gt;&gt; /tmp/.configecho &quot;CONFIG_BOOTPARAM_HARDLOCKUP_PANIC=y&quot; &gt;&gt; /tmp/.configecho &quot;CONFIG_BOOTPARAM_HARDLOCKUP_PANIC_VALUE=1&quot; &gt;&gt; /tmp/.config 配置之后重新编译运行，会发现在 /proc/lockdep 目录下多出几个文件 12345678/proc/sys/kernel/lock_stat--------置位则可以查看/proc/lock_stat统计信息，清楚则关闭lockdep统计信息。/proc/sys/kernel/max_lock_depth---/proc/sys/kernel/prove_locking/proc/locks/proc/lock_stat-------------------关于锁的使用统计信息/proc/lockdep---------------------存在依赖关系的锁/proc/lockdep_stats---------------存在依赖关系锁的统计信息/proc/lockdep_chains--------------依赖关系锁链表 lockdep 原理常见的死锁有如下两种： 递归死锁：中断等延迟操作中使用了锁，和外面的锁构成了递归死锁。 AB-BA死锁：多个锁因处理不当而引发死锁，多个内核路径上的所处理顺序不一致也会导致死锁。 Linux内核提供死锁调试模块Lockdep，跟踪每个锁的自身状态和各个锁之间的依赖关系，经过一系列的验证规则来确保锁之间依赖关系是正确的。 先看代码注释 123456789101112131415/* * this code maps all the lock dependencies as they occur in a live kernel * and will warn about the following classes of locking bugs: * * - lock inversion scenarios * - circular lock dependencies * - hardirq/softirq safe/unsafe locking bugs * * Bugs are reported even if the current locking scenario does not cause * any deadlock at this point. * * I.e. if anytime in the past two locks were taken in a different order, * even if it happened for another task, even if those were different * locks (but of the same class as this lock), this code will detect it. */ 翻译就是可以解决如下的问题 锁定反转方案 – ABBA 循环锁依赖性 – 递归锁 hardirq / softirq安全/不安全的锁定错误 lockdep 案例写了一个ABBA型 基于 spinlock的 deadlock demo。参考 代码其实这个demo 会触发多个问题 deadlock 被检测出来。 由于是基于 spinlock 的，所以在等待lock的时候一直处于spin，导致 rcu stall 等待 20s 之后，由于基于 spinlock的，所以两个cpu一直未调度，发生 soft lockup，然后 panic. 编译安装之后，kmsg 显示如下问题： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263[ 147.475517] lockdep_test: loading out-of-tree module taints kernel.[ 157.769995][ 157.770640] ======================================================[ 157.770683] WARNING: possible circular locking dependency detected[ 157.770683] 5.11.0-rc4+ #5 Tainted: G O[ 157.770683] ------------------------------------------------------[ 157.770683] krace_0/3755 is trying to acquire lock:[ 157.770683] ffffffffc0527468 (&amp;g_lockdep_test.lock_A){+.+.}-{2:2}, at: klockdep_test_BA+0x2e/0x80 [lockdep_test][ 157.770683] but task is already holding lock:[ 157.770683] ffffffffc05274a8 (&amp;g_lockdep_test.lock_B){+.+.}-{2:2}, at: klockdep_test_BA+0x18/0x80 [lockdep_test][ 157.770683] which lock already depends on the new lock.[ 157.770683] the existing dependency chain (in reverse order) is:[ 157.770683] -&gt; #1 (&amp;g_lockdep_test.lock_B){+.+.}-{2:2}:[ 157.770683] _raw_spin_lock+0x27/0x40[ 157.770683] klockdep_test_AB+0x2e/0x80 [lockdep_test][ 157.770683] kthread+0x10a/0x140[ 157.770683] ret_from_fork+0x22/0x30[ 157.770683] -&gt; #0 (&amp;g_lockdep_test.lock_A){+.+.}-{2:2}:[ 157.770683] __lock_acquire+0x139e/0x28a0[ 157.770683] lock_acquire+0xbd/0x360[ 157.770683] _raw_spin_lock+0x27/0x40[ 157.770683] klockdep_test_BA+0x2e/0x80 [lockdep_test][ 157.770683] kthread+0x10a/0x140[ 157.770683] ret_from_fork+0x22/0x30[ 157.770683] other info that might help us debug this:[ 157.770683] Possible unsafe locking scenario:[ 157.770683] CPU0 CPU1[ 157.770683] ---- ----[ 157.770683] lock(&amp;g_lockdep_test.lock_B);[ 157.770683] lock(&amp;g_lockdep_test.lock_A);[ 157.770683] lock(&amp;g_lockdep_test.lock_B);[ 157.770683] lock(&amp;g_lockdep_test.lock_A);[ 157.770683] *** DEADLOCK ***[ 157.770683] 1 lock held by krace_0/3755:[ 157.770683] #0: ffffffffc05274a8 (&amp;g_lockdep_test.lock_B){+.+.}-{2:2}, at: klockdep_test_BA+0x18/0x80 [lockdep_test][ 157.770683] stack backtrace:[ 157.770683] CPU: 1 PID: 3755 Comm: krace_0 Kdump: loaded Tainted: G O 5.11.0-rc4+ #5[ 157.770683] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS 1.13.0-1ubuntu1 04/01/2014[ 157.770683] Call Trace:[ 157.770683] dump_stack+0x77/0x97[ 157.770683] check_noncircular+0xfe/0x110[ 157.770683] ? find_held_lock+0x2b/0x80[ 157.770683] __lock_acquire+0x139e/0x28a0[ 157.770683] lock_acquire+0xbd/0x360[ 157.770683] ? klockdep_test_BA+0x2e/0x80 [lockdep_test][ 157.770683] ? klockdep_test_AB+0x80/0x80 [lockdep_test][ 157.770683] _raw_spin_lock+0x27/0x40[ 157.770683] ? klockdep_test_BA+0x2e/0x80 [lockdep_test][ 157.770683] klockdep_test_BA+0x2e/0x80 [lockdep_test][ 157.770683] kthread+0x10a/0x140[ 157.770683] ? kthread_park+0x80/0x80[ 157.770683] ret_from_fork+0x22/0x30 其中可以很明显看到如下提示 123456[ 157.770683] CPU0 CPU1[ 157.770683] ---- ----[ 157.770683] lock(&amp;g_lockdep_test.lock_B);[ 157.770683] lock(&amp;g_lockdep_test.lock_A);[ 157.770683] lock(&amp;g_lockdep_test.lock_B);[ 157.770683] lock(&amp;g_lockdep_test.lock_A); 这个提示简直无敌，完美显示了 deadlock 的原因 写了一个ABBA型 基于 mutex 的 deadlock demo。参考 代码其实这个demo 会触发多个问题 deadlock 被检测出来。 等待 120s 之后，由于基于 mutex的，所以发生死锁的俩线程都是 D状态，所以检测到发生 hung_task，然后 panic. 为啥 mutex 的waiter会 处于 D状态 ？看看代码 123456789101112static noinline void __sched__mutex_lock_slowpath(struct mutex *lock){ __mutex_lock(lock, TASK_UNINTERRUPTIBLE, 0, NULL, _RET_IP_);}void __sched mutex_lock(struct mutex *lock){ if (!__mutex_trylock_fast(lock)) __mutex_lock_slowpath(lock);}EXPORT_SYMBOL(mutex_lock); 发现等待 mutex的线程都会被设置成 TASK_UNINTERRUPTIBLE，也就是 D状态。 也可以通过 hung_task 之后panic 的现场看出来 1234567891011121314151617181920212223crash&gt; ps | grep UN 3193 2 2 ffff9127bb82b240 UN 0.0 0 0 [krace_0] 3194 2 0 ffff9127c3458040 UN 0.0 0 0 [krace_1]crash&gt; bt 3193PID: 3193 TASK: ffff9127bb82b240 CPU: 2 COMMAND: &quot;krace_0&quot; #0 [ffffb557007efd78] __schedule at ffffffff8c319af2 #1 [ffffb557007efe08] schedule at ffffffff8c31a1e6 #2 [ffffb557007efe20] schedule_preempt_disabled at ffffffff8c31a53c #3 [ffffb557007efe28] __mutex_lock at ffffffff8c31bcd5 #4 [ffffb557007eff08] klockdep_test_mutex_BA at ffffffffc02990b2 [lockdep_test_mutex] #5 [ffffb557007eff10] kthread at ffffffff8b6930da #6 [ffffb557007eff50] ret_from_fork at ffffffff8b601ae2crash&gt;crash&gt; bt 3194PID: 3194 TASK: ffff9127c3458040 CPU: 0 COMMAND: &quot;krace_1&quot; #0 [ffffb557004c3d78] __schedule at ffffffff8c319af2 #1 [ffffb557004c3e08] schedule at ffffffff8c31a1e6 #2 [ffffb557004c3e20] schedule_preempt_disabled at ffffffff8c31a53c #3 [ffffb557004c3e28] __mutex_lock at ffffffff8c31bcd5 #4 [ffffb557004c3f08] klockdep_test_mutex_AB at ffffffffc0299032 [lockdep_test_mutex] #5 [ffffb557004c3f10] kthread at ffffffff8b6930da #6 [ffffb557004c3f50] ret_from_fork at ffffffff8b601ae2crash&gt; 可以看到 krace_0 krace_1 线程都是处于 UN 状态，这也导致此时系统负载有2 1234567crash&gt; sys KERNEL: vmlinux DUMPFILE: dump.202101221553 [PARTIAL DUMP] CPUS: 4 DATE: Fri Jan 22 15:53:16 CST 2021 UPTIME: 00:04:06LOAD AVERAGE: 1.96, 1.10, 0.46 再看一下 检测到的死锁日志： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970[ 60.151205] lockdep_test_mutex: loading out-of-tree module taints kernel.[ 70.216063][ 70.216623] ======================================================[ 70.216866] WARNING: possible circular locking dependency detected[ 70.216866] 5.11.0-rc4+ #5 Tainted: G O[ 70.216866] ------------------------------------------------------[ 70.216866] krace_0/3193 is trying to acquire lock:[ 70.216866] ffffffffc029b4b8 (&amp;g_lockdep_test_mutex.lock_A){+.+.}-{3:3}, at: klockdep_test_mutex_BA+0x32/0x80 [lockdep_test_mutex][ 70.216866] but task is already holding lock:[ 70.216866] ffffffffc029b548 (&amp;g_lockdep_test_mutex.lock_B){+.+.}-{3:3}, at: klockdep_test_mutex_BA+0x1a/0x80 [lockdep_test_mutex][ 70.216866] which lock already depends on the new lock.[ 70.216866] the existing dependency chain (in reverse order) is:[ 70.216866] -&gt; #1 (&amp;g_lockdep_test_mutex.lock_B){+.+.}-{3:3}:[ 70.216866] __mutex_lock+0x8d/0x920[ 70.216866] klockdep_test_mutex_AB+0x32/0x80 [lockdep_test_mutex][ 70.216866] kthread+0x10a/0x140[ 70.216866] ret_from_fork+0x22/0x30[ 70.216866] -&gt; #0 (&amp;g_lockdep_test_mutex.lock_A){+.+.}-{3:3}:[ 70.216866] __lock_acquire+0x139e/0x28a0[ 70.216866] lock_acquire+0xbd/0x360[ 70.216866] __mutex_lock+0x8d/0x920[ 70.216866] klockdep_test_mutex_BA+0x32/0x80 [lockdep_test_mutex][ 70.216866] kthread+0x10a/0x140[ 70.216866] ret_from_fork+0x22/0x30[ 70.216866] other info that might help us debug this:[ 70.216866] Possible unsafe locking scenario:[ 70.216866] CPU0 CPU1[ 70.216866] ---- ----[ 70.216866] lock(&amp;g_lockdep_test_mutex.lock_B);[ 70.216866] lock(&amp;g_lockdep_test_mutex.lock_A);[ 70.216866] lock(&amp;g_lockdep_test_mutex.lock_B);[ 70.216866] lock(&amp;g_lockdep_test_mutex.lock_A);[ 70.216866] *** DEADLOCK ***[ 70.216866] 1 lock held by krace_0/3193:[ 70.216866] #0: ffffffffc029b548 (&amp;g_lockdep_test_mutex.lock_B){+.+.}-{3:3}, at: klockdep_test_mutex_BA+0x1a/0x80 [lockdep_test_mutex][ 70.216866] stack backtrace:[ 70.216866] CPU: 2 PID: 3193 Comm: krace_0 Kdump: loaded Tainted: G O 5.11.0-rc4+ #5[ 70.216866] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS 1.13.0-1ubuntu1 04/01/2014[ 70.216866] Call Trace:[ 70.216866] dump_stack+0x77/0x97[ 70.216866] check_noncircular+0xfe/0x110[ 70.216866] __lock_acquire+0x139e/0x28a0[ 70.216866] lock_acquire+0xbd/0x360[ 70.216866] ? klockdep_test_mutex_BA+0x32/0x80 [lockdep_test_mutex][ 70.216866] ? lockdep_hardirqs_on_prepare+0xd4/0x170[ 70.216866] ? _raw_spin_unlock_irqrestore+0x34/0x40[ 70.216866] __mutex_lock+0x8d/0x920[ 70.216866] ? klockdep_test_mutex_BA+0x32/0x80 [lockdep_test_mutex][ 70.216866] ? find_held_lock+0x2b/0x80[ 70.216866] ? klockdep_test_mutex_BA+0x32/0x80 [lockdep_test_mutex][ 70.216866] ? __next_timer_interrupt+0x100/0x100[ 70.216866] ? klockdep_test_mutex_AB+0x80/0x80 [lockdep_test_mutex][ 70.216866] ? klockdep_test_mutex_BA+0x32/0x80 [lockdep_test_mutex][ 70.216866] ? klockdep_test_mutex_AB+0x80/0x80 [lockdep_test_mutex][ 70.216866] klockdep_test_mutex_BA+0x32/0x80 [lockdep_test_mutex][ 70.216866] kthread+0x10a/0x140[ 70.216866] ? kthread_park+0x80/0x80[ 70.216866] ret_from_fork+0x22/0x30 同样也是 给出了很详细出错位置，对于找出问题代码一如反掌。 123456[ 70.216866] CPU0 CPU1[ 70.216866] ---- ----[ 70.216866] lock(&amp;g_lockdep_test_mutex.lock_B);[ 70.216866] lock(&amp;g_lockdep_test_mutex.lock_A);[ 70.216866] lock(&amp;g_lockdep_test_mutex.lock_B);[ 70.216866] lock(&amp;g_lockdep_test_mutex.lock_A); lockdep 代码之后填坑 死锁带来的影响对于线程自身 spinlock 的死锁可以带来 线程一直在spin，占用cpu且无法恢复. mutex semaphore rwsem 的死锁会在slowpath 中让线程进入了TASK_UNINTERRUPTIBLE 状态，导致不响应外部信号，也无法使用 kill 去杀死 对于系统 spinlock 的死锁可以带来 rcu stall soft lockup hard lockup问题，如果对应项设置了 panic 选项，就会导致 kernel panic. mutex semaphore rwsem 的死锁会带来系统的 load升高，因为都在 slowpath 中让线程进入了TASK_UNINTERRUPTIBLE 状态，会被统计为系统负载，最后会导致 hung_task 发生，如果对应项设置了 panic 选项，就会导致 kernel panic. 不同锁进入slowpath的行为 123456789101112131415161718192021222324mutex:static noinline void __sched__mutex_lock_slowpath(struct mutex *lock){ __mutex_lock(lock, TASK_UNINTERRUPTIBLE, 0, NULL, _RET_IP_);}semaphore:static noinline void __sched __down(struct semaphore *sem){ __down_common(sem, TASK_UNINTERRUPTIBLE, MAX_SCHEDULE_TIMEOUT);}semaphore:static noinline void __sched __down(struct semaphore *sem){ __down_common(sem, TASK_UNINTERRUPTIBLE, MAX_SCHEDULE_TIMEOUT);}rwsem:static inline void __down_write(struct rw_semaphore *sem){ __down_write_common(sem, TASK_UNINTERRUPTIBLE);} 可以参考：stack overflow提问博客园文章魅族内核团队文章内核文档","link":"/2021/01/23/%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86/%E4%BD%BF%E8%83%BDlock_dep%E8%A7%A3%E5%86%B3%E6%AD%BB%E9%94%81%E9%97%AE%E9%A2%98/"},{"title":"文件资源泄漏问题","text":"资源泄漏资源泄漏在实际编程中是一不小心就会遇到的一个问题，最常见的就是 内存泄漏。内存泄漏：对于短时间存在的进程，线程即使存在内存泄漏，往往也不会变现出来，开发人员也很难感知到。但是一旦是长时间运行的进程，线程存在内存泄漏的问题，那将是一个灾难，要么过一会被OOM KILL，要么自己重启，更严重的就是重启机器。 内存泄漏是指内存被分配出来，但是后续一直未使用，且失去了这个内存的引用，一直无法释放的问题。 但是除了最常见内存泄漏之外，还有其他各种资源泄漏的问题，比如这次的 进程文件资源泄漏。 场景虽然是嵌入式平台，也有一些业务需要做压力测试连续几天到一个月不关机长时间跑测试。 某个业务在运行两天之后就会出现进程挂掉的问题。 代码复现如果能用简单代码复现的问题，都不大 代码 123456789101112131415161718192021222324252627282930#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;#include &lt;sys/types.h&gt;#include &lt;dirent.h&gt;#include &lt;unistd.h&gt;#include &lt;sys/types.h&gt;#include &lt;sys/stat.h&gt;#include &lt;fcntl.h&gt;#define DIR_NAME &quot;/tmp/asdf&quot;int sync_dir(char *dir){ if(!dir) return -1; int fd = open(dir, O_ASYNC); if (-1 == fd) return -1; return 0;}void main(void){ int i = 0; int ret = -1; for (i = 0; i &lt; 1024 * 1024 * 64; i++) { ret = sync_dir(DIR_NAME); if (ret) printf(&quot;sync_dir failed. i = %d, ret = %d\\n&quot;, i, ret); else printf(&quot;sync_dir sucess. i = %d\\n&quot;, i); usleep(1000 * 10); }} 嵌入式设备上的资源限制 123root@device_gls:/proc/sys/fs # cat file-nr800 0 21378root@device_gls:/proc/sys/fs # 意思是当前系统允许打开 21378 个文件，已经打开了 800个文件了 我选择用 ubuntu 系统去复现，先将 系统 fd上限制调制与设备一样的水平 12345678tencent_clould@ubuntu: /proc/sys/fs# cat file-max9223372036854775807tencent_clould@1ubuntu: /proc/sys/fs# sudo suVM-0-11-ubuntu# echo 21378 &gt; file-maxVM-0-11-ubuntu# exittencent_clould@ubuntu: /proc/sys/fs# cat file-max21378tencent_clould@ubuntu: /proc/sys/fs# 尝试复现 123456789tencent_clould@130ubuntu: ~/workspace/hexo_blog/source/_posts/资源管理# ./a.outsync_dir sucess. i = 0sync_dir sucess. i = 1....sync_dir sucess. i = 5sync_dir sucess. i = 18194sync_dir sucess. i = 18195...sync_dir failed. i = 18196, ret = -1 同时观察 /proc/sys/fs/file-nr 12345678910111213141516tencent_clould@ubuntu: /proc/sys/fs# cat file-nr4704 0 21378tencent_clould@ubuntu: /proc/sys/fs# cat file-nr9824 0 21378tencent_clould@ubuntu: /proc/sys/fs# cat file-nr17760 0 21378tencent_clould@ubuntu: /proc/sys/fs# cat file-nr20224 0 21378tencent_clould@ubuntu: /proc/sys/fs# cat file-nr21024 0 21378tencent_clould@ubuntu: /proc/sys/fs# cat file-nr21216 0 21378tencent_clould@ubuntu: /proc/sys/fs# cat file-nrzsh: pipe failed: too many open files in systemtencent_clould@ubuntu: /proc/sys/fs# cat file-nrzsh: pipe failed: too many open files in system 可以看到 a.out 最后无法 通过 open 打开文件了，且同时 zsh shell 都无法打开file-nr了。 与此同时，看到 dmesg 的信息 12[57615.006011] VFS: file-max limit 21378 reached[57616.671617] VFS: file-max limit 21378 reached 也可以比较容易定位出来。 这个问题困扰了当时开发同事很久，最后发现原来是 系统 fd 资源全部泄漏殆尽导致的。 其他限制资源使用的方式 其实还有其他方式限制资源 如 ulimit 123456789101112131415161718tencent_clould@ubuntu: /proc/sys/fs# ulimit -a-t: cpu time (seconds) unlimited-f: file size (blocks) unlimited-d: data seg size (kbytes) unlimited-s: stack size (kbytes) 8192-c: core file size (blocks) 0-m: resident set size (kbytes) unlimited-u: processes 7582-n: file descriptors 1024-l: locked-in-memory size (kbytes) 65536-v: address space (kbytes) unlimited-x: file locks unlimited-i: pending signals 7582-q: bytes in POSIX msg queues 819200-e: max nice 0-r: max rt priority 0-N 15: unlimitedtencent_clould@ubuntu: /proc/sys/fs# /proc/pid/limits12345678910111213141516171819tencent_clould@ubuntu: /proc/sys/fs# cat /proc/4833/limitsLimit Soft Limit Hard Limit UnitsMax cpu time unlimited unlimited secondsMax file size unlimited unlimited bytesMax data size unlimited unlimited bytesMax stack size 8388608 unlimited bytesMax core file size 0 unlimited bytesMax resident set unlimited unlimited bytesMax processes 7582 7582 processesMax open files 1024 1048576 filesMax locked memory 67108864 67108864 bytesMax address space unlimited unlimited bytesMax file locks unlimited unlimited locksMax pending signals 7582 7582 signalsMax msgqueue size 819200 819200 bytesMax nice priority 0 0Max realtime priority 0 0Max realtime timeout unlimited unlimited ustencent_clould@ubuntu: /proc/sys/fs# /etc/security/limits.conf 这里可以预先配置。","link":"/2021/01/15/%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86/%E6%96%87%E4%BB%B6%E8%B5%84%E6%BA%90%E6%B3%84%E6%BC%8F%E9%97%AE%E9%A2%98/"},{"title":"进程虚拟地址空间泄漏问题","text":"虚拟内存泄漏一般情况我们说 内存泄漏，都是指的是物理内存泄漏，毕竟物理内存是实实在在的，一个进程泄漏了，那么整个系统中的可用内存就会变少。 但是linux的虚拟内存空间是各个进程之间相互隔离的，在arm64系统中 有 T,在 arm32系统中也有3G多。但是否意味着这种 虚拟内存我们就不需要关心呢？ 尝试复现代码： 12345678910111213141516171819#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;void main(void){ int i = 0; char *p = NULL; for (i = 0; i &lt; 1024 * 1024 * 64; i++) { p = malloc(1024 * 1024); if (!p) {printf(&quot;malloc failed,p = %p, i = %d\\n&quot;, p, i); break;} else {printf(&quot;malloc sucess.p = %p, i = %d\\n&quot;, p, i);} usleep(1000 * 10); } printf(&quot;malloc failed,just wait!!\\n&quot;); while(1) { usleep(1000 * 10); }} 每次尝试需分配 1G的虚拟内存，但是不会去使用，也就是不会去申请物理内存 x86_64 平台我一开始尝试在腾讯云 服务器上复现： 1234567tencent_clould@130ubuntu: ~/workspace/test_modules/resource_leak/process_virtual_address_memleak# ./a.outmalloc sucess. p = 0x7f8e3f5c6010, i = 0malloc sucess. p = 0x7f8e3f4c5010, i = 1......malloc sucess. p = 0x56771e4ba6e0, i = 461827malloc sucess. p = 0x56771e5ba6f0, i = 461828[1] 2721866 killed ./a.out 直到最后分配到 461828 MB 内存的时候，a.out 被OOM kill了。 到底是为啥呢？？？毕竟我也没有去往 malloc 的虚拟地址中写数据，也不应该会分配物理页面。从 top 命令看 123456789101112top - 15:47:46 up 3 days, 15:06, 2 users, load average: 0.24, 0.16, 0.10Tasks: 157 total, 1 running, 156 sleeping, 0 stopped, 0 zombie%Cpu(s): 3.3 us, 3.6 sy, 0.0 ni, 92.7 id, 0.3 wa, 0.0 hi, 0.0 si, 0.0 stMiB Mem : 1987.9 total, 70.5 free, 1660.6 used, 256.8 buff/cacheMiB Swap: 2048.0 total, 1499.9 free, 548.1 used. 162.0 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND2783846 ubuntu 20 0 173.4g 683692 1380 S 0.3 33.6 0:06.86 a.out 1378 jenkins 20 0 2510448 176168 2924 S 0.7 8.7 7:05.12 java2779920 ubuntu 20 0 925684 107776 15196 S 1.0 5.3 0:21.10 node2703822 ubuntu 20 0 1611828 41268 11624 S 0.3 2.0 0:24.40 node2773158 ubuntu 20 0 702408 39436 12496 S 0.7 1.9 0:15.87 已经分配了 173G 虚拟内存，但是同时可以发现 驻留在物理内存中的页面也达到了 683692kb.这到底是什么？ 通过对比 运行 mytest 前后的 /proc/meminfo 文件，发现差异主要在 1234567系统正常Active(anon): 243372 kBInactive(anon): 267836 kBmytest运行之后Active(anon): 550888 kBInactive(anon): 576840 kB 说明确实是 mytest 消耗了大量的内存 用作匿名页面接下来就需要查看具体是哪种页面消耗了物理内存了 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354tencent_clould@ubuntu: /proc/2798128# cat smaps | grep heap -A 407fbca3ff9000-7fc69e600000 rw-p 00000000 00:00 0 [heap]Size: 41850908 kBKernelPageSize: 4 kBMMUPageSize: 4 kBRss: 162844 kBPss: 162844 kBShared_Clean: 0 kBShared_Dirty: 0 kBPrivate_Clean: 0 kBPrivate_Dirty: 162844 kBReferenced: 162844 kBAnonymous: 162844 kBLazyFree: 0 kB此时top 输出top - 16:05:47 up 3 days, 15:24, 4 users, load average: 0.14, 0.09, 0.10Tasks: 157 total, 1 running, 156 sleeping, 0 stopped, 0 zombie%Cpu(s): 3.0 us, 2.7 sy, 0.0 ni, 94.0 id, 0.0 wa, 0.0 hi, 0.3 si, 0.0 stMiB Mem : 1987.9 total, 786.6 free, 917.0 used, 284.2 buff/cacheMiB Swap: 2048.0 total, 1512.2 free, 535.8 used. 912.0 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND2798128 ubuntu 20 0 45.1g 185412 1408 S 0.7 9.1 0:01.66 a.out......等待3min......tencent_clould@ubuntu: /proc/2798128# cat smaps | grep heap -A 407fb68e600000-7fc69e600000 rw-p 00000000 00:00 0 [heap]Size: 67371008 kBKernelPageSize: 4 kBMMUPageSize: 4 kBRss: 262144 kBPss: 262144 kBShared_Clean: 0 kBShared_Dirty: 0 kBPrivate_Clean: 0 kBPrivate_Dirty: 262144 kBReferenced: 262144 kBAnonymous: 262144 kBLazyFree: 0 kBAnonHugePages: 0 kB此时top 输出top - 16:09:43 up 3 days, 15:27, 4 users, load average: 0.24, 0.12, 0.10Tasks: 159 total, 1 running, 158 sleeping, 0 stopped, 0 zombie%Cpu(s): 2.7 us, 3.4 sy, 0.0 ni, 90.3 id, 3.7 wa, 0.0 hi, 0.0 si, 0.0 stMiB Mem : 1987.9 total, 585.4 free, 1072.9 used, 329.5 buff/cacheMiB Swap: 2048.0 total, 1523.9 free, 524.1 used. 754.9 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND2798128 ubuntu 20 0 67.9g 278604 1408 S 0.3 13.7 0:02.55 a.out 可以看到虽然 没有往堆内存中写入数据，但是还是消耗了较多的物理内存？这是怎么回事？ 猜想一， 可能是一直连续分配虚拟内存，导致 vma 使用很多？通过查看 /proc/slabinfo 和 /proc/pid/maps 看堆内存就一块，基本没有分块的，kernel中也有vma_merge 的操作，所以应该不是vma 占用的内存 ??？我还没有其他头绪，为什么会占用物理内存 arm32 平台然后想了想，x86_64平台可能是虚拟内存太大，复现时间成本有点高就使用了公司的板子，重新编译了一遍，在一个 arm32 板子上运行 12345678910111213141516171819202122232425262728293031323334353637root@device_gls:/ # mytestmalloc sucess. p = 0xb6b00000, i = 0malloc sucess. p = 0xb6a00000, i = 1malloc sucess. p = 0xb6900000, i = 2malloc sucess. p = 0xb6800000, i = 3malloc sucess. p = 0xb6700000, i = 4malloc sucess. p = 0xb6600000, i = 5malloc sucess. p = 0xb6500000, i = 6......malloc sucess. p = 0x10380000, i = 2663malloc sucess. p = 0x10280000, i = 2664malloc sucess. p = 0x10180000, i = 2665malloc sucess. p = 0x10080000, i = 2666......malloc sucess. p = 0xff80000, i = 2667malloc sucess. p = 0xfe80000, i = 2668malloc sucess. p = 0xfd80000, i = 2669malloc sucess. p = 0xfc80000, i = 2670......malloc sucess. p = 0x4180000, i = 2857malloc sucess. p = 0x4080000, i = 2858malloc sucess. p = 0x3f80000, i = 2859......malloc sucess. p = 0x1080000, i = 2906malloc sucess. p = 0xf80000, i = 2907malloc sucess. p = 0xe80000, i = 2908......malloc sucess. p = 0x80000, i = 2922malloc sucess. p = 0xb6f80000, i = 2923malloc sucess. p = 0xb7080000, i = 2924......malloc sucess. p = 0xbe880000, i = 3044malloc sucess. p = 0xbe980000, i = 3045malloc sucess. p = 0xbea80000, i = 3046malloc sucess. p = 0xbed80000, i = 3047malloc failed, p = 0x0, i = 3048malloc failed,just wait!! 可以看到，arm32 平台在分配了 3038MB 虚拟内存之后，在分配第3039块1MB的内存时，由于虚拟地址空间用完了，不能再继续分配虚拟内存？ 可以看到此时 mytest 进程的地址空间 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647root@device_gls:/proc/1293 # cat maps00080000-7f580000 rw-p 00000000 00:00 0 [anon:libc_malloc]7f5cd000-7f5d0000 r-xp 00000000 103:0f 1523 /system/bin/mytest7f5d0000-7f5d1000 r--p 00002000 103:0f 1523 /system/bin/mytest7f5d1000-7f5d2000 rw-p 00000000 00:00 0 [heap]7f600000-b6d00000 rw-p 00000000 00:00 0 [anon:libc_malloc]b6d64000-b6d68000 r-xp 00000000 103:0f 1428 /system/lib/libnetd_client.sob6d68000-b6d69000 r--p 00003000 103:0f 1428 /system/lib/libnetd_client.sob6d69000-b6d6a000 rw-p 00004000 103:0f 1428 /system/lib/libnetd_client.sob6d6a000-b6d8a000 r--s 00000000 00:0f 11859 /dev/__properties__b6d8a000-b6d8b000 rw-p 00000000 00:00 0 [anon:linker_alloc_vector]b6d8c000-b6d8d000 rw-p 00000000 00:00 0 [anon:linker_alloc_vector]b6d8d000-b6dac000 r-xp 00000000 103:0f 1421 /system/lib/libm.sob6dac000-b6dad000 ---p 00000000 00:00 0b6dad000-b6dae000 r--p 0001f000 103:0f 1421 /system/lib/libm.sob6dae000-b6daf000 rw-p 00020000 103:0f 1421 /system/lib/libm.sob6daf000-b6e23000 r-xp 00000000 103:0f 1355 /system/lib/libc.sob6e23000-b6e27000 r--p 00073000 103:0f 1355 /system/lib/libc.sob6e27000-b6e2a000 rw-p 00077000 103:0f 1355 /system/lib/libc.sob6e2a000-b6e34000 rw-p 00000000 00:00 0b6e34000-b6ebc000 r-xp 00000000 103:0f 1354 /system/lib/libc++.sob6ebc000-b6ebd000 ---p 00000000 00:00 0b6ebd000-b6ec1000 r--p 00088000 103:0f 1354 /system/lib/libc++.sob6ec1000-b6ec2000 rw-p 0008c000 103:0f 1354 /system/lib/libc++.sob6ec2000-b6ec3000 rw-p 00000000 00:00 0b6ec3000-b6ec4000 r--p 00000000 00:00 0b6ec4000-b6ec5000 r--p 00000000 00:00 0 [anon:linker_alloc]b6ec5000-b6ec6000 rw-p 00000000 00:00 0 [anon:linker_alloc]b6ec6000-b6ec7000 rw-p 00000000 00:00 0 [anon:linker_alloc_vector]b6ec7000-b6ec8000 rw-p 00000000 00:00 0 [anon:linker_alloc_32]b6ec8000-b6ec9000 r--p 00000000 00:00 0 [anon:linker_alloc]b6ec9000-b6ee9000 r--s 00000000 00:0f 11859 /dev/__properties__b6ee9000-b6eea000 r--p 00000000 00:00 0b6eea000-b6eeb000 ---p 00000000 00:00 0b6eeb000-b6eed000 rw-p 00000000 00:00 0 [anon:thread signal stack]b6eed000-b6f0a000 r-xp 00000000 103:0f 170 /system/bin/linkerb6f0a000-b6f0b000 r--p 0001c000 103:0f 170 /system/bin/linkerb6f0b000-b6f0d000 rw-p 0001d000 103:0f 170 /system/bin/linkerb6f0d000-b6f0f000 rw-p 00000000 00:00 0b6f80000-beb80000 rw-p 00000000 00:00 0 [anon:libc_malloc]bed01000-bed22000 rw-p 00000000 00:00 0 [stack]bed80000-bee80000 rw-p 00000000 00:00 0 [anon:libc_malloc]bef66000-bef67000 r-xp 00000000 00:00 0 [sigpage]bef67000-bef68000 r--p 00000000 00:00 0 [vvar]bef68000-bef69000 r-xp 00000000 00:00 0 [vdso]ffff0000-ffff1000 r-xp 00000000 00:00 0 [vectors]root@device_gls:/proc/1293 # 我们看 libc_malloc 区域的 的大小 123400080000-7f580000 rw-p 00000000 00:00 0 [anon:libc_malloc] == 2037MB7f600000-b6d00000 rw-p 00000000 00:00 0 [anon:libc_malloc] == 887MBb6f80000-beb80000 rw-p 00000000 00:00 0 [anon:libc_malloc] == 124MBbed80000-bee80000 rw-p 00000000 00:00 0 [anon:libc_malloc] == 1MB 总和是 2037 + 887 + 124 + 1 = 3049MB 这是项目中一个实际的bug，场景是 直播盒子 做长跑测试的时候，每到7天左右，直播业务就会因为malloc失败而导致失败，但是由于业务代码写的问题，没有检查 malloc返回值的原因，导致业务应用会crash, 现象是空指针错误。。很难联想到是因为进程虚拟地址空间全部泄漏导致的问题。业务的小伙伴差了一个多月都没有头绪。。 在 arm32 设备上，User 14 + Nice 0 + Sys 49 + Idle 246 + IOW 1 + IRQ 0 + SIRQ 0 = 310 PID PR CPU% S #THR VSS RSS PCY UID Name 3592 0 0% S 1 3124044K 1688K fg root mytest RSS 也占用了一些，但是远没 X86设备上夸张 123456789101112131415161718192021222324252627282930313233343536373839404142434445Inspiron-5548@ubuntu: ~/workspace# cat /tmp/123 | grep RssRss: 0 kBRss: 12 kBRss: 4 kBRss: 0 kBRss: 256 kBRss: 16 kBRss: 4 kBRss: 4 kBRss: 28 kBRss: 4 kBRss: 4 kBRss: 64 kBRss: 0 kBRss: 4 kBRss: 4 kBRss: 460 kBRss: 16 kBRss: 12 kBRss: 28 kBRss: 536 kBRss: 0 kBRss: 16 kBRss: 4 kBRss: 4 kBRss: 4 kBRss: 4 kBRss: 4 kBRss: 4 kBRss: 4 kBRss: 4 kBRss: 28 kBRss: 4 kBRss: 0 kBRss: 0 kBRss: 116 kBRss: 4 kBRss: 8 kBRss: 8 kBRss: 0 kBRss: 8 kBRss: 0 kBRss: 4 kBRss: 4 kBRss: 0 kB 其中 malloc 的 rss 只占用 256Kb,大部分是 libc, libc++ libm的代码段占用 12345678910117f680000-b6d80000 rw-p 00000000 00:00 0 [anon:libc_malloc]Name: [anon:libc_malloc]Size: 908288 kBRss: 256 kBPss: 256 kBShared_Clean: 0 kBShared_Dirty: 0 kBPrivate_Clean: 0 kBPrivate_Dirty: 256 kBReferenced: 256 kBAnonymous: 256 kB 由此可见 进程虚拟地址空间泄漏还是有可能会存在的，尽管可能概率非常低，对系统危害也比物理内存泄漏小很多。但是一旦发生，他的危害对于业务本身也是致命的，必须重启业务或者系统来恢复。 编程习惯养好，一定需要检查函数返回值。","link":"/2021/01/15/%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86/%E8%BF%9B%E7%A8%8B%E8%99%9A%E6%8B%9F%E5%9C%B0%E5%9D%80%E7%A9%BA%E9%97%B4%E6%B3%84%E6%BC%8F/"},{"title":"linux hungtask问题","text":"hungtask 定义我们先看一段实际hungtask的 dmesg打印 123456789101112131415[45312.818392] INFO: task ftpd:17682 blocked for more than 120 seconds.[45312.818470] Tainted: G OE 5.4.44 #1[45312.818472] &quot;echo 0 &gt; /proc/sys/kernel/hung_task_timeout_secs&quot; disables this message.[45312.818474] ftpd D 0 17682 2 0x80004000[45312.818478] Call Trace:[45312.818698] __schedule+0x2e3/0x740[45312.818700] schedule+0x42/0xb0[45312.818702] schedule_timeout+0x152/0x2f0[45312.818755] ? __next_timer_interrupt+0xe0/0xe0[45312.818756] msleep+0x2e/0x40[45312.818760] ftpd+0xaa/0x170 [ftpd][45312.818804] kthread+0x104/0x140[45312.818806] ? 0xffffffffc05bb000[45312.818807] ? kthread_park+0x90/0x90[45312.818808] ret_from_fork+0x35/0x40 第一行就写了 pid为 17682的 task被 block阻塞了超过120s得不到执行，这种往往只是打印hungtask的线程 和 backtrace而已，实际中一般也不会导致panic，但是这种问题确是我们不能忽略的点，有可能这就是后面会出事故的点 hungtask是内核的一种自我保护行为，在检测到一个线程长时间（可设置）处于D状态之后，会打印出线程相关信息和backtrace. 12sh@ubuntu:/var/crash$ ps -aux |grep ftpdroot 18007 0.0 0.0 0 0 ? D 01:01 0:00 [ftpd] D 状态也是进程的一种状态，对应内核中的 TASK_UNINTERRUPTIBLE 状态，一般等待磁盘IO的线程会设置为 TASK_UNINTERRUPTIBLE 状态，此状态无法wakeup，不管是 wake_up_process，还是kill -9去尝试杀死他 都不能将 TASK_UNINTERRUPTIBLE 状态线程唤醒，只有等他自己wakeup。 同时linux内核在统计系统load的时候也将 TASK_UNINTERRUPTIBLE 状态的现场统计了进去，这样如果系统中出现D状态线程之后，整体系统的load就会较高，但是CPU loading却很小，几乎为0。这种统计方式也使得linux系统中loadavg这个参考指标的意义不是那么的大了。 12345678910sh@ubuntu:~$ toptop - 01:04:54 up 13:09, 3 users, load average: 2.05, 1.06, 0.49Tasks: 341 total, 1 running, 340 sleeping, 0 stopped, 0 zombie%Cpu(s): 0.1 us, 0.1 sy, 0.0 ni, 99.8 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 stMiB Mem : 3716.5 total, 249.5 free, 2034.2 used, 1432.7 buff/cacheMiB Swap: 2048.0 total, 1557.2 free, 490.8 used. 1398.5 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 2253 rlk 20 0 299752 13200 10788 S 0.3 0.3 0:36.83 vmtoolsd 18184 rlk 20 0 20700 4292 3384 R 0.3 0.1 0:00.08 top 什么样的线程会处于D状态，即 TASK_UNINTERRUPTIBLE 状态呢? 123456789101. 磁盘IO线程，在等待磁盘读入数据的过程中会将自己设置为 TASK_UNINTERRUPTIBLE 状态2. 还有一个 msleep()，这个API会先将线程状态设置为 TASK_UNINTERRUPTIBLE,再调用 schedule_timeouot()void msleep(unsigned int msecs){ unsigned long timeout = msecs_to_jiffies(msecs) + 1; while (timeout) timeout = schedule_timeout_uninterruptible(timeout);} config配置 hung_task需要开启如下配置 1234echo &quot;CONFIG_DETECT_HUNG_TASK=y&quot; &gt;&gt; /tmp/.configecho &quot;CONFIG_DEFAULT_HUNG_TASK_TIMEOUT=120&quot; &gt;&gt; /tmp/.configecho &quot;CONFIG_BOOTPARAM_HUNG_TASK_PANIC=y&quot; &gt;&gt; /tmp/.configecho &quot;CONFIG_BOOTPARAM_HUNG_TASK_PANIC_VALUE=1&quot; &gt;&gt; /tmp/.config hungtask 检测的实现上面讲了hungtask的原理，下面通过代码走读的方式来分析一下hungtask实现细节 首先是init, hungtask机制在初始化的时候使用kthread_run 运行了一个 [khungtaskd]线程，hungtask 主要工作就是在 hungtaskd 线程中完成的 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475//这是主要干活的static void check_hung_task(struct task_struct *t, unsigned long timeout){ unsigned long switch_count = t-&gt;nvcsw + t-&gt;nivcsw; if (switch_count != t-&gt;last_switch_count) { //此task 没有hungtask t-&gt;last_switch_count = switch_count; t-&gt;last_switch_time = jiffies; return; } if (time_is_after_jiffies(t-&gt;last_switch_time + timeout * HZ)) return; // 此task虽然hung了，但是还未到 120s == 120 * HZ if (sysctl_hung_task_warnings) { pr_err(&quot;INFO: task %s:%d blocked for more than %ld seconds.\\n&quot;, t-&gt;comm, t-&gt;pid, (jiffies - t-&gt;last_switch_time) / HZ); xxxx //提示信息 sched_show_task(t); }}/* * Check whether a TASK_UNINTERRUPTIBLE does not get woken up for * a really long time (120 seconds). If that happens, print out a warning. */static void check_hung_uninterruptible_tasks(unsigned long timeout){ for_each_process_thread(g, t) { //遍历所有线程 if (t-&gt;state == TASK_UNINTERRUPTIBLE) // check当前线程是否处于D状态 check_hung_task(t, timeout); }}/* * kthread which checks for tasks stuck in D state */static int watchdog(void *dummy){ unsigned long hung_last_checked = jiffies; set_user_nice(current, 0); for ( ; ; ) { // 是内核守护线程，所以直接一个for循环 unsigned long timeout = sysctl_hung_task_timeout_secs; unsigned long interval = sysctl_hung_task_check_interval_secs; long t; if (interval == 0) interval = timeout; interval = min_t(unsigned long, interval, timeout); t = hung_timeout_jiffies(hung_last_checked, interval); if (t &lt;= 0) { // 因为最后调用的是 schedule_timout_interruptible()，可以被唤醒，所以需要判断一下 if (!atomic_xchg(&amp;reset_hung_task, 0) &amp;&amp; !hung_detector_suspended) check_hung_uninterruptible_tasks(timeout); // 给所有线程检查是否处于D状态 hung_last_checked = jiffies; continue; } schedule_timeout_interruptible(t); } return 0;}static int __init hung_task_init(void) // 初始化 khungtaskd，主体是 watchdog 函数{ atomic_notifier_chain_register(&amp;panic_notifier_list, &amp;panic_block); /* Disable hung task detector on suspend */ pm_notifier(hungtask_pm_notify, 0); watchdog_task = kthread_run(watchdog, NULL, &quot;khungtaskd&quot;); return 0;}subsys_initcall(hung_task_init); hungtask机制 如何判断这个线程一直没有进行切换得到运行呢？主要用的是 task_struct的几个成员， nvcsw nivcsw last_switch_count分别对应 非自愿上下文切换次数，自愿上下文切换次数，上次检查时上下文总切换次数。通过在扫描时，比较这几个成员的值就可以清晰的看到在两次扫描之间有没有进行过上下文切换 hungtask机制 如何判断这个线程超过120s都没有得到运行呢？主要用的是 task_struct 的 last_switch_time 成员，利用 last_switch_time + timeout * HZ 和 jiffies 比较就可以轻易得出是否超过120s没有被调度过 hungtask 行为控制一般系统中出现hungtask 可能都是短暂的异常，一般就打印一下 info 和 backtrace 即可，但是kernel也提供了相关的选择，比如可以选择hungtask的时候直接panic 1234echo 1 &gt; /proc/sys/kernel/hung_task_panic # hungtask的时候直接 panicecho 120 &gt; /proc/sys/kernel/hung_task_timeout_secs # 一个任务处于D状态多久我们认为他是hungtask了echo 3 &gt; /proc/sys/kernel/hung_task_check_interval_secs # 内核多久进行一次hungtask扫描echo 7 &gt; /proc/sys/kernel/hung_task_warnings # 设置hungtask的warning的等级 其实这些控制参数都对应在上面代码中，由于篇幅原因，精简了一下代码 总结一般hungtask出现，肯定会伴随着 D状态 的线程产生，且超过设定时间一直处于D状态，没有被调度过 D状态，即 TASK_UNINTERRUPTIBLE 线程，一般产生于 IO磁盘等待上 msleep 调用接口上 一些锁机制在等待锁的持有者的时候也会将线程状态设为 TASK_UNINTERRUPTIBLE，那样如果出现某些类型锁的死锁的时候也会出现 hungtask 贴一个其他博客：进程D状态死锁检测","link":"/2020/09/08/%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/hung_task%E9%97%AE%E9%A2%98/"},{"title":"linux lockup问题","text":"lockup定义lockup是指是指某段内核代码占着CPU不放。 lockup分两种：hard lockup 与 soft lockup，区别是hard lockup是屏蔽系统中断情况下发生的。贴一段原文： 1234Short answer：A ‘soft lockup’ is defined as a bug that causes the kernel to loop in kernel mode for more than 20 seconds […], without giving other tasks a chance to run.A ‘hard lockup’ is defined as a bug that causes the CPU to loop in kernel mode for more than 10 seconds […], without letting other interrupts have a chance to run. 有几个概念需要澄清一下： 发生lockup的一定是内核代码，因为用户态代码都是可以抢占的。 在内核态代码中发生lockup时，一般伴随着 preempt_disable()或者local_irq_disable()，产生lockup 的基本条件是禁止抢占，当然禁止中断这种就更猛了 Soft lockupSoft lockup是指CPU被内核代码一直占用着，系统某个CPU上超过20s其他进程无法得到运行。系统是如何检测 Soft lockup的呢？ 实现机制设立涉及到几个概念：一般优先级的普通进程，最高优先级的watchdog/?内核线程，Hr-timer时钟中断，其中Hr-timer可以打断watchdog/?执行，watchdog/?可以打断普通优先级的进程执行。 Soft lockup机制在初始化的时候会init一个 Hr-timer，定时执行在执行过程中 获取 percpu变量 watchdog_touch_ts 的值 唤醒 watchdog/? 内核线程 比较 当前timestamp 和 watchdog_touch_ts值 内核线程会用当前的time_stamp 更新 watchdog_touch_ts per-cpu 变量watchdog_touch_ts 在 softlockup_fn –&gt; update_touch_ts 中更新 12345678910111213141516static DEFINE_PER_CPU(unsigned long, watchdog_touch_ts);/* Commands for resetting the watchdog */static void update_touch_ts(void){ __this_cpu_write(watchdog_touch_ts, get_timestamp()); update_report_ts();}static int softlockup_fn(void *data){ update_touch_ts(); complete(this_cpu_ptr(&amp;softlockup_completion)); return 0;} softlockup_fn 主要是在 watchdog_timer_fn中使用 stop_one_cpu_nowait 来调用的，watchdog_timer_fn 是 设置的 per-cpu 的 hrtimer的回调函数。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586/* watchdog kicker functions */static enum hrtimer_restart watchdog_timer_fn(struct hrtimer *hrtimer){ unsigned long touch_ts, period_ts, now; struct pt_regs *regs = get_irq_regs(); int duration; int softlockup_all_cpu_backtrace = sysctl_softlockup_all_cpu_backtrace; if (!watchdog_enabled) return HRTIMER_NORESTART; /* kick the hardlockup detector */ watchdog_interrupt_count(); /* kick the softlockup detector */ if (completion_done(this_cpu_ptr(&amp;softlockup_completion))) { // 不是直接的函数调用，只是 queue_work 而已。 reinit_completion(this_cpu_ptr(&amp;softlockup_completion)); stop_one_cpu_nowait(smp_processor_id(), softlockup_fn, NULL, this_cpu_ptr(&amp;softlockup_stop_work)); } ...... /* Check for a softlockup. */ touch_ts = __this_cpu_read(watchdog_touch_ts); duration = is_softlockup(touch_ts, period_ts, now); if (unlikely(duration)) { /* * Prevent multiple soft-lockup reports if one cpu is already * engaged in dumping all cpu back traces. */ if (softlockup_all_cpu_backtrace) { if (test_and_set_bit_lock(0, &amp;soft_lockup_nmi_warn)) return HRTIMER_RESTART; } /* Start period for the next softlockup warning. */ update_report_ts(); pr_emerg(&quot;BUG: soft lockup - CPU#%d stuck for %us! [%s:%d]\\n&quot;, smp_processor_id(), duration, current-&gt;comm, task_pid_nr(current)); print_modules(); print_irqtrace_events(current); if (regs) show_regs(regs); else dump_stack(); if (softlockup_all_cpu_backtrace) { trigger_allbutself_cpu_backtrace(); clear_bit_unlock(0, &amp;soft_lockup_nmi_warn); } add_taint(TAINT_SOFTLOCKUP, LOCKDEP_STILL_OK); if (softlockup_panic) panic(&quot;softlockup: hung tasks&quot;); }}static void watchdog_enable(unsigned int cpu){ struct hrtimer *hrtimer = this_cpu_ptr(&amp;watchdog_hrtimer); struct completion *done = this_cpu_ptr(&amp;softlockup_completion); WARN_ON_ONCE(cpu != smp_processor_id()); init_completion(done); complete(done); /* * Start the timer first to prevent the NMI watchdog triggering * before the timer has a chance to fire. */ hrtimer_init(hrtimer, CLOCK_MONOTONIC, HRTIMER_MODE_REL_HARD); hrtimer-&gt;function = watchdog_timer_fn; hrtimer_start(hrtimer, ns_to_ktime(sample_period), HRTIMER_MODE_REL_PINNED_HARD); /* Initialize timestamp */ update_touch_ts(); /* Enable the perf event */ if (watchdog_enabled &amp; NMI_WATCHDOG_ENABLED) watchdog_nmi_enable(cpu);} stop_one_cpu_nowait 并不是 直接调用 softlockup_fn，只是 queue_work 而已， 1234567891011121314151617181920212223242526272829/* queue @work to @stopper. if offline, @work is completed immediately */static bool cpu_stop_queue_work(unsigned int cpu, struct cpu_stop_work *work){ struct cpu_stopper *stopper = &amp;per_cpu(cpu_stopper, cpu); DEFINE_WAKE_Q(wakeq); unsigned long flags; bool enabled; preempt_disable(); raw_spin_lock_irqsave(&amp;stopper-&gt;lock, flags); enabled = stopper-&gt;enabled; if (enabled) __cpu_stop_queue_work(stopper, work, &amp;wakeq); else if (work-&gt;done) cpu_stop_signal_done(work-&gt;done); raw_spin_unlock_irqrestore(&amp;stopper-&gt;lock, flags); wake_up_q(&amp;wakeq); preempt_enable(); return enabled;}bool stop_one_cpu_nowait(unsigned int cpu, cpu_stop_fn_t fn, void *arg, struct cpu_stop_work *work_buf){ *work_buf = (struct cpu_stop_work){ .fn = fn, .arg = arg, .caller = _RET_IP_, }; return cpu_stop_queue_work(cpu, work_buf);} 此时如果系统中有 代码路径关闭了抢占，且执行了较长时间，就会实际导致 softlockup_fn 这个应该和 hr-timer 发生频率一致的函数得不到执行，从而 watchdog_touch_ts 一直没有更新，最后在 watchdog_timer_fn 中 会根据touch_ts = __this_cpu_read(watchdog_touch_ts); 与 now 的值 比较，得出是否发生了 soft lockup问题 123/* Check for a softlockup. */touch_ts = __this_cpu_read(watchdog_touch_ts);duration = is_softlockup(touch_ts, period_ts, now); 贴一个其他博客：进程R状态死锁检测 Hard lockupHard lockup是指CPU被内核代码一直占用着，系统某个CPU上超过10s无 hr-timer中断发生，就判断为发生了 hard lockup. 实现机制Hard lockup 是基于 hw-PMU 实现的。 12345678910111213141516171819202122232425262728293031323334/** * hardlockup_detector_perf_init - Probe whether NMI event is available at all */int __init hardlockup_detector_perf_init(void){ int ret = hardlockup_detector_event_create(); if (ret) { pr_info(&quot;Perf NMI watchdog permanently disabled\\n&quot;); } else { perf_event_release_kernel(this_cpu_read(watchdog_ev)); this_cpu_write(watchdog_ev, NULL); } return ret;}/* Return 0, if a NMI watchdog is available. Error code otherwise */int __weak __init watchdog_nmi_probe(void){ return hardlockup_detector_perf_init();}void __init lockup_detector_init(void){ if (tick_nohz_full_enabled()) pr_info(&quot;Disabling watchdog on nohz_full cores by default\\n&quot;); cpumask_copy(&amp;watchdog_cpumask, housekeeping_cpumask(HK_FLAG_TIMER)); if (!watchdog_nmi_probe()) nmi_watchdog_available = true; lockup_detector_setup();} 主要的初始化工作是 hardlockup_detector_event_create 做的，将 watchdog_overflow_callback 设置为 pmu nmi interrupt 中断 handler. 1234567891011121314151617181920static int hardlockup_detector_event_create(void){ unsigned int cpu = smp_processor_id(); struct perf_event_attr *wd_attr; struct perf_event *evt; wd_attr = &amp;wd_hw_attr; wd_attr-&gt;sample_period = hw_nmi_get_sample_period(watchdog_thresh); /* Try to register using hardware perf events */ evt = perf_event_create_kernel_counter(wd_attr, cpu, NULL, watchdog_overflow_callback, NULL); if (IS_ERR(evt)) { pr_debug(&quot;Perf event create on CPU %d failed with %ld\\n&quot;, cpu, PTR_ERR(evt)); return PTR_ERR(evt); } this_cpu_write(watchdog_ev, evt); return 0;} 在 watchdog_overflow_callback 中，is_hardlockup 来判断是否发生了 hardlockup.hrtimer_interrupts 是 per-cpu 的变量，在 per-cpu 的 hr-timer 中断handler中增加 hrtimer_interrupts 数值，在 is_hardlockup 中判断是否发生了 hardlockup. 主要利用了 hr-timer 是可屏蔽的中断，但是 HW-pmu 是不可屏蔽的中断。如果本地 cpu 中断禁止了，那么 hr-timer 的 handler就无法执行，但是 HW-pmu 的中断是可以执行的。 123456789101112131415161718192021222324252627282930313233343536373839static void watchdog_interrupt_count(void){ __this_cpu_inc(hrtimer_interrupts);}/* watchdog detector functions */bool is_hardlockup(void){ unsigned long hrint = __this_cpu_read(hrtimer_interrupts); // hr-timer 中增加的 if (__this_cpu_read(hrtimer_interrupts_saved) == hrint) return true; __this_cpu_write(hrtimer_interrupts_saved, hrint); return false;}/* Callback function for perf event subsystem */static void watchdog_overflow_callback(struct perf_event *event, struct perf_sample_data *data, struct pt_regs *regs){ /* Ensure the watchdog never gets throttled */ event-&gt;hw.interrupts = 0; if (__this_cpu_read(watchdog_nmi_touch) == true) { __this_cpu_write(watchdog_nmi_touch, false); return; } if (!watchdog_check_timestamp()) return; if (is_hardlockup()) { int this_cpu = smp_processor_id(); ... } ...} 参考内核如何检测SOFT LOCKUP与HARD LOCKUP？参考softlockup/hardlockup原理详细介绍 lockup 与 hung_task 区别soft lockup：一定是 RU 状态进程触发的， RU 状态进程一直运行，占用CPU超过20s之后，还未有过进程切换，就会出现这个问题。 最好复现方式是: preempt_disable()之后 一直循环等待，不去preempt_enable()，这时候就会触发这个问题。a. spin_lock() / preempt_disable() 12345spin_lock(&amp;lock);while(1) { i = i + 1;}spin_unlock(&amp;lock); b. spinlock的 dead_lock 也会触发这种问题 123456 CPU0 CPU1 spin_lock(&amp;lock_B);spin_lock(&amp;lock_A); spin_lock(&amp;lock_A);spin_lock(&amp;lock_B);xxx 此时 CPU0 CPU1 都会检测到 soft lockup hard lockup：也一定是 RU 状态进程触发的， RU 状态进程一直运行，同时禁止了本地中断。占用CPU超过10s之后，还未有过进程切换且本地中断还未打开，就会出现这个问题。 与 soft lockup 相比，只是加了一个条件是中断关闭。最好复现方式是: local_irq_disable() 之后 一直循环等待，不去 local_irq_enable()，这时候就会触发 hard lockup 这个问题。 1 hung task：hung task 一定是 UN 状态进程触发的，hung_task从 注释可以看到 12345/* * Check whether a TASK_UNINTERRUPTIBLE does not get woken up for * a really long time (120 seconds). If that happens, print out * a warning. */ UN 是 TASK_UNINTERRUPTIBLE 缩写，也就是D状态的线程。","link":"/2020/09/12/%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/lockup%E9%97%AE%E9%A2%98/"},{"title":"关于preempt_count的思考","text":"preempt_conut 本质就是一个int型的数，是每个 task_struct 的 thread_info 的一个成员变量，但是他和系统的调度密切相关，当然也十分重要。 12345struct thread_info { unsigned long flags; /* low level flags */ int preempt_count; /* 0 =&gt; preemptable, &lt;0 =&gt; bug */ .......}; 在 inlcude/linux/preempt.h 文件看相关定义 12345678910111213141516171819202122232425262728293031323334static __always_inline int preempt_count(void){ return READ_ONCE(current_thread_info()-&gt;preempt_count);}/* * PREEMPT_MASK: 0x000000ff * SOFTIRQ_MASK: 0x0000ff00 * HARDIRQ_MASK: 0x000f0000 * NMI_MASK: 0x00100000 * PREEMPT_NEED_RESCHED: 0x80000000 */#define PREEMPT_BITS 8#define SOFTIRQ_BITS 8#define HARDIRQ_BITS 4#define NMI_BITS 1#define PREEMPT_SHIFT 0#define SOFTIRQ_SHIFT (PREEMPT_SHIFT + PREEMPT_BITS)#define HARDIRQ_SHIFT (SOFTIRQ_SHIFT + SOFTIRQ_BITS)#define NMI_SHIFT (HARDIRQ_SHIFT + HARDIRQ_BITS)#define hardirq_count() (preempt_count() &amp; HARDIRQ_MASK)#define softirq_count() (preempt_count() &amp; SOFTIRQ_MASK)#define irq_count() (preempt_count() &amp; (HARDIRQ_MASK | SOFTIRQ_MASK \\ | NMI_MASK))#define in_irq() (hardirq_count())#define in_softirq() (softirq_count())#define in_interrupt() (irq _count())#define in_serving_softirq() (softirq_count() &amp; SOFTIRQ_OFFSET)#define in_nmi() (preempt_count() &amp; NMI_MASK)#define in_task() (!(preempt_count() &amp; \\ (NMI_MASK | HARDIRQ_MASK | SOFTIRQ_OFFSET))) 可以看出 preempt_count 0-7 bit被用来抢占计数，8-15 bit被用来软中断计数，16-19 bit被用来硬件中断计数，20bit 被用来 NMI中断计数，31 bit被用来记录是否需要立即sched. in_interrupt() 这些宏本质也是根据 preempt_count()来判断的。 hard irq在进入irq的时候通过 irq_enter 将preempt_count 的 16-19bit ++，在退出irq的时候通过 irq_exit 将preempt_count 的 16-19bit –，但是由于目前linux中的中断往往是不可嵌套的，所以一般 hardirq 只会用到 16 bit，为什么linux给 hardirq 保留了4bit呢，這是歷史原因造成的，早期hardirq还是可以嵌套的。 irq_enter() 123456789101112131415161718192021222324#define preempt_count_add(val) __preempt_count_add(val)#define __irq_enter() \\ do { \\ account_irq_enter_time(current); \\ preempt_count_add(HARDIRQ_OFFSET); \\ // preempt_count 和 hardirq相关++ trace_hardirq_enter(); \\ } while (0)void irq_enter(void) //进入中断上下文{ rcu_irq_enter(); if (is_idle_task(current) &amp;&amp; !in_interrupt()) { /* * Prevent raise_softirq from needlessly waking up ksoftirqd * here, as softirq will be serviced on return from interrupt. */ local_bh_disable(); tick_irq_enter(); _local_bh_enable(); } __irq_enter();} irq_exit() 123456789101112131415161718#define preempt_count_sub(val) __preempt_count_sub(val)void irq_exit(void) //退出中断上下文{#ifndef __ARCH_IRQ_EXIT_IRQS_DISABLED local_irq_disable();#else lockdep_assert_irqs_disabled();#endif account_irq_exit_time(current); preempt_count_sub(HARDIRQ_OFFSET); // preempt_count 和 hardirq相关-- if (!in_interrupt() &amp;&amp; local_softirq_pending()) invoke_softirq(); tick_irq_exit(); rcu_irq_exit(); trace_hardirq_exit(); /* must be last! */} soft irqpreempt_count中的第8到15个bit表示softirq count，它记录了进入softirq的嵌套次数，如果softirq count的值为正数，说明现在正处于softirq上下文中。由于softirq在单个CPU上是不会嵌套执行的，因此和hardirq count一样，实际只需要一个bit(bit 8)就可以了。还有一种情况，softirq count 会用到不止 bit8，在禁用中断下半部的情况下，每禁用一次softirq count 就会增加1，理论上最多可以嵌套16次。 进入退出软中断的case 1234567891011121314asmlinkage __visible void __softirq_entry __do_softirq(void){ ...... __local_bh_disable_ip(_RET_IP_, SOFTIRQ_OFFSET); ...... while ((softirq_bit = ffs(pending))) { ...... h-&gt;action(h); ...... } ...... __local_bh_enable(SOFTIRQ_OFFSET); ......} 禁止、开启中断下半部的case 123456789101112131415161718192021222324252627282930313233343536void __local_bh_enable_ip(unsigned long ip, unsigned int cnt){ ...xxx}static inline void local_bh_enable_ip(unsigned long ip){ __local_bh_enable_ip(ip, SOFTIRQ_DISABLE_OFFSET);// 开启中断下半部的preempt_count 和 softirq相关++}//开启下半部---------------//禁止下半部static __always_inline void __local_bh_disable_ip(unsigned long ip, unsigned int cnt){ preempt_count_add(cnt); barrier();}static inline void __raw_spin_lock_bh(raw_spinlock_t *lock){ __local_bh_disable_ip(_RET_IP_, SOFTIRQ_LOCK_OFFSET); // 禁止中断下半部的preempt_count 和 softirq相关++ spin_acquire(&amp;lock-&gt;dep_map, 0, 0, _RET_IP_); LOCK_CONTENDED(lock, do_raw_spin_trylock, do_raw_spin_lock);}void __lockfunc _raw_spin_lock_bh(raw_spinlock_t *lock){ __raw_spin_lock_bh(lock);}#define raw_spin_lock_bh(lock) _raw_spin_lock_bh(lock)static __always_inline void spin_lock_bh(spinlock_t *lock){ raw_spin_lock_bh(&amp;lock-&gt;rlock);} 可以看出来执行软中断 和 禁止中断下半部都属于软中断上下文。（我有个疑问，禁止hard irq 属于硬件中断上下文吗？从 in_irq的定义上看不是，但是和软中断不太一样） process context进程上下文，当然不仅仅是进程，只要不是出于NMI、HARD IRQ、SOFT IRQ上下文的都算进程上下文，包括内核线程（包括ksoftirqd、kworker内核线程等） 12#define in_task() (!(preempt_count() &amp; \\ (NMI_MASK | HARDIRQ_MASK | SOFTIRQ_OFFSET))) 中断上下文不会发生进程切换，是一种隐式的进制调度方法。通常也可以使用 preempt_disable 来显式关闭调度，对 preempt_count ++。 atomic context处于中断上下文中(NMI、hard、soft) 或者 禁止抢占的情况下，都属于原子上下文 123456static __always_inline int preempt_count(void){ return READ_ONCE(current_thread_info()-&gt;preempt_count);}#define in_atomic() (preempt_count() != 0) 参考：Linux中的preempt_count","link":"/2020/09/21/%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/%E5%85%B3%E4%BA%8Epreempt_count%E7%9A%84%E6%80%9D%E8%80%83/"},{"title":"内核线程退出不正确导致的问题","text":"什么是内核线程？内核线程就是一直运行在内核态的线程，与一般用户态进程的明显区别是 task-&gt;mm == NULL，就是没有自己的地址空间，所有的内核线程都共享一个地址空间 前段时间遇到一个模块卸载之后，crash的问题，我大概抽象了一下源码： 12345678910111213141516171819202122232425262728int run_flag = 0;struct my_struct {int a; char c;};struct my_struct a;static int les_test(void *arg){ while(run_flag){ msleep(1000); func(&amp;a);// 操作较为耗时 } return 0;}static int les_init(void){ run_flag = 1; task = kthread_run(les_test, NULL, &quot;les_test&quot;); return 0;}static void les_exit(void){ run_flag = 0; msleep(100);}module_init(les_init); module_exit(les_exit);MODULE_LICENSE(&quot;GPL&quot;); MODULE_AUTHOR(&quot;XXXX&quot;); 这个模块在insmod 之后work的很好，偶尔在rmmod的时候就会系统crash查看crash现场之后发现是 123456789101112131415161718192021222324252627282930 PANIC: &quot;Oops: 0010 [#1] SMP PTI&quot; (check log for details) PID: 4270 COMMAND: &quot;test&quot; TASK: ffff8e881d3b5f00 [THREAD_INFO: ffff8e881d3b5f00] CPU: 1 STATE: TASK_RUNNING (PANIC)crash&gt; btPID: 4270 TASK: ffff8e881d3b5f00 CPU: 1 COMMAND: &quot;test&quot; #0 [ffffb9c74092bb58] machine_kexec at ffffffff9546fa63 #1 [ffffb9c74092bbb8] __crash_kexec at ffffffff95557502 #2 [ffffb9c74092bc88] crash_kexec at ffffffff95558289 #3 [ffffb9c74092bca0] oops_end at ffffffff954353a9 #4 [ffffb9c74092bcc8] no_context at ffffffff9547efee #5 [ffffb9c74092bd38] __bad_area_nosemaphore at ffffffff9547f200 #6 [ffffb9c74092bd80] bad_area_nosemaphore at ffffffff9547f366 #7 [ffffb9c74092bd90] do_kern_addr_fault at ffffffff9547fd16 #8 [ffffb9c74092bdb8] __do_page_fault at ffffffff9547fdd7 #9 [ffffb9c74092bde0] do_page_fault at ffffffff9547fe0c#10 [ffffb9c74092be10] page_fault at ffffffff96001284 [exception RIP: unknown or invalid address] RIP: ffffffffc08500aa RSP: ffffb9c74092bec0 RFLAGS: 00010246 RAX: 0000000000000000 RBX: 00004b3f0220450c RCX: 0000000000000000 RDX: 0000000000000000 RSI: 0000000000000246 RDI: 0000000000000000 RBP: ffffb9c74092bf00 R8: 0000000000000000 R9: 0000000000000001 R10: 0000000000100000 R11: 0000000000000000 R12: 00004b3f02204508 R13: 00004b3f02206ac0 R14: 000000000002d020 R15: 0000000000000001 ORIG_RAX: ffffffffffffffff CS: 0010 SS: 0018#11 [ffffb9c74092bf08] kthread at ffffffff954c7124#12 [ffffb9c74092bf50] ret_from_fork at ffffffff96000215 可以看到crash原因是 “Oops: 0010 [#1] SMP PTI”，出问题的test线程正是 内核模块起的线程，为什么内核模块卸载的时候这个test内核线程就出问题了呢? 其实仔细看一下module_exit的源码就会发现，这里用 run_flag 来同步线程是否运行，且 msleep(100)等待 test线程退出。 这种写法就看起来无比诡异 实验我首先将这个 msleep(100) 去掉之后重新编译安装，在rmmod的时候就几乎是100%必现内核crash的问题，这里msleep(100) 也应该是驱动开发者自己debug的时候加上的。看test线程代码我们发现在func(&amp;a)函数中，会使用一个全局变量a。这个操作也比较长，看调用栈也发现大概是这里crash了，再结合是卸载模块的时候crash，应该是可以想到，模块卸载之后模块的相关资源已经被释放，但是这个内核线程由于刚刚msleep(100)结束，并没有退出，此时变量a的资源已经被module_exit释放了，再去将&amp;a 传给func，后续执行crash也不意外了。 这个去掉module_exit 里面msleep(100)的操作，为什么就会导致几乎必现crash问题呢。加上module_exit 里面msleep(100)的操作，概率就会很低了，这是为什么呢。考虑加上msleep(100)的情况，run_flag 被置位，module_exit 会等待100ms再退出，这100ms内大概率test内核线程已经被wakeup起来，然后已经退出了，但是在某些场景下（系统有RT进程，或者load比较重的情况下），test线程刚刚醒来就又被抢占了，而module_exit会释放模块资源，等test线程在此醒来的时候，a的资源其实已经被释放了后续操作很有可能会导致crash。其中test进程何时会再次得到运行呢？答案是不确定的，取决于系统繁忙程度，但只要test一运行就会导致crash。 模块加载、模块卸载都干了啥 修改其实熟悉内核线程API接口的人看到这抽象的源码基本一眼就能看出端倪，内核线程的框架本来就不是这么用的修改之后源码如下 123456789101112131415161718192021222324252627int run_flag = 0;struct my_struct {int a; char c;};struct my_struct a;static int les_test(void *arg){ while(kthread_should_stop){ msleep(1000); func(&amp;a);// 操作较为耗时 } return 0;}static int les_init(void){ run_flag = 1; task = kthread_run(les_test, NULL, &quot;les_test&quot;); return 0;}static void les_exit(void){ kthread_stop(task);}module_init(les_init); module_exit(les_exit);MODULE_LICENSE(&quot;GPL&quot;); MODULE_AUTHOR(&quot;XXXX&quot;); 其中 kthread_stop会设置 kthread的 flag为 KTHREAD_SHOULD_STOP，然后等待 test 内核线程退出 1234567891011121314int kthread_stop(struct task_struct *k){ struct kthread *kthread; int ret; kthread = to_kthread(k); set_bit(KTHREAD_SHOULD_STOP, &amp;kthread-&gt;flags); kthread_unpark(k); wake_up_process(k); wait_for_completion(&amp;kthread-&gt;exited); ret = k-&gt;exit_code; return ret;} 关于内核线程API的实现后面再讲内核线程API","link":"/2020/09/06/%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/%E5%86%85%E6%A0%B8%E7%BA%BF%E7%A8%8B%E9%80%80%E5%87%BA%E4%B8%8D%E6%AD%A3%E7%A1%AE%E5%AF%BC%E8%87%B4%E7%9A%84%E9%97%AE%E9%A2%98/"},{"title":"bpftrace","text":"bpf epbf bcc bpftrace 关系bpf 与 ebpf关系BPF 是 Berkeley Packet Filter简称，现在也被称为cBPF(classic BPF)，要用于 tcpdump 和 seccomp，BPF早在1992 年就诞生了。 eBPF 是Enahnced Berkeley Packet Filter简称，诞生于2011年。其实 eBPF 本身其实远没有想象中的复杂，我们可以把内核想象成一个庞大而复杂的电路，如果电路出现了异常，我们可以通过使用万用表测量电路，如果当前电路不能满足需求，但又不能推倒重新设计，我们需要串入其他元件。eBPF 之于电路，既是万用表，也是各种功能元件。再回到内核中的 eBPF，内核预先在各个关键路径埋设了 eBPF 程序入口，用户可以编写不同类型的 eBPF 程序，将 eBPF 程序 attach 在内核中不同路径中执行。 bcc 与 ebpfbcc 是 ebpf compile collection简称，是一个包含丰富的内核跟踪分析的 eBPF 工具集，使得 “编 写 BPF 代码-编译成字节码-注入内核-获取结果-展示” 整个过程更加便捷。 bpftrace 与 ebpfbpftrace 可以动态跟踪分析内核，bpftrace 提供了一种类 awk 和 C 的语言，使用 bpftrace 语言编写各种跟踪和分析脚本，并编译成 eBPF 字节码与内核交互，从而实现动态跟踪 Linux 内核。 总的来说bcc 和 bpftrace都是基于 ebpf特性实现的工具，实际使用中我发现bpftrace尤其好用相对于ftrace、perf等。后面会有相应的对比分析。 bpftrace一行代码告诉你性能问题在哪下面主要是 bpftrace 的一些使用方法 列出所有探测点，并且可以添加搜索项，也可以管道传递给grep 1&quot;bpftrace -l&quot; 观察文件open事件 1bpftrace -e 'tracepoint:syscalls:sys_enter_openat { printf(&quot;%s %s\\n&quot;, comm, str(args-&gt;filename)); }' 进程系统调用数量统计 1bpftrace -e 'tracepoint:raw_syscalls:sys_enter { @[comm] = count(); }' read系统调用的分布/xxx/: 可以过滤{}中执行的条件，一般可以是进程名，pid等 1bpftrace -e 'tracepoint:syscalls:sys_exit_read /pid == 18644/ { @bytes = hist(args-&gt;ret); }' 分析内核实时函数栈profile:是采样 1bpftrace -e 'profile:hz:99 { @[kstack] = count(); }' 调度器跟踪 1bpftrace -e 'tracepoint:sched:sched_switch { @[kstack] = count(); }' 具体有哪些参数变量可以使用，可以参考 /sys/kernel/debug/tracing/events/ 里面具体某个跟踪项的 format 选项，里面列出了可以使用的变量，结构等 12345678910111213141516171819sh@ubuntu[root]:/sys/kernel/debug/tracing/events/sched/sched_switch# cat formatname: sched_switchID: 323format: field:unsigned short common_type; offset:0; size:2; signed:0; field:unsigned char common_flags; offset:2; size:1; signed:0; field:unsigned char common_preempt_count; offset:3; size:1; signed:0; field:int common_pid; offset:4; size:4; signed:1; field:char prev_comm[16]; offset:8; size:16; signed:1; field:pid_t prev_pid; offset:24; size:4; signed:1; field:int prev_prio; offset:28; size:4; signed:1; field:long prev_state; offset:32; size:8; signed:1; field:char next_comm[16]; offset:40; size:16; signed:1; field:pid_t next_pid; offset:56; size:4; signed:1; field:int next_prio; offset:60; size:4; signed:1;print fmt: &quot;prev_comm=%s prev_pid=%d prev_prio=%d prev_state=%s%s ==&gt; next_comm=%s next_pid=%d next_prio=%d&quot;, REC-&gt;prev_comm, REC-&gt;prev_pid, REC-&gt;prev_prio, (REC-&gt;prev_state &amp; ((((0x0000 | 0x0001 | 0x0002 | 0x0004 | 0x0008 | 0x0010 | 0x0020 | 0x0040) + 1) &lt;&lt; 1) - 1)) ? __print_flags(REC-&gt;prev_state &amp; ((((0x0000 | 0x0001 | 0x0002 | 0x0004 | 0x0008 | 0x0010 | 0x0020 | 0x0040) + 1) &lt;&lt; 1) - 1), &quot;|&quot;, { 0x0001, &quot;S&quot; }, { 0x0002, &quot;D&quot; }, { 0x0004, &quot;T&quot; }, { 0x0008, &quot;t&quot; }, { 0x0010, &quot;X&quot; }, { 0x0020, &quot;Z&quot; }, { 0x0040, &quot;P&quot; }, { 0x0080, &quot;I&quot; }) : &quot;R&quot;, REC-&gt;prev_state &amp; (((0x0000 | 0x0001 | 0x0002 | 0x0004 | 0x0008 | 0x0010 | 0x0020 | 0x0040) + 1) &lt;&lt; 1) ? &quot;+&quot; : &quot;&quot;, REC-&gt;next_comm, REC-&gt;next_pid, REC-&gt;next_priosh@ubuntu[root]:/sys/kernel/debug/tracing/events/sched/sched_switch# kprobes 跟踪12345678root@ubuntu-Inspiron-5548:~# bpftrace -e 'kprobe:do_sys_open {printf(&quot;[pid-%d:%s]: do_sys_open %s\\n&quot;, pid,comm, str(arg1))}'Attaching 1 probe...[pid-42080:gsd-housekeepin]: do_sys_open /etc/fstab[pid-42080:gsd-housekeepin]: do_sys_open /proc/self/mountinfo[pid-42080:gsd-housekeepin]: do_sys_open /run/mount/utab[pid-42080:gsd-housekeepin]: do_sys_open /proc/self/mountinfo[pid-42080:gsd-housekeepin]: do_sys_open /run/mount/utab^C 利用 kprobes 可以直接判断有没有执行到函数某一行，函数的参数等。 1234amd_server@ubuntu: ~/workspace/linux-stable/out# sudo bpftrace -e 'kretprobe:do_sys_open { printf(&quot;comm:%s, pid:%d, returned: %d\\n&quot;, comm, pid ,retval)}' Attaching 1 probe...comm:node, pid:4158123, returned: 24comm:node, pid:4158123, returned: 24 更多的需要去动手实践，在项目中灵活运用 bpftrace 这样的神器更多资料可以参考：bpftrace的一行代码中文指引brendan大神的博客bpftrace官方guide宋宝华大师的总结","link":"/2020/09/12/%E5%86%85%E6%A0%B8%E8%A7%82%E6%B5%8B/bpf%E7%9B%B8%E5%85%B3/bpftrace/"},{"title":"ftrace可以看的几个参数","text":"Ftrace是一个内部跟踪器，旨在帮助系统的开发人员和设计人员查找内核内部发生的情况。 它可以用于调试或分析在用户空间之外发生的延迟和性能问题。 尽管通常将ftrace视为函数跟踪器，但实际上它是多个分类跟踪实用程序的框架。 可以进行延迟跟踪，以检查禁用和启用中断之间的情况以及抢占以及从唤醒任务到计划任务的时间。 ftrace的最常见用途之一是事件跟踪。 整个内核中有数百个静态事件点，可以通过tracefs文件系统启用这些事件点，以查看内核某些部分的情况。 irqsoff我们都知道linux执行中断的时候都是关中断的，也不存在什么中断嵌套，中断被禁时CPU无法响应任何其他外部事件（除了NMI和SMI）。万一某个驱动开发者代码没有注意将临界区设置的比较大，或者中断处理函数中执行了较多内容，就会直接导致调度延迟增大，直接表现为业务抖动较大，我们有没有什么手段观测这个关中断的时间长短呢?答案是肯定的： 12sh@ubuntu[root]:/sys/kernel/debug/tracing# cat /sys/kernel/debug/tracing/available_tracers hwlat blk mmiotrace function_graph wakeup_dl wakeup_rt wakeup irqsoff function nop irqsoff 这个 tracer 就是来完成这个使命的 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647sh@ubuntu[root]:~# echo 0 &gt; /sys/kernel/debug/tracing/tracing_on sh@ubuntu[root]:~# echo irqsoff &gt; /sys/kernel/debug/tracing/current_tracersh@ubuntu[root]:~# echo 0 &gt; /sys/kernel/debug/tracing/tracing_max_latency sh@ubuntu[root]:~# echo 1 &gt; /sys/kernel/debug/tracing/tracing_on sh@ubuntu[root]:~# sleep 5sh@ubuntu[root]:~# echo 0 &gt; /sys/kernel/debug/tracing/tracing_on sh@ubuntu[root]:~# cat /sys/kernel/debug/tracing/trace | head -n 30# tracer: irqsoff## irqsoff latency trace v1.1.5 on 5.4.44# --------------------------------------------------------------------# latency: 769 us, #15/15, CPU#2 | (M:desktop VP:0, KP:0, SP:0 HP:0 #P:4)# -----------------# | task: kworker/2:0-5528 (uid:0 nice:0 policy:0 rt_prio:0)# -----------------# =&gt; started at: e1000_update_stats# =&gt; ended at: e1000_update_stats### _------=&gt; CPU# # / _-----=&gt; irqs-off # | / _----=&gt; need-resched # || / _---=&gt; hardirq/softirq # ||| / _--=&gt; preempt-depth # |||| / delay # cmd pid ||||| time | caller # \\ / ||||| \\ | / kworker/-5528 2d... 0us!: _raw_spin_lock_irqsave &lt;-e1000_update_statskworker/-5528 2d... 609us : e1000_read_phy_reg &lt;-e1000_update_statskworker/-5528 2d... 610us : _raw_spin_lock_irqsave &lt;-e1000_read_phy_regkworker/-5528 2d... 620us : __const_udelay &lt;-e1000_read_phy_regkworker/-5528 2d... 620us+: delay_tsc &lt;-__const_udelaykworker/-5528 2d... 678us : _raw_spin_unlock_irqrestore &lt;-e1000_read_phy_regkworker/-5528 2d... 679us : e1000_read_phy_reg &lt;-e1000_update_statskworker/-5528 2d... 679us : _raw_spin_lock_irqsave &lt;-e1000_read_phy_regkworker/-5528 2d... 687us : __const_udelay &lt;-e1000_read_phy_regkworker/-5528 2d... 688us+: delay_tsc &lt;-__const_udelaykworker/-5528 2d... 770us : tracer_hardirqs_on &lt;-e1000_update_statskworker/-5528 2d... 774us : &lt;stack trace&gt; =&gt; e1000_update_stats =&gt; e1000_watchdog =&gt; process_one_work =&gt; worker_thread =&gt; kthread =&gt; ret_from_forksh@ubuntu[root]:~# cat /sys/kernel/debug/tracing/tracing_max_latency 769 可以看到 tracing_max_latency 是 769，意味着系统最长关中断时间就是 769us.(要重置最大值，需要将0回显到tracing_max_latency中) 这是未设置函数跟踪的结果 可以 echo 1 &gt; /sys/kernel/debug/tracing/options/function-trace 获取更详细输出 preemptoff禁用抢占功能后，我们可能会收到中断，但无法抢占该任务，优先级较高的任务必须等待再次启用抢占功能，才能抢占优先级较低的任务。 preemptoff跟踪程序将跟踪禁用抢占的位置。 与irqsoff跟踪器一样，它记录禁用了抢占的最大延迟。 preemptoff跟踪器的控制与irqsoff跟踪器非常相似。 1234567891011121314151617181920212223242526272829303132333435# echo preemptoff &gt; current_tracer# echo 1 &gt; tracing_on# echo 0 &gt; tracing_max_latency# ls -ltr[...]# echo 0 &gt; tracing_on# cat trace# tracer: preemptoff## preemptoff latency trace v1.1.5 on 3.8.0-test+# --------------------------------------------------------------------# latency: 46 us, #4/4, CPU#1 | (M:preempt VP:0, KP:0, SP:0 HP:0 #P:4)# -----------------# | task: sshd-1991 (uid:0 nice:0 policy:0 rt_prio:0)# -----------------# =&gt; started at: do_IRQ# =&gt; ended at: do_IRQ### _------=&gt; CPU## / _-----=&gt; irqs-off# | / _----=&gt; need-resched# || / _---=&gt; hardirq/softirq# ||| / _--=&gt; preempt-depth# |||| / delay# cmd pid ||||| time | caller# \\ / ||||| \\ | / sshd-1991 1d.h. 0us+: irq_enter &lt;-do_IRQ sshd-1991 1d..1 46us : irq_exit &lt;-do_IRQ sshd-1991 1d..1 47us+: trace_preempt_on &lt;-do_IRQ sshd-1991 1d..1 52us : &lt;stack trace&gt; =&gt; sub_preempt_count =&gt; irq_exit =&gt; do_IRQ =&gt; ret_from_intr preemptirqsoffpreemptirqsoff 是以上两者的和，会显示禁止 抢占或者中断的最长时间比如 1234567local_irq_disable();call_function_with_irqs_off();preempt_disable();call_function_with_irqs_and_preemption_off();local_irq_enable();call_function_with_preemption_off();preempt_enable(); wakeup、wakeup_rtwakeup、wakeup_rt 是 trace一个进程从进入CPU的就绪队列到真正被执行的时间的一个tracer，即跟踪调度器的延迟。 参考ftrace 内核文档","link":"/2020/09/19/%E5%86%85%E6%A0%B8%E8%A7%82%E6%B5%8B/ftrace/ftrace%E5%8F%AF%E4%BB%A5%E7%9C%8B%E7%9A%84%E5%87%A0%E4%B8%AA%E5%8F%82%E6%95%B0/"},{"title":"vmtouch 观测文件page cache","text":"vmtouch 介绍便携式文件系统缓存诊断和控制 是 vmtouch 作者对于vmtouch的的定义。首先他可以很方便的知道某一个文件当前有多少在 kernel memory 里面作为pagecache 存在。 123456789101112131415ubuntu@ubuntu-Inspiron-5548:~$ vmtouch haha Files: 1 Directories: 0 Resident Pages: 11584/1024000 45M/3G 1.13% Elapsed: 0.031335 secondsubuntu@ubuntu-Inspiron-5548:~$ubuntu@ubuntu-Inspiron-5548:~$ubuntu@ubuntu-Inspiron-5548:~$ cat haha^Cubuntu@ubuntu-Inspiron-5548:~$ vmtouch haha Files: 1 Directories: 0 Resident Pages: 36800/1024000 143M/3G 3.59% Elapsed: 0.033472 secondsubuntu@ubuntu-Inspiron-5548:~$ 明显可以看到在经过 cat 访问之后 文件更多部分被读入 memory，作为 pagecache。 作者对于他的功能介绍： 12345671. 发现你的操作系统正在缓存哪些文件2. 告诉操作系统缓存或清除某些文件或文件区域3. 将文件锁定在内存中，这样操作系统就不会删除它们4. 在服务器故障转移时保留虚拟内存配置文件5. 保持“热备”文件服务器6. 绘制文件系统缓存随时间的使用情况7. 维护缓存使用的“软配额” vmtouch 使用控制增加 pagecache可以将整个文件读入内存，其实我们通过访问这个文件（从头到尾）也可以做到 12345678910111213141516171819202122232425262728ubuntu@ubuntu-Inspiron-5548:~$ vmtouch -vt hahahaha[OOo ] 44737/1024000[OOOOOo ] 90849/1024000[OOOOOOOOo ] 150209/1024000[OOOOOOOOOOOOo ] 215937/1024000[OOOOOOOOOOOOOOOo ] 271489/1024000[OOOOOOOOOOOOOOOOOOOo ] 326305/1024000[OOOOOOOOOOOOOOOOOOOOOo ] 370721/1024000[OOOOOOOOOOOOOOOOOOOOOOOOo ] 414305/1024000[OOOOOOOOOOOOOOOOOOOOOOOOOOOo ] 469697/1024000[OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOo ] 524289/1024000[OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOo ] 577153/1024000[OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOo ] 652481/1024000[OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOo ] 705953/1024000[OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOo ] 757281/1024000[OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOo ] 810593/1024000[OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOo ] 876769/1024000[OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOo ] 920545/1024000[OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOo ] 973473/1024000[OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO] 1024000/1024000 Files: 1 Directories: 0 Touched Pages: 1024000 (3G) Elapsed: 9.4376 secondsubuntu@ubuntu-Inspiron-5548:~$ 控制减少 pagecacheevict a file from memory将一个文件的pagecache 从内存中移除 1234567ubuntu@ubuntu-Inspiron-5548:~$ vmtouch -ve hahaEvicting haha Files: 1 Directories: 0 Evicted Pages: 1024000 (3G) Elapsed: 0.085208 seconds 与drop cache不同，vmtouch做到了精准控制单个文件page_cache的效果，而 drop cache不行 12root@ubuntu-Inspiron-5548:/home/ubuntu# echo 1 &gt; /proc/sys/vm/drop_cachesroot@ubuntu-Inspiron-5548:/home/ubuntu# echo 3 &gt; /proc/sys/vm/drop_caches 保持文件在pagecache中1234ubuntu@ubuntu-Inspiron-5548:~$ vmtouch -dl hahaubuntu@ubuntu-Inspiron-5548:~$ vmtouch: FATAL: mlock: haha (Cannot allocate memory)ubuntu@ubuntu-Inspiron-5548:~$ (内存4G，文件4G 是没办法将文件常驻在 内存中的)从此报错信息可以看出 vmtouch 也是通过 mlock 系统调用来实现 文件内容 或者 文件目录内容锁定在 内存中的 vmtouch 作者的文章","link":"/2021/01/07/%E5%86%85%E6%A0%B8%E8%A7%82%E6%B5%8B/tools/vmtouch%20%E8%A7%82%E6%B5%8B%E6%96%87%E4%BB%B6page%20cache/"},{"title":"60s定位内核性能瓶颈","text":"这篇主要是翻译Brendan的文章perf in 60s和自己见解。主要是在linux性能分析前60s，你应该做什么。 uptime通过 uptime 可以看到系统的最近1min,5min,15min的平均负载load情况，可以知道此时系统的繁忙情况，也可以看出一段时间内系统变得 busier 或者 quiter. 12sh@ubuntu[root]:~# uptime 02:29:52 up 6 days, 14:34, 2 users, load average: 0.00, 0.01, 0.00 dmesg | tail通过 dmesg | tail 可以看到系统的最近有没有有用的错误日志信息 1234567891011sh@ubuntu[root]:~# dmesg | tail[527247.600091] e1000: ens33 NIC Link is Down[527259.696854] e1000: ens33 NIC Link is Up 1000 Mbps Full Duplex, Flow Control: None[534840.083000] e1000: ens33 NIC Link is Down[534844.115753] e1000: ens33 NIC Link is Up 1000 Mbps Full Duplex, Flow Control: None[534846.131190] e1000: ens33 NIC Link is Down[534850.162855] e1000: ens33 NIC Link is Up 1000 Mbps Full Duplex, Flow Control: None[534906.610317] e1000: ens33 NIC Link is Down[534910.643119] e1000: ens33 NIC Link is Up 1000 Mbps Full Duplex, Flow Control: None[534912.657953] e1000: ens33 NIC Link is Down[534916.690646] e1000: ens33 NIC Link is Up 1000 Mbps Full Duplex, Flow Control: None vmstat通过 vmstat 可以看到系统一个overall的视图，包括 CPU MEM IO Inturrupts 123456sh@ubuntu[root]:~# vmstat 1procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 0 0 590592 150108 202604 766576 0 0 6 3 13 15 0 0 100 0 0 0 0 590592 150108 202604 766576 0 0 0 0 289 523 0 0 100 0 0 0 0 590592 150068 202604 766616 0 0 0 0 320 581 0 0 100 0 0 mpstat 通过 mpstat -P ALL 1 可以看到系统的每个 CPU的状态，看看CPU使用是否balance，有时候会有线程绑核出现一些问题 12345678910111213141516sh@ubuntu[root]:~# mpstat -P ALL 1Linux 5.4.44 (rlk) 2020年09月13日 _x86_64_ (4 CPU)02时36分15秒 CPU %usr %nice %sys %iowait %irq %soft %steal %guest %gnice %idle02时36分16秒 all 0.00 0.00 0.00 0.00 0.00 0.25 0.00 0.00 0.00 99.7502时36分16秒 0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 100.0002时36分16秒 1 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 100.0002时36分16秒 2 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 100.0002时36分16秒 3 0.00 0.00 0.00 0.00 0.00 0.99 0.00 0.00 0.00 99.0102时36分16秒 CPU %usr %nice %sys %iowait %irq %soft %steal %guest %gnice %idle02时36分17秒 all 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 100.0002时36分17秒 0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 100.0002时36分17秒 1 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 100.0002时36分17秒 2 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 100.0002时36分17秒 3 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 100.00 pidstat通过 pidstat 1 可以看到系统中最繁忙的几个进程 12345678910111213141516sh@ubuntu[root]:~# pidstat 1Linux 5.4.44 (rlk) 2020年09月13日 _x86_64_ (4 CPU)02时38分37秒 UID PID %usr %system %guest %wait %CPU CPU Command02时38分38秒 0 943 0.93 0.93 0.00 0.00 1.87 0 containerd02时38分38秒 1000 3282 0.00 0.93 0.00 0.00 0.93 3 sshd02时38分38秒 0 78276 0.00 1.87 0.00 0.00 1.87 2 pidstat02时38分38秒 UID PID %usr %system %guest %wait %CPU CPU Command02时38分39秒 1000 15201 0.00 1.00 0.00 0.00 1.00 0 node02时38分39秒 0 77927 0.00 1.00 0.00 0.00 1.00 2 kworker/2:0-events02时38分39秒 0 78276 0.00 1.00 0.00 0.00 1.00 2 pidstat02时38分39秒 UID PID %usr %system %guest %wait %CPU CPU Command02时38分40秒 1000 3347 0.00 1.00 0.00 0.00 1.00 0 node02时38分40秒 0 78276 1.00 0.00 0.00 0.00 1.00 2 pidstat iostat通过 iostat -xz 1 可以看到系统中disk io的状态 12345678910111213141516171819202122232425sh@ubuntu[root]:~# iostat -xz 1Linux 5.4.44 (rlk) 2020年09月13日 _x86_64_ (4 CPU)avg-cpu: %user %nice %system %iowait %steal %idle 0.05 0.01 0.09 0.01 0.00 99.84Device r/s rkB/s rrqm/s %rrqm r_await rareq-sz w/s wkB/s wrqm/s %wrqm w_await wareq-sz d/s dkB/s drqm/s %drqm d_await dareq-sz aqu-sz %utilloop0 0.00 0.00 0.00 0.00 0.12 1.48 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00loop1 0.00 0.00 0.00 0.00 0.11 2.26 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00loop2 0.04 0.04 0.00 0.00 0.45 1.05 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00loop3 0.01 0.01 0.00 0.00 0.24 1.09 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00loop4 0.00 0.00 0.00 0.00 0.15 7.37 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00loop5 0.02 0.02 0.00 0.00 0.49 1.02 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00loop6 0.00 0.00 0.00 0.00 0.13 1.89 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00loop7 0.05 0.05 0.00 0.00 0.41 1.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00loop8 0.05 0.05 0.00 0.00 0.67 1.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00loop9 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00sda 1.00 22.93 0.36 26.65 0.56 23.03 0.65 10.07 0.60 47.67 0.36 15.39 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.06avg-cpu: %user %nice %system %iowait %steal %idle 0.00 0.00 0.25 0.00 0.00 99.75Device r/s rkB/s rrqm/s %rrqm r_await rareq-sz w/s wkB/s wrqm/s %wrqm w_await wareq-sz d/s dkB/s drqm/s %drqm d_await dareq-sz aqu-sz %utilsda 2.00 12.00 0.00 0.00 0.50 6.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.40 free通过 free 可以看到系统中内存的使用情况1234sh@ubuntu[root]:~# free total used free shared buff/cache availableMem: 3805660 2729828 158612 3108 917220 784960Swap: 2097148 595968 1501180 sar通过 sar -n DEV 1 可以看出系统中网络收发是否有问题通过 sar -n TCP,ETCP 1 可以看出系统中TCP连接是否有问题 1234567891011121314sh@ubuntu[root]:~# sar -n DEV 1Linux 5.4.44 (rlk) 2020年09月13日 _x86_64_ (4 CPU)02时44分23秒 IFACE rxpck/s txpck/s rxkB/s txkB/s rxcmp/s txcmp/s rxmcst/s %ifutil02时44分24秒 ens33 4.00 5.00 0.49 0.58 0.00 0.00 0.00 0.0002时44分24秒 lo 11.00 11.00 0.91 0.91 0.00 0.00 0.00 0.0002时44分24秒 IFACE rxpck/s txpck/s rxkB/s txkB/s rxcmp/s txcmp/s rxmcst/s %ifutil02时44分25秒 ens33 7.00 9.00 0.68 1.56 0.00 0.00 0.00 0.0002时44分25秒 lo 18.00 18.00 1.93 1.93 0.00 0.00 0.00 0.0002时44分25秒 IFACE rxpck/s txpck/s rxkB/s txkB/s rxcmp/s txcmp/s rxmcst/s %ifutil02时44分26秒 ens33 2.00 3.00 0.23 0.96 0.00 0.00 0.00 0.0002时44分26秒 lo 6.00 6.00 1.10 1.10 0.00 0.00 0.00 0.00 1234567891011121314sh@ubuntu[root]:~# sar -n TCP,ETCP 1Linux 5.4.44 (rlk) 2020年09月13日 _x86_64_ (4 CPU)02时45分45秒 active/s passive/s iseg/s oseg/s02时45分46秒 0.00 0.00 16.00 16.0002时45分45秒 atmptf/s estres/s retrans/s isegerr/s orsts/s02时45分46秒 0.00 0.00 0.00 0.00 0.0002时45分46秒 active/s passive/s iseg/s oseg/s02时45分47秒 0.00 0.00 16.00 18.0002时45分46秒 atmptf/s estres/s retrans/s isegerr/s orsts/s02时45分47秒 0.00 0.00 0.00 0.00 0.00 top如果一般没啥问题，一般会以top结尾1234567891011121314top - 02:47:39 up 6 days, 14:52, 2 users, load average: 0.16, 0.07, 0.01Tasks: 341 total, 1 running, 340 sleeping, 0 stopped, 0 zombie%Cpu(s): 0.0 us, 1.5 sy, 0.0 ni, 98.5 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 stMiB Mem : 3716.5 total, 149.4 free, 2670.2 used, 896.9 buff/cacheMiB Swap: 2048.0 total, 1466.0 free, 582.0 used. 762.2 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 1 root 20 0 168136 8196 5688 S 0.0 0.2 0:17.47 systemd 2 root 20 0 0 0 0 S 0.0 0.0 0:00.61 kthreadd 3 root 0 -20 0 0 0 I 0.0 0.0 0:00.00 rcu_gp 4 root 0 -20 0 0 0 I 0.0 0.0 0:00.00 rcu_par_gp 6 root 0 -20 0 0 0 I 0.0 0.0 0:00.00 kworker/0:0H-kblockd 9 root 0 -20 0 0 0 I 0.0 0.0 0:00.00 mm_percpu_wq 10 root 20 0 0 0 0 S 0.0 0.0 0:00.42 ksoftirqd/0","link":"/2020/09/13/%E5%86%85%E6%A0%B8%E8%A7%82%E6%B5%8B/%E6%80%A7%E8%83%BD%E7%A8%B3%E5%AE%9A%E6%80%A7/60s%E5%AE%9A%E4%BD%8D%E5%86%85%E6%A0%B8%E6%80%A7%E8%83%BD%E7%93%B6%E9%A2%88/"},{"title":"快速消耗完物理内存","text":"题目之前遇到一个面试，是写一个快速消耗完物理内存的代码 需要注意的是是物理内存，很多不了解内核的同学往往会忽视这一点主要考察的是linux 内存分配的延迟分配策略 内存分配的延迟分配主要是说linux 应用程序在通过brk系统调用向linux内核申请内存时，都是申请的虚拟内存，只有在这个内存真正被使用（访问）的时候，kernel会发生缺页中断，进而去分配物理内存。 解答那这个问题就变成了，分配很多内存，且分配之后去访问一下，让内核产生缺页中断，给申请的虚拟内存分配物理内存 12345678910111213141516#include &lt;stdio.h&gt;int main(int argc, char **argv){ int *p = NULL; int i = 0, j = 0; for (i = 0; i &lt; (1 &lt;&lt; 30); i++) { for (j = 0; i &lt; (1 &lt;&lt; 30); j++) { p = (int *)malloc(4096); p = 1; } } return 0;} 我在ubuntu20.04上运行这个代码之后，几秒钟后这个a.out就被oomkill干掉了dmesg 最后打印的消息是 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677[992416.124437] node invoked oom-killer: gfp_mask=0x100cca(GFP_HIGHUSER_MOVABLE), order=0, oom_score_adj=0[992416.128825] CPU: 1 PID: 44301 Comm: node Kdump: loaded Not tainted 5.4.44 #1[992416.128825] Hardware name: VMware, Inc. VMware Virtual Platform/440BX Desktop Reference Platform, BIOS 6.00 07/29/2019[992416.128826] Call Trace:[992416.144917] dump_stack+0x6d/0x9a[992416.145738] dump_header+0x4f/0x1eb[992416.145742] oom_kill_process.cold+0xb/0x10[992416.145743] out_of_memory+0x1bc/0x490[992416.145777] __alloc_pages_slowpath+0xd5e/0xe50[992416.145793] ? __switch_to_asm+0x40/0x70[992416.145795] __alloc_pages_nodemask+0x2d0/0x320[992416.145811] alloc_pages_current+0x87/0xe0[992416.146058] __page_cache_alloc+0x72/0x90[992416.146061] pagecache_get_page+0xbf/0x300[992416.146062] filemap_fault+0x69a/0xa40[992416.146143] ? unlock_page_memcg+0x12/0x20[992416.146166] ? page_add_file_rmap+0xff/0x1a0[992416.146181] ? xas_load+0xd/0x80[992416.146182] ? xas_find+0x17f/0x1c0[992416.146184] ? filemap_map_pages+0x24c/0x380[992416.146458] ext4_filemap_fault+0x32/0x46[992416.146475] __do_fault+0x3c/0x130[992416.146477] __handle_mm_fault+0xff5/0x1700[992416.146723] ? hrtimer_init_sleeper+0x90/0x90[992416.146726] handle_mm_fault+0xca/0x200[992416.147382] do_user_addr_fault+0x1f9/0x450[992416.147385] __do_page_fault+0x58/0x90[992416.147386] do_page_fault+0x2c/0xe0[992416.147387] page_fault+0x34/0x40[992416.147623] RIP: 0033:0x7f9a73ac1210[992416.147628] Code: Bad RIP value.[992416.147629] RSP: 002b:00007ffc77fb2448 EFLAGS: 00010213[992416.147646] RAX: 00007f9a739de698 RBX: 000000000000000b RCX: 00007f9a73b03266[992416.147646] RDX: 0000000000000400 RSI: 00007ffc77fb24a0 RDI: 0000000000000001[992416.147974] RBP: 00007ffc77fb5560 R08: 0000000000000000 R09: 0000000000000008[992416.147975] R10: 0000000000001388 R11: 0000000000000000 R12: 0000000000001388[992416.147976] R13: 0000000000000000 R14: 0000000002887f98 R15: 0000000002887f40[992416.148030] Mem-Info:[992416.148188] active_anon:478453 inactive_anon:255010 isolated_anon:0 active_file:161 inactive_file:27 isolated_file:0 unevictable:16 dirty:0 writeback:0 unstable:0 slab_reclaimable:26060 slab_unreclaimable:48649 mapped:35873 shmem:35877 pagetables:8285 bounce:0 free:21419 free_pcp:0 free_cma:0[992416.148190] Node 0 active_anon:1913812kB inactive_anon:1020040kB active_file:644kB inactive_file:108kB unevictable:64kB isolated(anon):0kB isolated(file):0kB mapped:143492kB dirty:0kB writeback:0kB shmem:143508kB shmem_thp: 0kB shmem_pmdmapped: 0kB anon_thp: 0kB writeback_tmp:0kB unstable:0kB all_unreclaimable? no[992416.148191] Node 0 DMA free:14928kB min:284kB low:352kB high:420kB active_anon:748kB inactive_anon:104kB active_file:0kB inactive_file:0kB unevictable:0kB writepending:0kB present:15988kB managed:15904kB mlocked:0kB kernel_stack:0kB pagetables:0kB bounce:0kB free_pcp:0kB local_pcp:0kB free_cma:0kB[992416.148193] lowmem_reserve[]: 0 2772 3665 3665 3665[992416.148194] Node 0 DMA32 free:54416kB min:50896kB low:63620kB high:76344kB active_anon:1478944kB inactive_anon:856680kB active_file:20kB inactive_file:416kB unevictable:0kB writepending:0kB present:3129152kB managed:2867008kB mlocked:0kB kernel_stack:10032kB pagetables:26552kB bounce:0kB free_pcp:0kB local_pcp:0kB free_cma:0kB[992416.148196] lowmem_reserve[]: 0 0 893 893 893[992416.148196] Node 0 Normal free:16332kB min:16400kB low:20500kB high:24600kB active_anon:434120kB inactive_anon:163256kB active_file:476kB inactive_file:412kB unevictable:64kB writepending:0kB present:1048576kB managed:922756kB mlocked:64kB kernel_stack:7056kB pagetables:6588kB bounce:0kB free_pcp:0kB local_pcp:0kB free_cma:0kB[992416.148198] lowmem_reserve[]: 0 0 0 0 0[992416.148199] Node 0 DMA: 8*4kB (UM) 6*8kB (UM) 6*16kB (U) 7*32kB (UM) 1*64kB (U) 1*128kB (M) 0*256kB 0*512kB 2*1024kB (UM) 0*2048kB 3*4096kB (ME) = 14928kB[992416.148202] Node 0 DMA32: 2340*4kB (UME) 1077*8kB (UME) 606*16kB (UME) 419*32kB (UME) 180*64kB (UME) 18*128kB (UME) 1*256kB (M) 0*512kB 0*1024kB 0*2048kB 0*4096kB = 55160kB[992416.148205] Node 0 Normal: 1046*4kB (UME) 629*8kB (UME) 246*16kB (UME) 110*32kB (UME) 2*64kB (M) 0*128kB 0*256kB 0*512kB 0*1024kB 0*2048kB 0*4096kB = 16800kB[992416.148413] Node 0 hugepages_total=0 hugepages_free=0 hugepages_surp=0 hugepages_size=1048576kB[992416.148413] Node 0 hugepages_total=0 hugepages_free=0 hugepages_surp=0 hugepages_size=2048kB[992416.148414] 43441 total pagecache pages[992416.148415] 7332 pages in swap cache[992416.148415] Swap cache stats: add 540973, delete 533621, find 171870/177923[992416.148416] Free swap = 0kB[992416.148416] Total swap = 2097148kB[992416.148416] 1048429 pages RAM[992416.148417] 0 pages HighMem/MovableOnly[992416.148417] 97012 pages reserved[992416.148417] 0 pages cma reserved[992416.148417] 0 pages hwpoisoned[992416.148418] Tasks state (memory values in pages):[992416.148418] [ pid ] uid tgid total_vm rss pgtables_bytes swapents oom_score_adj name[992416.148431] [ 348] 0 348 13050 18 122880 480 -250 systemd-journal[992416.148448] [ 381] 0 381 5900 117 65536 707 -1000 systemd-udevd#省略了很多进程内存信息的打印..............[992416.148679] [ 140627] 1000 140627 5180 168 73728 68 0 top[992416.148680] [ 140709] 1000 140709 4177 0 61440 22 0 sleep[992416.148681] [ 140711] 1000 140711 687213 440843 5550080 243091 0 a.out[992416.148682] oom-kill:constraint=CONSTRAINT_NONE,nodemask=(null),cpuset=/,mems_allowed=0,global_oom,task_memcg=/user.slice/user-1000.slice/session-4.scope,task=a.out,pid=140711,uid=1000[992416.148857] Out of memory: Killed process 140711 (a.out) total-vm:2748852kB, anon-rss:1763368kB, file-rss:4kB, shmem-rss:0kB, UID:1000 pgtables:5420kB oom_score_adj:0[992416.268209] oom_reaper: reaped process 140711 (a.out), now anon-rss:0kB, file-rss:0kB, shmem-rss:0kB 在OOM的时候，kernel将内核栈都print出来了，原因是 __alloc_pages_slowpath 之后没有成功，然后就 out_of_memory 启动了大杀器 OOMkill process. Oom Killer 杀进程策略在这个例子中，看到最后杀死的进程是 a.out 1[992416.148857] Out of memory: Killed process 140711 (a.out) 为什么是a.out被杀呢？可以看一下 out_of_memory 代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344unsigned long oom_badness(struct task_struct *p, unsigned long totalpages){ long points; long adj; if (oom_unkillable_task(p)) // init 进程 和内核线程不能被选中 return 0; adj = (long)p-&gt;signal-&gt;oom_score_adj; /* * The baseline for the badness score is the proportion of RAM that each * task's rss, pagetable and swap space use. */ points = get_mm_rss(p-&gt;mm) + get_mm_counter(p-&gt;mm, MM_SWAPENTS) + mm_pgtables_bytes(p-&gt;mm) / PAGE_SIZE; // 考虑进程的 FILE ANON SHMEM SWAP 分区占用 和 页表PTE 占用的内存}static int oom_evaluate_task(struct task_struct *task, void *arg){ points = oom_badness(task, oc-&gt;totalpages);}static void select_bad_process(struct oom_control *oc){ struct task_struct *p; rcu_read_lock(); for_each_process(p) if (oom_evaluate_task(p, oc)) break; rcu_read_unlock();}bool out_of_memory(struct oom_control *oc){ check_panic_on_oom(oc); select_bad_process(oc); if (oc-&gt;chosen &amp;&amp; oc-&gt;chosen != (void *)-1UL) oom_kill_process(oc, !is_memcg_oom(oc) ? &quot;Out of memory&quot; : &quot;Memory cgroup out of memory&quot;); return !!oc-&gt;chosen;} select_bad_process 会选择一个最坏的一个 process，这个坏怎么定义呢，可以看到 oom_badness 主要是针对用户进程的 FILE ANON SHMEM SWAP 分区占用 和 页表PTE 占用的内存 做了一个评估 和 事先设置的 p-&gt;signal-&gt;oom_score_adj 值所以一般不建议 给一般进程设置 很小的 p-&gt;signal-&gt;oom_score_adj值，这样会导致 进程在内存泄露时 无法被OOM选中，从而误杀其他进程。 一般触发OOM有两种： 121. pagefault_out_of_memory --&gt; out_of_memory2. __alloc_pages_slowpath --&gt; __alloc_pages_may_oom --&gt; out_of_memory 我们刚刚遇到的这次就是 读入page cache 需要分配内存，内存不够导致的一次OOM 123456789101112131415[992416.146062] filemap_fault+0x69a/0xa40[992416.146143] ? unlock_page_memcg+0x12/0x20[992416.146166] ? page_add_file_rmap+0xff/0x1a0[992416.146181] ? xas_load+0xd/0x80[992416.146182] ? xas_find+0x17f/0x1c0[992416.146184] ? filemap_map_pages+0x24c/0x380[992416.146458] ext4_filemap_fault+0x32/0x46[992416.146475] __do_fault+0x3c/0x130[992416.146477] __handle_mm_fault+0xff5/0x1700[992416.146723] ? hrtimer_init_sleeper+0x90/0x90[992416.146726] handle_mm_fault+0xca/0x200[992416.147382] do_user_addr_fault+0x1f9/0x450[992416.147385] __do_page_fault+0x58/0x90[992416.147386] do_page_fault+0x2c/0xe0[992416.147387] page_fault+0x34/0x40 关于OOM 还有很多需要注意的，就不在这里细说了","link":"/2020/09/05/memory/%E9%9D%A2%E8%AF%95/%E5%86%99%E4%B8%80%E4%B8%AA%E6%B6%88%E8%80%97%E5%AE%8C%E7%89%A9%E7%90%86%E5%86%85%E5%AD%98%E7%9A%84%E7%A8%8B%E5%BA%8F/%E6%B6%88%E8%80%97%E7%89%A9%E7%90%86%E5%86%85%E5%AD%98/"},{"title":"SystemTap examples","text":"SystemTap脚本集锦本章列举了若干可用于监控和调查内核子系统的SystemTap脚本。所有这些示例都能在 centos的 /usr/share/systemtap/testsuite/systemtap.examples/下找到。 SystemTap脚本集锦SystemTap脚本集锦 解读错误信息解读错误信息 参考SystemTap 内核文档 参考 RedHat systemTap 文档 参考 文档","link":"/2021/01/07/%E5%86%85%E6%A0%B8%E8%A7%82%E6%B5%8B/systemTap/examples/SystemTap%20examples/"},{"title":"How to Use system Tap","text":"SystemTap允许使用者监控Linux系统当前的运行情况，以便进一步分析。这将有助于运维或开发人员缉查bug或性能问题的罪魁祸首。SystemTap提供了一门领域特定语言，使得用户可以编写自定义脚本，调查和监控各种内核函数、系统调用，和其它发生在内核空间的事件。 就此而言，SystemTap不仅仅是个工具，它是一个让你能够自定义内核取证和监控工具的生态系统。 当前版本的SystemTap提供的探测内核空间事件的众多选项，可以在不同版本的内核下使用。然而，SystemTap对探测用户空间事件的支持依赖于内核的支持（需要uprobe机制），而多数内核缺乏这一支持。结果是，仅有部分内核上的SystemTap版本支持用户空间探测。 安装SystemTap安装 systemtap 软件 12Inspiron-5548@ubuntu: /var/crash# sudo apt install systemtapInspiron-5548@ubuntu: /var/crash# sudo apt install systemtap-runtime 安装 kernel debug info 12Inspiron-5548@ubuntu: /var/crash# sudo apt install systemtapInspiron-5548@ubuntu: /var/crash# sudo apt install systemtap-runtime SystemTap 可以用来干什么SystemTap允许用户仅需编写和重用简单的脚本即可获取Linux繁多的运行数据。通过SystemTap脚本，你可以又好又快地提取数据、过滤数据、汇总数据。诊断复杂的性能问题（或功能问题）再也不是难事。整个SystemTap脚本所做的，无非就是声明感兴趣的事件，然后添加对应的处理程序。当SystemTap脚本运行时，SystemTap会监控声明的事件；一旦事件发生，Linux内核会临时切换到对应的处理程序，完成后再重拾原先的工作。 12可供监控的事件种类繁多：进入/退出某个函数，定时器到期，会话终止，等等。处理程序由一组SystemTap语句构成，指明事件发生后要做的工作。其中包括从事件上下文中提取数据，存储到内部变量中，输出结果。 SystemTap 使用最简单的一行代码123456789Inspiron-5548@127ubuntu: ~/workspace# echo &quot;probe timer.s(1) {exit()}&quot; | sudo stap -v -Pass 1: parsed user script and 476 library scripts using 108312virt/90968res/7440shr/83392data kb, in 260usr/30sys/315real ms.Pass 2: analyzed script: 1 probe, 1 function, 0 embeds, 0 globals using 109896virt/92776res/7688shr/84976data kb, in 10usr/0sys/9real ms.Pass 3: translated to C into &quot;/tmp/stappbkhvF/stap_629b1ee8abda600005ad17f270124c66_947_src.c&quot; using 110032virt/92776res/7688shr/85112data kb, in 0usr/0sys/1real ms.Pass 4: compiled C into &quot;stap_629b1ee8abda600005ad17f270124c66_947.ko&quot; in 15530usr/2160sys/17963real ms.Pass 5: starting run.Pass 5: run completed in 20usr/30sys/1459real ms.Inspiron-5548@ubuntu: ~/workspace# 可以看出 SystemTap 脚本运行需要结果5个步骤，在加载 SystemTap脚本过程(生成ko)的时候，SystemTap 耗时较多，尤其是CPU资源。 SystemTap脚本运行时，会启动一个对应的SystemTap会话。整个会话大致流程如下： 首先，SystemTap会检查脚本中用到的tapset，确保它们都存在于tapset库中（通常是/usr/share/systemtap/tapset/）。然后SystemTap会把找到的tapset替换成在tapset库中对应的定义。tapset是tap（听诊器）的集合，指一些预定义的SystemTap事件或函数。完整的tapset列表 SystemTap接着会把脚本转化成C代码，运行系统的C编译器编译出一个内核模块。完成这一步的工具包含在systemtap包中SystemTap随即加载该模块，并启用脚本中所有的探针（包括事件和对应的处理程序）。这一步由system-runtime包的staprun完成。每当被监控的事件发生，对应的处理程序就会被执行。一旦SystemTap会话终止，探针会被禁用，内核模块也会被卸载。这一串流程皆始于一个简单的命令行程序：stap。这个程序包揽了SystemTap主要的功能。 脚本如何编写在大多数情况下，SystemTap脚本是每个SystemTap会话的基石。SystemTap脚本决定了需要收集的信息类型，也决定了对收集到的信息的处理方式。SystemTap脚本由两部分组成：事件和处理程序。一旦SystemTap会话准备就绪，SystemTap会监控操作系统中特定的事件，并在事件发生的时候触发对应的处理程序。 1一个事件和它对应的处理程序合称探针。一个SystemTap脚本可以有多个探针。 一个探针的处理程序部分通常称之为探针主体（probe body） 与应用开发的方式类比，使用事件和处理程序就像在程序的特定位置插入打日志的语句。每当程序运行时，这些日志会帮助你查看程序执行的流程。但是SystemTasp脚本允许你在无需重新编译代码，即可插入检测指令，而且处理程序也不限于单纯地打印数据。事件会触发对应的处理程序；对应的处理程序记录下感兴趣的数据，并以你指定的格式输出。 SystemTap脚本的后缀是.stp，并以这样的语句表示一个探针： 1probe event {statements} SystemTap支持给一个探针指定多个事件；每个事件以逗号隔开。如果给某一个探针指定了多个事件，只要其中一个事件发生，SystemTap就会执行对应的处理程序。每个探针有自己对应的语句块。语句块由花括号 {} 括住，包含事件发生时需要执行的所有语句。SystemTap会顺序执行这些语句；语句间通常不需要特殊的分隔符或终止符。 SystemTap还允许你编写函数来提取探针间公共的逻辑。所以，与其在多个探针间复制粘贴重复的语句，你不如把它们放入函数中，就像函数调用一样： 123function function_name(arguments) {statements}probe event {function_name(arguments)} 当探针被触发时，function_name中的语句会被执行。arguments是传递给函数的可选的入参。 SystemTap 事件我们还需要了解SystemTap 事件，这里主要分为 同步事件 和 异步事件。 同步事件同步事件会在任意进程执行到内核特定位置时触发。你可以用它来作为其它事件的参照点，毕竟同步事件有着清晰的上下文信息。包括： syscall.system_call 进入名为system_call的系统调用。如果想要监控的是退出某个系统调用的事件，在后面添加.return。举个例子，要想监控进入和退出系统调用close的事件，应该使用syscall.close和syscall.close.return。 vfs.file_operation 进入虚拟文件系统（VFS）名为file_operation的文件操作。跟系统调用事件一样，在后面添加.return可以监控对应的退出事件。 译注：file_operation取值的范畴，取决于当前内核中struct file_operations的定义的操作. kernel.function(&quot;function&quot;) 进入名为function的内核函数。举个例子，kernel.function(“sys_open”)即内核函数sys_open被调用时所触发的事件。同样，kernel.function(“sys_open”).return会在sys_open函数调用返回时被触发。 在定义探测事件时，可以使用像*这样的通配符。你也可以用内核源码文件名限定要跟踪的函数。看下面的例子： 12probe kernel.function(&quot;*@net/socket.c&quot;) { }probe kernel.function(&quot;*@net/socket.c&quot;).return { } kernel.trace(&quot;tracepoint&quot;)到达名为tracepoint的静态内核探测点（tracepoint）。较新的内核（&gt;= 2.6.30）包含了特定事件的检测代码。这些事件一般会被标记成静态内核探测点。一个例子是，kernel.trace(“kfree_skb”)表示内核释放了一个网络缓冲区的事件。（译注：想知道当前内核设置了哪些静态内核探测点吗？你需要运行sudo perf list。） module(&quot;module&quot;).function(&quot;function&quot;) 进入指定模块module的函数function。举个例子： 12probe module(&quot;ext3&quot;).function(&quot;*&quot;) { }probe module(&quot;ext3&quot;).function(&quot;*&quot;).return { } 异步事件异步事件跟特定的指令或代码的位置无关。 这部分事件主要包含计数器、定时器和其它类似的东西。 begin SystemTap会话的启动事件，会在脚本开始时触发。 end SystemTap会话的结束事件，会在脚本结束时触发。 timer events 用于周期性执行某段处理程序。举个例子： 1probe timer.s(4) { printf(&quot;hello world\\n&quot;) } 上面的例子中，每隔4秒就会输出hello world。还可以使用其它规格的定时器： 12345timer.ms(milliseconds)timer.us(microseconds)timer.ns(nanoseconds)timer.hz(hertz)timer.jiffies(jiffies) 定时事件总是跟其它事件搭配使用。其它事件负责收集信息，而定时事件定期输出当前状况，让你看到数据随时间的变化情况。 SystemTap 处理程序有了事件之后，我们还需要在事件发生之后进行处理。 example1: 12345probe begin{ printf (&quot;hello world\\n&quot;) exit ()} SystemTap脚本会一直运行，直到执行了exit()函数。如果你想中途退出一个脚本，可以用Ctrl+c中断。这是一个异步事件 begin 之后开始打印 一个字符串. printf 是一个标准输出函数 1printf (&quot;format string\\n&quot;, arguments) example2: 123456probe syscall.open{ printf (&quot;%s(%d) open\\n&quot;, execname(), pid())}echo ' probe syscall.open { printf (&quot;%s(%d) open\\n&quot;, execname(), pid()) }' | sudo stap -v - example2中 SystemTap会在每次open被调用时，输出调用程序的名字和PID。 该探针输出的结果看上去会是这样(not in zsh, in bash)： 123456789101112ubuntu@ubuntu-Inspiron-5548:~/workspace$ echo ' probe syscall.open { printf (&quot;%s(%d) open\\n&quot;, execname(), pid()) }' | sudo stap -v -Pass 1: parsed user script and 476 library scripts using 108308virt/90696res/7172shr/83388data kb, in 290usr/40sys/364real ms.Pass 2: analyzed script: 4 probes, 5 functions, 97 embeds, 4 globals using 110292virt/93060res/7724shr/85372data kb, in 230usr/330sys/575real ms.Pass 3: using cached /root/.systemtap/cache/4c/stap_4cd2c557b5457e5f955c640275432033_64778.cPass 4: using cached /root/.systemtap/cache/4c/stap_4cd2c557b5457e5f955c640275432033_64778.koPass 5: starting run.rg(25671) openrg(25671) openrg(25671) openrg(25671) openrg(25671) open 下面是常用的 SystemTap 内建函数： tid() 当前的tid（thread id）。 uid() 当前的uid。 cpu() 当前的CPU号 gettimeofday_s() 自epoch以来的秒数 ctime() 将上一个函数返回的秒数转化成时间字符串 pp() 返回描述当前处理的探测点的字符串 thread_indent() name 返回系统调用的名字。这个变量只能在syscall.system_call触发的处理程序中使用。 target() 当你通过stap script -x PID或stap script -c command来执行某个脚本script时，target()会返回你指定的PID或命令名。举个例子： 123456probe syscall.* { if (pid() == target()) printf(&quot;%s\\n&quot;, name)}echo ' probe syscall.* { if (pid() == target()) printf(&quot;%s\\n&quot;, name) }' | sudo stap -v - 这个 SystemTap 脚本使用了通配符 probe了所以系统调用，在对脚本解析，编译成为 kernel module ko的时候尤其耗时，在我 I5 5200U的机器上居然准备工作做了进1min… 12345678910111213141516top - 12:38:07 up 12:53, 2 users, load average: 2.56, 1.77, 1.14任务: 277 total, 5 running, 271 sleeping, 0 stopped, 1 zombie%Cpu(s): 10.0 us, 17.6 sy, 0.0 ni, 66.9 id, 0.0 wa, 0.0 hi, 5.5 si, 0.0 stMiB Mem : 3658.2 total, 196.4 free, 2043.1 used, 1418.8 buff/cacheMiB Swap: 2048.0 total, 1572.1 free, 475.9 used. 986.8 avail Mem 进程号 USER PR NI VIRT RES SHR %CPU %MEM TIME+ COMMAND 54775 root 20 0 171592 154320 18248 R 99.7 4.1 1:05.72 stap 932 ubuntu 20 0 1750472 40364 14900 S 3.7 1.1 11:16.15 Xorg 1279 ubuntu 20 0 4777288 218332 57988 S 3.0 5.8 12:57.76 gnome-shell 2458 ubuntu 20 0 4633228 103048 65556 R 3.0 2.8 0:10.23 chrome 1747 ubuntu 20 0 987308 43280 31604 S 1.7 1.2 0:46.33 gnome-terminal- 2371 ubuntu 20 0 1342876 232676 99800 S 0.7 6.2 8:23.72 chrome 18 root 20 0 0 0 0 S 0.3 0.0 0:01.14 ksoftirqd/1 30 root 20 0 0 0 0 S 0.3 0.0 0:00.87 ksoftirqd/3Inspiron-5548@ubuntu: ~/workspace/linux-stable/drivers# 输出是 1234567891011121314151617181920212223242526272829303132333435363738394041ubuntu@ubuntu-Inspiron-5548:~/workspace$ echo ' probe syscall.* { if (pid() == target()) printf(&quot;%s\\n&quot;, name) }' | sudo stap -v -x 2371 -Pass 1: parsed user script and 476 library scripts using 108312virt/90872res/7344shr/83392data kb, in 510usr/40sys/559real ms.qWARNING: cross-file global variable reference to identifier 'syscall_string_trunc' at /usr/share/systemtap/tapset/linux/syscalls_cfg_trunc.stp:3:8 from: identifier 'syscall_string_trunc' at /usr/share/systemtap/tapset/linux/sysc_add_key.stp:19:59 source: user_buffer_quoted(payload_uaddr, plen, syscall_string_trunc), ^ in expansion of macro: operator '@_SYSCALL_ADD_KEY_ARGSTR' at /usr/share/systemtap/tapset/linux/sysc_add_key.stp:72:2 source: @_SYSCALL_ADD_KEY_ARGSTR ^WARNING: cross-file global variable reference to identifier 'syscall_string_trunc' at /usr/share/systemtap/tapset/linux/syscalls_cfg_trunc.stp:3:8 from: identifier 'syscall_string_trunc' at /usr/share/systemtap/tapset/linux/sysc_mount.stp:31:46 source: data = user_string_n_quoted(pointer_arg(5), syscall_string_trunc) ^WARNING: cross-file global variable reference to identifier 'syscall_string_trunc' at /usr/share/systemtap/tapset/linux/syscalls_cfg_trunc.stp:3:8 from: identifier 'syscall_string_trunc' at :23:49 source: buf_str = user_buffer_quoted(buf_uaddr, count, syscall_string_trunc) ^ in expansion of macro: operator '@_SYSCALL_WRITE_REGARGS' at /usr/share/systemtap/tapset/linux/sysc_write.stp:100:2 source: @_SYSCALL_WRITE_REGARGS ^Pass 2: analyzed script: 853 probes, 29 functions, 100 embeds, 5 globals using 129252virt/113636res/9172shr/104332data kb, in 35560usr/79740sys/116322real ms.Pass 3: using cached /root/.systemtap/cache/e2/stap_e2b35608bd8c0499c68f451dc8b09a85_432536.cPass 4: using cached /root/.systemtap/cache/e2/stap_e2b35608bd8c0499c68f451dc8b09a85_432536.koPass 5: starting run.recvmsgwritewriteepoll_waitepoll_waitepoll_waitrecvmsgreadsendtofutexrecvmsgrecvmsgrecvmsgpollrecvmsgrecvmsg SystemTap 处理程序的基本结构SystemTap 在处理程序中 它们的语法基本上类似于C或awk。 变量处理程序里面当然可以使用变量，你所需的不过是给它取个好名字，把函数或表达式的值赋给它，然后就可以使用它了。SystemTap可以自动判定变量的类型。举个例子，如果你用gettimeofday_s()给变量foo赋值，那么foo就是数值类型的，可以在printf()中通过%d输出。 变量默认只能在其所定义的探针内可用。这意味着变量的生命周期仅仅是处理程序的某次运行。不过你也可以在探针外定义变量，并使用global修饰它们，这样就能在探针间共享变量了。 ⁠ example1: 12345678910global count_jiffies, count_msprobe timer.jiffies(100) { count_jiffies ++ }probe timer.ms(100) { count_ms ++ }probe timer.ms(12345){ hz=(1000*count_jiffies) / count_ms printf (&quot;jiffies:ms ratio %d:%d =&gt; CONFIG_HZ=%d\\n&quot;, count_jiffies, count_ms, hz) exit ()} example1 中 timer-jiffies.stp 通过累加jiffies和milliseconds，来求出内核的CONFIG_HZ配置。global语句使得count_jiffies和count_ms在每个探针中可用。 目标变量(Target Variables)跟内核代码相关的事件，如kernel.function(“function”)和kernel.statement(“statement”)，允许使用目标变量获取这部分代码中可访问到的变量的值。你可以使用-L选项来列出特定探测点下可用的目标变量。如果已经安装了内核调试信息，你可以通过这个命令获取vfs_read中可用的目标变量 由于 我当前笔记本环境问题，暂时没法使用这个，会报错，后续补充 123456789101112131415ubuntu@ubuntu-Inspiron-5548:/etc/apt$ sudo stap -L 'kernel.function(&quot;vfs_read&quot;)'Tip: /usr/share/doc/systemtap/README.Debian should help you get started.ubuntu@ubuntu-Inspiron-5548:/etc/apt$ sudo stap -e 'probe kernel.function(&quot;vfs_read&quot;) {&gt; printf (&quot;current files_stat max_files: %d\\n&quot;,&gt; @var(&quot;[email protected]/file_table.c&quot;)-&gt;max_files);&gt; exit(); }'semantic error: while resolving probe point: identifier 'kernel' at &lt;input&gt;:1:7 source: probe kernel.function(&quot;vfs_read&quot;) { ^semantic error: missing x86_64 kernel/module debuginfo [man warning::debuginfo] under '/lib/modules/5.4.0-58-generic/build'Pass 2: analysis failed. [man error::pass2]Tip: /usr/share/doc/systemtap/README.Debian should help you get started.ubuntu@ubuntu-Inspiron-5548:/etc/apt$ 123456789Inspiron-5548@ubuntu: ~/workspace/linux-stable/drivers# stap -l 'syscall.*'syscall.acceptsyscall.accept4syscall.accessosyscall.acctsyscall.add_keysyscall.adjtimexsyscall.alarmsyscall.arch_prctl 整齐打印目标变量（Pretty Printing Target Variables）某些场景中，我们可能需要输出当前可访问的各种变量，以便于记录底层的变化。SystemTap提供了一些操作，可以生成描述特定目标变量的字符串： $$vars 输出作用域内每个变量的值。等价于sprintf(“parm1=%x … parmN=%x var1=%x … varN=%x”, parm1, …, parmN, var1, …, varN)。如果变量的值在运行时找不到，输出=?。 $$locals 同$$vars，只输出本地变量。 $$parms 同$$vars，只输出函数入参。 $$return 仅在带return的探针中可用。如果被监控的函数有返回值，它等价于sprintf(“return=%x”, $return)，否则为空字符串。 条件语句有些时候，你写的SystemTap脚本较为复杂，可能需要用上条件语句。SystemTap支持C风格的条件语句，另外还支持foreach (VAR in ARRAY) {}形式的遍历。 命令行参数通过$或@加个数字的形式可以访问对应位置的命令行参数。用$会把用户输入当作整数，用@会把用户输入当作字符串。 probe kernel.function(@1) { }probe kernel.function(@1).return { }上面的脚本期望用户把要监控的函数作为命令行参数传递进来。你可以让脚本接受多个命令行参数，分别命名为@1，@2等等，按用户输入的次序逐个对应。 Tapsetstapsets是一些包含常用的探针和函数的内置脚本，你可以在SystemTap脚本中复用它们。 当用户运行一个SystemTap脚本时，SystemTap会检测脚本中的事件和处理程序，并在翻译脚本成C代码之前，加载用到的tapset。就像SystemTap脚本一样，tapset的拓展名也是.stp。默认情况下tapset位于/usr/share/systemtap/tapset/。 跟SystemTap脚本不同的是，tapset不能被直接运行；它只能作为库使用。tapset库让用户能够在更高的抽象层次上定义事件和函数。tapset提供了一些常用的内核函数的别名，这样用户就不需要记住完整的内核函数名了（尤其是有些函数名可能会因内核版本的不同而不同）。另外tapset也提供了常用的辅助函数，比如之前我们见过的thread_indent()。 总结SystemTap 使用过程中发现 他解析编译极其慢，很耗费 CPU资源，最好做到一次解析编译模块，可以到处部署，实际SystemTap 也已经支持了这样的做法。 参考SystemTap 内核文档 参考 RedHat systemTap 文档 参考 文档","link":"/2021/01/09/%E5%86%85%E6%A0%B8%E8%A7%82%E6%B5%8B/systemTap/How%20to%20Use%20system%20Tap/"},{"title":"定位slub内存泄露的几种方法","text":"总结一下最近最近遇到的内存泄露的一个bug，其实内核态的内存泄露还是比较难搞的，严重情况下基本只能重启设备解决，对于互联网公司来说大规模重启服务器来说代价有点大，有了一些工具我们还是比较容易观测到的，我用写了一个内核模块来模拟内核态的内存泄露，代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667#include &lt;linux/init.h&gt;#include &lt;linux/module.h&gt;#include &lt;linux/fs.h&gt;#include &lt;linux/version.h&gt;#include &lt;linux/kthread.h&gt;#include &lt;linux/delay.h&gt;#include &lt;linux/slab.h&gt;#define LIULANGRENAAA_CACHE_SIZE (5678)static struct kmem_cache *liulangrenaaa_cache;struct task_struct *task = NULL;static int memleak_task_test(void *arg){ int m = 1000 * 10; int n = 1000 * 20; int i = 0; gfp_t gfp = GFP_KERNEL; while(!kthread_should_stop()){ i++; kmem_cache_alloc(liulangrenaaa_cache, gfp); if (i &lt; m) { msleep_interruptible(50); } else if ((i &gt; m) &amp;&amp; (i &lt; n)) { msleep_interruptible(500); } else if (i &gt; n) { msleep_interruptible(5000); } if (i % 50 == 1) { printk(KERN_ERR &quot;i = %d\\n&quot;, i); } } return 0;}static int memleak_init(void){ liulangrenaaa_cache = kmem_cache_create(&quot;liulangrenaaa_cache&quot;, LIULANGRENAAA_CACHE_SIZE, LIULANGRENAAA_CACHE_SIZE, SLAB_PANIC, NULL); if (liulangrenaaa_cache == NULL) { return 1; } task = kthread_run(memleak_task_test, NULL, &quot;memleak_task_test&quot;); if (IS_ERR(task)) { printk(KERN_ERR &quot;Ecard: unable to memleak_task_test kernel thread: %ld\\n&quot;, PTR_ERR(task)); return PTR_ERR(task); } return 0;}static void memleak_exit(void){ if (NULL != task) { kthread_stop(task); } kmem_cache_destroy(liulangrenaaa_cache);}module_init(memleak_init);module_exit(memleak_exit);MODULE_LICENSE(&quot;GPL&quot;);MODULE_AUTHOR(&quot;liulangrenaaa&quot;); 通过下面命令之后，安装内核模块 12makesudo insmod memleak.ko 过一段时间之后，就会发现内核内存不足，其中 /proc/meminfo 的 SUnreclaim 较大，且存在持续增加现象，这说明slab内存存在泄漏现象，属于典型的slab内存泄露 123sh@ubuntu[root]:/sys/kernel/slab# cat /proc/meminfoSReclaimable: 256132 kBSUnreclaim: 897648 kB 123sh@ubuntu[root]:/sys/kernel/slab# cat /proc/meminfoSReclaimable: 256132 kBSUnreclaim: 997652 kB 定位到slab内存泄露之后还不够，还需要了解到底是哪种slab内存泄露，linux内核提供了相关工具 slabtop slabinfo，可以观察内核中slab使用情况 123456789101112131415161718192021222324252627sh@ubuntu[root]:/sys/kernel/slab# slabtop Active / Total Objects (% used) : 1158388 / 1252082 (92.5%) Active / Total Slabs (% used) : 27004 / 27004 (100.0%) Active / Total Caches (% used) : 103 / 147 (70.1%) Active / Total Size (% used) : 409641.83K / 438329.62K (93.5%) Minimum / Average / Maximum Object : 0.01K / 0.35K / 10.08K OBJS ACTIVE USE OBJ SIZE SLABS OBJ/SLAB CACHE SIZE NAME204498 172768 84% 0.19K 4869 42 38952K dentry134580 129007 95% 0.13K 2243 60 17944K kernfs_node_cache122061 112693 92% 1.07K 4209 29 134688K ext4_inode_cache 75348 61356 81% 0.10K 1932 39 7728K buffer_head 61952 61952 100% 0.50K 968 64 30976K kmalloc-512 61659 60140 97% 0.20K 1581 39 12648K vm_area_struct 54144 53470 98% 0.03K 423 128 1692K kmalloc-32 53184 51912 97% 0.06K 831 64 3324K anon_vma_chain 45888 42616 92% 0.25K 717 64 11472K filp 43232 35798 82% 0.57K 772 56 24704K radix_tree_node 38743 35893 92% 0.59K 731 53 23392K inode_cache 34960 34556 98% 0.09K 760 46 3040K anon_vma 31872 26373 82% 0.06K 498 64 1992K kmalloc-64 21248 21248 100% 0.02K 83 256 332K kmalloc-16 19968 19968 100% 0.01K 39 512 156K kmalloc-8 16320 12472 76% 0.04K 160 102 640K ext4_extent_status 13200 12367 93% 0.66K 275 48 8800K proc_inode_cache 13018 12877 98% 0.69K 283 46 9056K squashfs_inode_cache 12070 12070 100% 0.05K 142 85 568K ftrace_event_field 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657sh@ubuntu[root]:/sys/kernel/slab# slabinfoName Objects Objsize Space Slabs/Part/Cpu O/S O %Fr %Ef Flg........dax_cache 42 752 32.7K 0/0/1 42 3 0 96 Aadentry 25537 192 6.1M 674/195/74 42 1 26 80 adma-kmalloc-512 64 512 32.7K 0/0/1 64 3 0 100 ddmaengine-unmap-128 30 1056 32.7K 0/0/1 30 3 0 96 Admaengine-unmap-256 15 2080 32.7K 0/0/1 15 3 0 95 Aext4_extent_status 12472 40 655.3K 151/61/9 102 0 38 76 aext4_groupinfo_4k 8008 144 1.1M 142/0/1 56 1 0 98 aext4_inode_cache 1131 1088 1.2M 32/0/7 29 3 0 96 aext4_system_zone 612 40 24.5K 2/0/4 102 0 0 99fat_inode_cache 44 736 32.7K 0/0/1 44 3 0 98 afile_lock_cache 148 216 32.7K 0/0/4 37 1 0 97files_cache 276 704 196.6K 2/0/4 46 3 0 98 Afilp 2213 256 720.8K 10/10/34 64 2 22 78 Afuse_inode 78 784 65.5K 0/0/2 39 3 0 93 Aafuse_request 112 144 16.3K 0/0/2 56 1 0 98hugetlbfs_inode_cache 102 624 65.5K 0/0/2 51 3 0 97inode_cache 22871 600 15.4M 442/47/30 53 3 9 88 ajbd2_journal_head 2244 112 303.1K 4/4/33 68 1 10 82 ajbd2_revoke_table_s 256 16 4.0K 0/0/1 256 0 0 100 akernfs_node_cache 129007 128 18.3M 2226/283/17 60 1 12 89kmalloc-128 1536 128 196.6K 10/0/14 64 1 0 100kmalloc-16 16128 16 258.0K 39/0/24 256 0 0 100kmalloc-192 2184 192 425.9K 10/0/42 42 1 0 98kmalloc-1k 2498 1024 2.7M 67/30/16 32 3 36 94kmalloc-256 4224 256 1.0M 39/0/27 64 2 0 100kmalloc-2k 2023 2048 4.2M 114/7/15 16 3 5 98kmalloc-32 41694 32 1.3M 296/35/35 128 0 10 98kmalloc-4k 1738 4096 7.4M 216/30/11 8 3 13 95kmalloc-512 54016 512 27.6M 823/0/21 64 3 0 100kmalloc-64 14637 64 1.2M 248/176/58 64 0 57 74kmalloc-8 12800 8 102.4K 12/0/13 512 0 0 100kmalloc-8k 398 8192 3.4M 97/6/7 4 3 5 95kmalloc-96 3738 96 364.5K 35/0/54 42 0 0 98kmalloc-rcl-128 192 128 24.5K 0/0/3 64 1 0 100 akmalloc-rcl-64 384 64 24.5K 0/0/6 64 0 0 100 akmalloc-rcl-96 168 96 16.3K 0/0/4 42 0 0 98 akmem_cache 2149 408 1.0M 43/15/19 36 2 24 86 Akmem_cache_node 2496 64 159.7K 9/0/30 64 0 0 100 Aliulangrenaaa_cache 26080 5678 228.5M 870/1/0 3 3 0 51 *mm_struct 120 1048 131.0K 0/0/4 30 3 0 95 Amqueue_inode_cache 34 920 32.7K 0/0/1 34 3 0 95 Anames_cache 72 4096 294.9K 0/0/9 8 3 0 100 Anet_namespace 18 4928 98.3K 0/0/3 6 3 0 90numa_policy 186 264 49.1K 1/0/2 62 2 0 99pde_opener 408 40 16.3K 0/0/4 102 0 0 99proc_dir_entry 1176 192 229.3K 20/0/8 42 1 0 98proc_inode_cache 1789 672 1.3M 5/5/37 48 3 11 87 aradix_tree_node 6181 576 6.5M 177/149/22 56 3 74 54 aRAW 641 976 819.2K 13/5/12 32 3 20 76 ARAWv6 494 1176 622.5K 5/0/14 26 3 0 93 Arequest_queue 60 2104 131.0K 0/0/4 15 3 0 96request_sock_TCP 212 296 65.5K 0/0/4 53 2 0 95scsi_sense_cache 1536 96 196.6K 21/0/3 64 1 0 75 A........ 在 slabinfo 的输出中明显看到 liulangrenaaa 的slab使用有异常，size为 5678，使用的个数高达26080，总内存占用是 228.5M，看来我们是找到了内存泄露的真凶了。 找到 liulangrenaaa 这个slab内存泄露之后，如何从代码上找到内存泄露的源头呢。我的一般做法是 直接在代码中搜索 liulangrenaaa 看slab的创建，根据创建的名字再去搜索 alloc 和 free的代码，对于我抽象的这个例子来说直接就可以看出端倪，有时候在复杂的代码逻辑想找出分配但没有释放的点是比较困难的 根据 liulangrenaaa slab 的 size，直接使用 bpftrace 来跟踪 kmem_cache_alloc 和 kmem_cache_free 函数且参数 size和我们相符的点，将comm 和 pid打印出来，基本是一把就能确定是谁在泄露这个slab，需要依赖bpftrace ftrace 来跟踪所有的 kmem_cache_alloc 和 kmem_cache_free点，然后将trace保存下来一一去对比，这个方法类似于方法二，但是工作量很大，因为ftrace不能过滤参数 使用kmemleak工具检查内存泄露，但是这种方法需要内核支持，如果不支持可能需要重新编译内核，大服务器上工作量有点大，嵌入式系统中还好 使用slub_debug 这个特性，但是也需要内核支持，同方法4 使用bpftrace来过滤首先要知道slab的分配和释放接口是 kmem_cache_alloc kmem_cache_free,然后去寻找 内核有没有提供 tracepoint点或者可以krpobe的点 1234567891011121314sh@ubuntu[root]:/sys/kernel/slab# bpftrace -l | grep trace |grep mem | grep alloctracepoint:kmem:kmalloctracepoint:kmem:kmem_cache_alloctracepoint:kmem:kmalloc_nodetracepoint:kmem:kmem_cache_alloc_nodetracepoint:kmem:mm_page_alloctracepoint:kmem:mm_page_alloc_zone_lockedtracepoint:kmem:mm_page_alloc_extfragsh@ubuntu[root]:/sys/kernel/slab# bpftrace -l | grep trace |grep mem | grep freetracepoint:kmem:kfreetracepoint:kmem:kmem_cache_freetracepoint:kmem:mm_page_freetracepoint:kmem:mm_page_free_batched 幸运的是发现内核居然都提供了这两个tracepoint点，然后需要去观察一下这两个tracepoint点可以给我们带来什么信息，直接到 /sys/kernel/debug/tracing/events/ 目录下看一下 1234567891011121314sh@ubuntu[root]:/sys/kernel/debug/tracing/events# cat kmem/kmem_cache_alloc/formatname: kmem_cache_allocID: 539format: field:unsigned short common_type; offset:0; size:2; signed:0; field:unsigned char common_flags; offset:2; size:1; signed:0; field:unsigned char common_preempt_count; offset:3; size:1; signed:0; field:int common_pid; offset:4; size:4; signed:1; field:unsigned long call_site; offset:8; size:8; signed:0; field:const void * ptr; offset:16; size:8; signed:0; field:size_t bytes_req; offset:24; size:8; signed:0; field:size_t bytes_alloc; offset:32; size:8; signed:0; field:gfp_t gfp_flags; offset:40; size:4; signed:0; 可以看到 bytes_req 就是申请的slab大小，下面我们使用bpftrace来跟踪这个tp点，加上条件限制 12345678910111213141516171819sh@ubuntu[root]:/sys/kernel/slab# bpftrace -e 'tracepoint:kmem:kmem_cache_alloc / args-&gt;bytes_req == 5678 / { printf(&quot;pid:%d, comm:%s, kstack:[%s]\\n&quot;, pid, comm, kstack);}'Attaching 1 probe...pid:40953, comm:memleak_task_te, kstack:[ kmem_cache_alloc+340 kmem_cache_alloc+340 memleak_task_test+112 kthread+260 ret_from_fork+53]pid:40953, comm:memleak_task_te, kstack:[ kmem_cache_alloc+340 kmem_cache_alloc+340 memleak_task_test+112 kthread+260 ret_from_fork+53]sh@ubuntu[root]:/sys/kernel/debug/tracing/events# ps -aux |grep 40953root 40953 0.0 0.0 0 0 ? S 14:17 0:00 [memleak_task_te]root 40958 0.0 0.0 17668 736 pts/4 S+ 14:18 0:00 grep --color=auto 40953 根据内核线程名 memleak_task_te 就可以直接在代码中搜索，再结合 kstack 基本就可以直接找到bug 使用ftrace来跟踪其实和上面bpftrace 使用的原理是一样的，但是bpftrace 对内核版本要求较高，很多centos服务器的版本都比较老，甚至还在2.6.32左右，这样bpftrace基本不能用，所以ftrace一直很热门同样也需要找到分配释放slab的接口，也需要知道有没有提供相关tracepoint点，上面bpftrace方法已经说了，不赘述。 12345678910111213141516171819sh@ubuntu[root]:/sys/kernel/debug/tracing# echo 1 &gt; events/kmem/kmem_cache_alloc/enablesh@ubuntu[root]:/sys/kernel/debug/tracing# cat trace# tracer: nop## entries-in-buffer/entries-written: 348/348 #P:4## _-----=&gt; irqs-off# / _----=&gt; need-resched# | / _---=&gt; hardirq/softirq# || / _--=&gt; preempt-depth# ||| / delay# TASK-PID CPU# |||| TIMESTAMP FUNCTION# | | | |||| | | &lt;idle&gt;-0 [003] ..s. 312444.111637: kmem_cache_alloc: call_site=__build_skb+0x24/0x60 ptr=0000000003218205 bytes_req=224 bytes_alloc=256 gfp_flags=GFP_ATOMIC &lt;...&gt;-39771 [000] .... 312444.141944: kmem_cache_alloc: call_site=getname_flags+0x4f/0x1f0 ptr=00000000873301d3 bytes_req=4096 bytes_alloc=4096 gfp_flags=GFP_KERNEL &lt;...&gt;-39771 [000] .... 312444.141956: kmem_cache_alloc: call_site=__alloc_file+0x28/0x110 ptr=000000003ce0135a bytes_req=256 bytes_alloc=256 gfp_flags=GFP_KERNEL|__GFP_ZERO &lt;...&gt;-39771 [000] .... 312444.141957: kmem_cache_alloc: call_site=security_file_alloc+0x29/0x90 ptr=00000000142c82a3 bytes_req=24 bytes_alloc=24 gfp_flags=GFP_KERNEL|__GFP_ZERO &lt;...&gt;-39771 [000] .... 312444.342794: kmem_cache_alloc: call_site=getname_flags+0x4f/0x1f0 ptr=00000000873301d3 bytes_req=4096 bytes_alloc=4096 gfp_flags=GFP_KERNEL &lt;...&gt;-39771 [000] .... 312444.342800: kmem_cache_alloc: call_site=__alloc_file+0x28/0x110 ptr=000000003ce0135a bytes_req=256 bytes_alloc=256 gfp_flags=GFP_KERNEL|__GFP_ZERO 这样的信息对我们来说实在是太多了，我们知道trace保存这些信息的其实是一个 ringbuffer，短时间这么多的信息肯定会一遍一遍的填充这个 ringbufer，往往其中也没几条我们想要的信息，这就轮到filter上场了，可以根据规则想 filter 中写入相应规则，过滤我们想要trace的事件 12345678910111213141516171819202122232425262728293031323334353637383940414243sh@ubuntu[root]:/sys/kernel/debug/tracing/events/kmem/kmem_cache_alloc# lsenable filter format hist id triggersh@ubuntu[root]:/sys/kernel/debug/tracing# echo &quot;bytes_req == 5678&quot; &gt; events/kmem/kmem_cache_alloc/filtersh@ubuntu[root]:/sys/kernel/debug/tracing# cat events/kmem/kmem_cache_alloc/filterbytes_req == 5678sh@ubuntu[root]:/sys/kernel/debug/tracing# echo 0 &gt; tracesh@ubuntu[root]:/sys/kernel/debug/tracing# cat trace# tracer: nop## entries-in-buffer/entries-written: 0/0 #P:4## _-----=&gt; irqs-off# / _----=&gt; need-resched# | / _---=&gt; hardirq/softirq# || / _--=&gt; preempt-depth# ||| / delay# TASK-PID CPU# |||| TIMESTAMP FUNCTION# | | | |||| | |sh@ubuntu[root]:/sys/kernel/debug/tracing# echo 1 &gt; events/kmem/kmem_cache_alloc/enablesh@ubuntu[root]:/sys/kernel/debug/tracing# cat trace# tracer: nop## entries-in-buffer/entries-written: 55/55 #P:4## _-----=&gt; irqs-off# / _----=&gt; need-resched# | / _---=&gt; hardirq/softirq# || / _--=&gt; preempt-depth# ||| / delay# TASK-PID CPU# |||| TIMESTAMP FUNCTION# | | | |||| | | &lt;...&gt;-41112 [002] .... 313039.418842: kmem_cache_alloc: call_site=memleak_task_test+0x70/0xb0 [memleak] ptr=00000000844671fe bytes_req=5678 bytes_alloc=10320 gfp_flags=GFP_KERNEL &lt;...&gt;-41112 [002] .... 313039.477171: kmem_cache_alloc: call_site=memleak_task_test+0x70/0xb0 [memleak] ptr=00000000157b6ef2 bytes_req=5678 bytes_alloc=10320 gfp_flags=GFP_KERNEL &lt;...&gt;-41112 [002] .... 313039.537008: kmem_cache_alloc: call_site=memleak_task_test+0x70/0xb0 [memleak] ptr=00000000a69f35fe bytes_req=5678 bytes_alloc=10320 gfp_flags=GFP_KERNEL &lt;...&gt;-41112 [002] .... 313039.597138: kmem_cache_alloc: call_site=memleak_task_test+0x70/0xb0 [memleak] ptr=00000000b724443c bytes_req=5678 bytes_alloc=10320 gfp_flags=GFP_KERNEL &lt;...&gt;-41112 [002] .... 313039.656971: kmem_cache_alloc: call_site=memleak_task_test+0x70/0xb0 [memleak] ptr=000000003b39979a bytes_req=5678 bytes_alloc=10320 gfp_flags=GFP_KERNEL &lt;...&gt;-41112 [002] .... 313039.716942: kmem_cache_alloc: call_site=memleak_task_test+0x70/0xb0 [memleak] ptr=000000009a7e8ccf bytes_req=5678 bytes_alloc=10320 gfp_flags=GFP_KERNEL &lt;...&gt;-41112 [002] .... 313039.777130: kmem_cache_alloc: call_site=memleak_task_test+0x70/0xb0 [memleak] ptr=0000000088be3316 bytes_req=5678 bytes_alloc=10320 gfp_flags=GFP_KERNEL &lt;...&gt;-41112 [002] .... 313039.837584: kmem_cache_alloc: call_site=memleak_task_test+0x70/0xb0 [memleak] ptr=00000000b53a68b4 bytes_req=5678 bytes_alloc=10320 gfp_flags=GFP_KERNEL &lt;...&gt;-41112 [002] .... 313039.896236: kmem_cache_alloc: call_site=memleak_task_test+0x70/0xb0 [memleak] ptr=000000004173b1a7 bytes_req=5678 bytes_alloc=10320 gfp_flags=GFP_KERNELsh@ubuntu[root]:/sys/kernel/debug/tracing# ps -aux | grep 41112root 41112 0.0 0.0 0 0 ? S 14:36 0:00 [memleak_task_te]root 41119 0.0 0.0 17668 724 pts/4 S+ 14:37 0:00 grep --color=auto 41112 ftrace的 event tracing 功能也可以实现类似和 bpftrace的功能，但是不能看到 kstack。看到泄露 slab的内核线程后面的工作也比较好做了，可以通过 proc接口查看stack，虽然这种方法看到的stack不是问题线程的stack，但是也有一些帮助 12345sh@ubuntu[root]:/proc/41112# cat stack[&lt;0&gt;] msleep_interruptible+0x30/0x60[&lt;0&gt;] memleak_task_test+0x82/0xb0 [memleak][&lt;0&gt;] kthread+0x104/0x140[&lt;0&gt;] ret_from_fork+0x35/0x40 相关 event trace的其他方法详见Event Trace Doc 使用memleak来定位如果发现内核中有 memleak 现象直接 cat /sys/kernel/debug/kmemleak这是内核每过一段时间救过自动扫描内核空间然后检查出来的memleak，当然也可以手动触发kmemleak检查 echo scan /sys/kernel/debug/kmemleak 1234567891011121314151617181920212223242526272829303132333435363738394041sh@ubuntu[root]:/sys/kernel/debug# cat kmemleakunreferenced object 0xffff8ea54df1a990 (size 5678): comm &quot;memleak_task_te&quot;, pid 4246, jiffies 4295117333 (age 199.916s) hex dump (first 32 bytes): 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b kkkkkkkkkkkkkkkk 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b kkkkkkkkkkkkkkkk backtrace: [&lt;0000000044bb737d&gt;] kmem_cache_alloc+0xec/0x260 [&lt;00000000adf25388&gt;] 0xffffffffc08ea070 [&lt;00000000ba36b6c9&gt;] kthread+0x104/0x140 [&lt;00000000c2aa164b&gt;] ret_from_fork+0x3a/0x50unreferenced object 0xffff8ea54df1d310 (size 5678): comm &quot;memleak_task_te&quot;, pid 4246, jiffies 4295117349 (age 199.852s) hex dump (first 32 bytes): 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b kkkkkkkkkkkkkkkk 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b kkkkkkkkkkkkkkkk backtrace: [&lt;0000000044bb737d&gt;] kmem_cache_alloc+0xec/0x260 [&lt;00000000adf25388&gt;] 0xffffffffc08ea070 [&lt;00000000ba36b6c9&gt;] kthread+0x104/0x140 [&lt;00000000c2aa164b&gt;] ret_from_fork+0x3a/0x50unreferenced object 0xffff8ea54df18010 (size 5678): comm &quot;memleak_task_te&quot;, pid 4246, jiffies 4295117364 (age 199.792s) hex dump (first 32 bytes): 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b kkkkkkkkkkkkkkkk 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b kkkkkkkkkkkkkkkk backtrace: [&lt;0000000044bb737d&gt;] kmem_cache_alloc+0xec/0x260 [&lt;00000000adf25388&gt;] 0xffffffffc08ea070 [&lt;00000000ba36b6c9&gt;] kthread+0x104/0x140 [&lt;00000000c2aa164b&gt;] ret_from_fork+0x3a/0x50unreferenced object 0xffff8ea548de5310 (size 5678): comm &quot;memleak_task_te&quot;, pid 4246, jiffies 4295117379 (age 199.732s) hex dump (first 32 bytes): 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b kkkkkkkkkkkkkkkk 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b 6b kkkkkkkkkkkkkkkk backtrace: [&lt;0000000044bb737d&gt;] kmem_cache_alloc+0xec/0x260 [&lt;00000000adf25388&gt;] 0xffffffffc08ea070 [&lt;00000000ba36b6c9&gt;] kthread+0x104/0x140 [&lt;00000000c2aa164b&gt;] ret_from_fork+0x3a/0x50 这是我内核模块卸载之后打印的kstack，可以根据comm线程名字来找出 kmemleak 的内鬼真凶。 kmemleak的原理可以参考内核文档 kmemleak 实现原理 使用slub_debug来定位在打开 slub_debug的情况下，定位到 liulangrenaaa_cache slab 泄露之后，可以trace这个slab的分配行为。需要打开内核编译选项，只有slub有这个trace功能，slab slob没有 12+CONFIG_SLUB_DEBUG=y+CONFIG_SLUB_DEBUG_ON=y 重新编译内核之后，使能slub trace 1234sh@ubuntu[root]:~# echo 1 &gt; /sys/kernel/slab/liulangrenaaa_cache/tracesh@ubuntu[root]:~# sleep 1sh@ubuntu[root]:~# echo 0 &gt; /sys/kernel/slab/liulangrenaaa_cache/tracesh@ubuntu[root]:~# dmesg | tail 观察kmesg输出，每次分配，释放都会被trace下来，最后destory也会，如果在没有完全释放所有分配的slub之前destory slub 就会产生一个bug警告 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081[ 2109.158214] TRACE liulangrenaaa_cache alloc 0x00000000459b1c3f inuse=3 fp=0x0000000000000000[ 2109.158217] CPU: 0 PID: 4561 Comm: memleak_task_te Kdump: loaded Tainted: G OE 5.4.44 #2[ 2109.158217] Hardware name: VMware, Inc. VMware Virtual Platform/440BX Desktop Reference Platform, BIOS 6.00 07/29/2019[ 2109.158218] Call Trace:[ 2109.158221] dump_stack+0x98/0xda[ 2109.158224] alloc_debug_processing.cold+0x53/0x72[ 2109.158225] ___slab_alloc+0x506/0x590[ 2109.158226] ? memleak_task_test+0x70/0xb0 [memleak][ 2109.158227] ? kmem_cache_alloc+0x23e/0x260[ 2109.158228] ? memleak_task_test+0x70/0xb0 [memleak][ 2109.158229] ? memleak_task_test+0x70/0xb0 [memleak][ 2109.158230] __slab_alloc+0x51/0x90[ 2109.158231] kmem_cache_alloc+0x23e/0x260[ 2109.158232] ? memleak_task_test+0x70/0xb0 [memleak][ 2109.158233] memleak_task_test+0x70/0xb0 [memleak][ 2109.158234] kthread+0x104/0x140[ 2109.158235] ? 0xffffffffc08ea000[ 2109.158236] ? kthread_park+0x90/0x90[ 2109.158237] ret_from_fork+0x3a/0x50[ 2109.238159] =============================================================================[ 2109.238162] BUG liulangrenaaa_cache (Tainted: G OE ): Objects remaining in liulangrenaaa_cache on __kmem_cache_shutdown()[ 2109.238171] -----------------------------------------------------------------------------[ 2109.238172] Disabling lock debugging due to kernel taint[ 2109.238173] INFO: Slab 0x000000001be073de objects=3 used=2 fp=0x00000000fd86908a flags=0xfffffc0010200[ 2109.238176] CPU: 1 PID: 4580 Comm: rmmod Kdump: loaded Tainted: G B OE 5.4.44 #2[ 2109.238176] Hardware name: VMware, Inc. VMware Virtual Platform/440BX Desktop Reference Platform, BIOS 6.00 07/29/2019[ 2109.238177] Call Trace:[ 2109.238180] dump_stack+0x98/0xda[ 2109.238183] slab_err+0xb7/0xdc[ 2109.238184] __kmem_cache_shutdown.cold+0x1b/0x121[ 2109.238186] shutdown_cache+0x16/0x160[ 2109.238187] kmem_cache_destroy+0x21c/0x240[ 2109.238189] memleak_exit+0x26/0x28 [memleak][ 2109.238191] __x64_sys_delete_module+0x147/0x2b0[ 2109.238192] ? entry_SYSCALL_64_after_hwframe+0x49/0xbe[ 2109.238193] ? trace_hardirqs_on+0x38/0xf0[ 2109.238195] do_syscall_64+0x5f/0x1a0[ 2109.238196] entry_SYSCALL_64_after_hwframe+0x49/0xbe[ 2109.238197] RIP: 0033:0x7f78838e7a3b[ 2109.238199] Code: 73 01 c3 48 8b 0d 55 84 0c 00 f7 d8 64 89 01 48 83 c8 ff c3 66 2e 0f 1f 84 00 00 00 00 00 90 f3 0f 1e fa b8 b0 00 00 00 0f 05 &lt;48&gt; 3d 01 f0 ff ff 73 01 c3 48 8b 0d 25 84 0c 00 f7 d8 64 89 01 48[ 2109.238199] RSP: 002b:00007ffda198cbf8 EFLAGS: 00000206 ORIG_RAX: 00000000000000b0[ 2109.238200] RAX: ffffffffffffffda RBX: 000055a871197790 RCX: 00007f78838e7a3b[ 2109.238201] RDX: 000000000000000a RSI: 0000000000000800 RDI: 000055a8711977f8[ 2109.238201] RBP: 00007ffda198cc58 R08: 0000000000000000 R09: 0000000000000000[ 2109.238201] R10: 00007f7883963ac0 R11: 0000000000000206 R12: 00007ffda198ce30[ 2109.238202] R13: 00007ffda198e752 R14: 000055a8711972a0 R15: 000055a871197790[ 2109.238204] INFO: Object 0x00000000459b1c3f @offset=16[ 2109.238205] INFO: Allocated in memleak_task_test+0x70/0xb0 [memleak] age=20 cpu=0 pid=4561[ 2109.238206] __slab_alloc+0x51/0x90[ 2109.238207] kmem_cache_alloc+0x23e/0x260[ 2109.238208] memleak_task_test+0x70/0xb0 [memleak][ 2109.238209] kthread+0x104/0x140[ 2109.238210] ret_from_fork+0x3a/0x50[ 2109.238211] INFO: Object 0x00000000f693e050 @offset=10640[ 2109.238212] INFO: Allocated in memleak_task_test+0x70/0xb0 [memleak] age=35 cpu=0 pid=4561[ 2109.238213] __slab_alloc+0x51/0x90[ 2109.238214] kmem_cache_alloc+0x23e/0x260[ 2109.238215] memleak_task_test+0x70/0xb0 [memleak][ 2109.238215] kthread+0x104/0x140[ 2109.238216] ret_from_fork+0x3a/0x50[ 2109.238219] kmem_cache_destroy liulangrenaaa_cache: Slab cache still has objects[ 2109.238221] CPU: 1 PID: 4580 Comm: rmmod Kdump: loaded Tainted: G B OE 5.4.44 #2[ 2109.238221] Hardware name: VMware, Inc. VMware Virtual Platform/440BX Desktop Reference Platform, BIOS 6.00 07/29/2019[ 2109.238221] Call Trace:[ 2109.238222] dump_stack+0x98/0xda[ 2109.238223] kmem_cache_destroy.cold+0x15/0x1a[ 2109.238224] memleak_exit+0x26/0x28 [memleak][ 2109.238225] __x64_sys_delete_module+0x147/0x2b0[ 2109.238226] ? entry_SYSCALL_64_after_hwframe+0x49/0xbe[ 2109.238227] ? trace_hardirqs_on+0x38/0xf0[ 2109.238228] do_syscall_64+0x5f/0x1a0[ 2109.238229] entry_SYSCALL_64_after_hwframe+0x49/0xbe[ 2109.238229] RIP: 0033:0x7f78838e7a3b[ 2109.238230] Code: 73 01 c3 48 8b 0d 55 84 0c 00 f7 d8 64 89 01 48 83 c8 ff c3 66 2e 0f 1f 84 00 00 00 00 00 90 f3 0f 1e fa b8 b0 00 00 00 0f 05 &lt;48&gt; 3d 01 f0 ff ff 73 01 c3 48 8b 0d 25 84 0c 00 f7 d8 64 89 01 48[ 2109.238230] RSP: 002b:00007ffda198cbf8 EFLAGS: 00000206 ORIG_RAX: 00000000000000b0[ 2109.238238] RAX: ffffffffffffffda RBX: 000055a871197790 RCX: 00007f78838e7a3b[ 2109.238239] RDX: 000000000000000a RSI: 0000000000000800 RDI: 000055a8711977f8[ 2109.238239] RBP: 00007ffda198cc58 R08: 0000000000000000 R09: 0000000000000000[ 2109.238240] R10: 00007f7883963ac0 R11: 0000000000000206 R12: 00007ffda198ce30[ 2109.238240] R13: 00007ffda198e752 R14: 000055a8711972a0 R15: 000055a871197790","link":"/2020/09/18/memory/%E9%9D%A2%E8%AF%95/%E5%86%99%E4%B8%80%E4%B8%AA%E6%B6%88%E8%80%97%E5%AE%8C%E7%89%A9%E7%90%86%E5%86%85%E5%AD%98%E7%9A%84%E7%A8%8B%E5%BA%8F/%E5%AE%9A%E4%BD%8Dslub%E5%86%85%E5%AD%98%E6%B3%84%E9%9C%B2%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E6%B3%95/"},{"title":"vscode task.json 与 launch.json配置","text":"快捷键 撤销光标移动 Ctrl + U 一行代码太长被折叠成为多行了，想切换回一行 Alt + z 多行编辑 Alt + 鼠标左键可以选择好要编辑位置 跳转 F12 跳转回来 带前进后退的鼠标后退 or Alt + &lt;- 折叠文件代码 Ctrl + 0, 展开文件代码 Ctrl + J 折叠部分代码 Ctrl + Shift + [, 展开部分代码 Ctrl + Shift + ] 显示参数：放到函数后面()内，Ctrl + Shift + Space 切换到原有终端 1Ctrl + ` 创建一个新终端 1Ctrl + Shift + ` 移动到行首 Home， 移动到行尾 END 移动到文件头 Ctrl + Home， 移动到文件尾 Ctrl + END 编辑器Tab 切换 Ctrl + PageUp + Ctrl + PageDown 快速格式化当前文件代码 Shift + Alt + F task.jsonWhat?vscode 提供了 默认task的配置 和 每个workspace的 task配置。可以通过 Ctrl + Shift + p，输入task，可以选择 配置 运行 task。可以减少运行的时候少敲几个命令。。 example下面看我给linux kernel 工作区配置的一些task。 文件位于 123amd_server@ubuntu: ~/workspace/linux-stable# ls .vscodec_cpp_properties.json launch.json tasks.jsonamd_server@ubuntu: ~/workspace/linux-stable# 内容： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889{ // See https://go.microsoft.com/fwlink/?LinkId=733558 // for the documentation about the tasks.json format &quot;version&quot;: &quot;2.0.0&quot;, &quot;tasks&quot;: [ { &quot;label&quot;: &quot;vm-start&quot;, &quot;type&quot;: &quot;shell&quot;, &quot;command&quot;: &quot;sudo qemu-system-x86_64 -kernel /home/ubuntu/workspace/linux-stable/out/arch/x86/boot/bzImage -hda /home/ubuntu/myspace/qemu_build/stable_ubuntu.img -append &quot;root=/dev/sda5 console=ttyS0 crashkernel=256M&quot; -smp 4 -m 2048 --enable-kvm -net nic -net user,hostfwd=tcp::2222-:22 --nographic -fsdev local,id=fs1,path=/home/ubuntu/workspace/share,security_model=none -device virtio-9p-pci,fsdev=fs1,mount_tag=host_share&quot;, &quot;presentation&quot;: { &quot;echo&quot;: true, &quot;clear&quot;: true, &quot;group&quot;: &quot;vm&quot; }, }, { &quot;label&quot;: &quot;vm-stop&quot;, &quot;type&quot;: &quot;shell&quot;, &quot;command&quot;: &quot;sudo pkill qemu&quot;, &quot;presentation&quot;: { &quot;echo&quot;: true, &quot;clear&quot;: true, &quot;group&quot;: &quot;vm&quot; }, }, { &quot;label&quot;: &quot;vm-restart&quot;, &quot;type&quot;: &quot;shell&quot;, &quot;command&quot;: &quot;sudo pkill qemu ; echo &quot;qemu have been stopped, now run the new qemu...&quot;; sudo qemu-system-x86_64 -kernel /home/ubuntu/workspace/linux-stable/out/arch/x86/boot/bzImage -hda /home/ubuntu/myspace/qemu_build/stable_ubuntu.img -append &quot;root=/dev/sda5 console=ttyS0 crashkernel=256M&quot; -smp 4 -m 2048 --enable-kvm -net nic -net user,hostfwd=tcp::2222-:22 --nographic -fsdev local,id=fs1,path=/home/ubuntu/workspace/share,security_model=none -device virtio-9p-pci,fsdev=fs1,mount_tag=host_share&quot;, &quot;presentation&quot;: { &quot;echo&quot;: true, &quot;clear&quot;: true, &quot;group&quot;: &quot;vm&quot; }, }, { &quot;label&quot;: &quot;make&quot;, &quot;type&quot;: &quot;shell&quot;, &quot;command&quot;: &quot;make -j10 O=./out &amp;&amp; python ./scripts/clang-tools/gen_compile_commands.py -d ./out/&quot;, &quot;group&quot;: { &quot;kind&quot;: &quot;build&quot;, &quot;isDefault&quot;: true }, &quot;presentation&quot;: { &quot;echo&quot;: false, &quot;group&quot;: &quot;build&quot; } }, { &quot;label&quot;: &quot;make clean&quot;, &quot;type&quot;: &quot;shell&quot;, &quot;command&quot;: &quot;make mrproper;&quot;, &quot;group&quot;: { &quot;kind&quot;: &quot;build&quot;, &quot;isDefault&quot;: true }, &quot;presentation&quot;: { &quot;echo&quot;: false, &quot;group&quot;: &quot;build&quot; } }, { &quot;label&quot;: &quot;remake&quot;, &quot;type&quot;: &quot;shell&quot;, &quot;command&quot;: &quot;make mrproper &amp;&amp; make -j10 O=./out &amp;&amp; python ./scripts/clang-tools/gen_compile_commands.py -d ./out/&quot;, &quot;group&quot;: { &quot;kind&quot;: &quot;build&quot;, &quot;isDefault&quot;: true }, &quot;presentation&quot;: { &quot;echo&quot;: false, &quot;group&quot;: &quot;build&quot; } }, { &quot;label&quot;: &quot;generate label&quot;, &quot;type&quot;: &quot;shell&quot;, &quot;command&quot;: &quot;python ./scripts/clang-tools/gen_compile_commands.py -d ./out/&quot;, &quot;group&quot;: { &quot;kind&quot;: &quot;build&quot;, &quot;isDefault&quot;: true }, &quot;presentation&quot;: { &quot;echo&quot;: false, &quot;group&quot;: &quot;build&quot; } } ]} 设置了 vm-start vm-stop vm-restart make make clean remake generate label，这些task，可以随时 Ctrl + Shift + p 呼出来运行。极大方便了编译运行配置。。 launch.jsonWhat?vscode 提供了 launch 来配置debug相关参数。后面会以 benos 为例记录一下 example下面看我给benos 工作区配置的一些参数 参考CodeTalks参考极客课程","link":"/2021/01/24/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/vscode%E9%87%8D%E8%A6%81%E9%85%8D%E7%BD%AE%E5%92%8C%E5%BF%AB%E6%8D%B7%E9%94%AE%E8%AE%B0%E5%BD%95/"},{"title":"stack_overflow","text":"stack_overflow 概述一般linux内核栈大小都是8kb，当内核中调用层次太多，或者局部变量太大的时候，就会使得栈实际使用的大小超过8Kb，但是8kb之外就是别的内存了，不属于当前task的 stack了。一般会破坏其他task的stack或者其他的重要数据结构(slub等)，轻则导致运行的程序的异常，重则导致系统直接panic，而且由于破坏数据结构不确定，panic的位置也是不确定的，所以往往很难定位这种问题。 stack_overflow 如何检测传统RTOS上如何做的？这是我大四总结的RTOS栈溢出如何检测的节选 检查指针：每次任务切换时检查栈指针是在合法范围内（是否越界），但是仍然存在检测不到情况，比如在任务执行过程中栈指针越界了，但是在任务切换前，栈指针pop到了合法位置。 检查数据：将栈空间顶部16字节数据初始化一定值如0XA5，然后在每次任务切换的时候检查栈顶部16字节有没有改变，如果改变了就说明肯定栈溢出了，想较于检查指针方法费时费事。但是仍然可能检测不到栈溢出，比如虽然溢出但是栈顶部数据没有被修改，溢出后的数据被修改了。 linux上如何做的？其实在linux 系统上也无外乎这几种方法。（其实多了一种方法） 首先linux会在kernel stack的结尾处放置一个magic num，就是方法212345678910111213141516171819void set_task_stack_end_magic(struct task_struct *tsk) { *end_of_stack(tsk) = STACK_END_MAGIC; /* for overflow detection */}asmlinkage __visible void __init __no_sanitize_address start_kernel(void) { set_task_stack_end_magic(&amp;init_task);}static struct task_struct *dup_task_struct(struct task_struct *orig, int node) { setup_thread_stack(tsk, orig); clear_user_return_notifier(tsk); clear_tsk_need_resched(tsk); set_task_stack_end_magic(tsk);}static __latent_entropy struct task_struct *copy_process(...) { p = dup_task_struct(current, node); ......} 那么何时来检测呢？ 1234567891011121314#define STACK_END_MAGIC 0x57AC6E9D#define task_stack_end_corrupted(task) \\ (*(end_of_stack(task)) != STACK_END_MAGIC)static inline void schedule_debug(struct task_struct *prev, bool preempt){#ifdef CONFIG_SCHED_STACK_END_CHECK if (task_stack_end_corrupted(prev)) panic(&quot;corrupted stack end detected inside scheduler\\n&quot;); if (task_scs_end_corrupted(prev)) panic(&quot;corrupted shadow stack detected inside scheduler\\n&quot;);#endif} 这个功能还需要 开启 CONFIG_SCHED_STACK_END_CHECK 配置。 使用虚拟内存的特性来保证。以前内核栈都是通过连续物理内存页面来分配的，且是线性映射的内存，这样做的好处是减少了部分开销。但是这样也导致如果发生栈溢出，就是真切的踩到别的物理内存了。所以内核提供了一个新的方式，配置CONFIG_VMAP_STACK之后，可以使用 vmalloc的方式来提供栈空间，由于 vmalloc 还提供了 guard page机制，所以一旦检测到栈溢出，就可以立即报告出来。由这个commit：(ba14a194a434ccc8f733e263ad2ce941e35e5787)加入,后面又优化了一把(ac496bf48d97f2503eaa353996a4dd5e4383eaf0)，主要是用cached_stacks 这个 percpu 变量缓存了 一些vmalloc页面。 1234567891011121314151617181920212223static unsigned long *alloc_thread_stack_node(struct task_struct *tsk, int node){#ifdef CONFIG_VMAP_STACK void *stack; stack = __vmalloc_node_range(THREAD_SIZE, THREAD_ALIGN, VMALLOC_START, VMALLOC_END, THREADINFO_GFP &amp; ~__GFP_ACCOUNT, PAGE_KERNEL, 0, node, __builtin_return_address(0)); return stack;#else struct page *page = alloc_pages_node(node, THREADINFO_GFP,THREAD_SIZE_ORDER); if (likely(page)) { tsk-&gt;stack = kasan_reset_tag(page_address(page)); return tsk-&gt;stack; } return NULL;#endif}static struct task_struct *dup_task_struct(struct task_struct *orig, int node) { stack = alloc_thread_stack_node(tsk, node);} gcc特性保证可以在函数在gcc编译的时候，加入”-fstack-protector”选项即可。每个函数会对应一个stack frame，这样每个stack frame都需要一个canary，这会消耗掉一部分的栈空间。此外，由于每次函数返回时都需要检测canary，代码的整执行时间也势必会增加。 12345jenkins@server:~/workspace/linux_kernel_ltp$ cat ./debug_out/.config | grep STACKPROTECTORCONFIG_CC_HAS_SANE_STACKPROTECTOR=yCONFIG_HAVE_STACKPROTECTOR=yCONFIG_STACKPROTECTOR=yCONFIG_STACKPROTECTOR_STRONG=y 看makefile文件 1234stackp-flags-y := -fno-stack-protectorstackp-flags-$(CONFIG_STACKPROTECTOR) := -fstack-protectorstackp-flags-$(CONFIG_STACKPROTECTOR_STRONG) := -fstack-protector-strongKBUILD_CFLAGS += $(stackp-flags-y) 可以参考文章1可以参考文章2 stack_overflow 配置 开启CONFIG_SCHED_STACK_END_CHECK，关闭CONFIG_VMAP_STACK查看效果参考代码 123456789101112131415161718192021222324crash&gt; log | tail -n 18[ 16.480669] systemd-journald[122]: File /var/log/journal/99626a991f7d4bfa8cb2df0e3b8ce643/user-1000.journal corrupted or uncleanly shut .[ 28.311464] rfkill: input handler disabled[ 32.585818] loop0: detected capacity change from 63672 to 0[ 33.713469] loop0: detected capacity change from 113424 to 0[ 36.372258] stack_overflow: loading out-of-tree module taints kernel.[ 37.396337] k_stackoverflow_thread have alloc 4096 bytes in stack, times = 9[ 38.420098] k_stackoverflow_thread have alloc 4096 bytes in stack, times = 8[ 39.444356] k_stackoverflow_thread have alloc 4096 bytes in stack, times = 7[ 39.444369] Kernel panic - not syncing: corrupted stack end detected inside scheduler[ 39.445278] CPU: 3 PID: 3378 Comm: k_stackoverflow Kdump: loaded Tainted: G O 5.11.0-rc5+ #10[ 39.445278] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS 1.13.0-1ubuntu1.1 04/01/2014[ 39.445278] Call Trace:[ 39.445278] ? k_stackoverflow_thread.cold+0x16/0x20 [stack_overflow][ 39.445278] ? k_stackoverflow_thread.cold+0x16/0x20 [stack_overflow][ 39.445278] ? k_stackoverflow_thread.cold+0x16/0x20 [stack_overflow][ 39.445278] ? kthread+0x10a/0x140[ 39.445278] ? kthread_park+0x80/0x80[ 39.445278] ? ret_from_fork+0x22/0x30crash&gt;crash&gt; btPID: 3378 TASK: ffff8fce4d6d4b40 CPU: 3 COMMAND: &quot;k_stackoverflow&quot;bt: invalid size request: 0 type: &quot;stack contents&quot;bt: read of stack at fffffe00000b1000 failed 可以看到报错信息，显示很明显，打印出了backtrace 1[ 39.444369] Kernel panic - not syncing: corrupted stack end detected inside scheduler 但是看到bt 命令已经不能正常显示了。 开启CONFIG_VMAP_STACK，使用虚拟内存机制保证参考代码重新编译内核启动，编译安装test 驱动，得到如下报错信息，由于检测到栈溢出，直接panic了，需要 crash查看现场信息12345678910111213141516171819202122232425262728293031323334353637383940414243[ 1188.110899] k_stackoverflow_thread have alloc 4096 bytes in stack, times = 9[ 1189.134809] k_stackoverflow_thread have alloc 4096 bytes in stack, times = 8[ 1190.158790] k_stackoverflow_thread have alloc 4096 bytes in stack, times = 7[ 1190.159424] BUG: stack guard page was hit at 000000009af4b1c0 (stack is 0000000013046cba..00000000bab11d7d)[ 1190.159427] kernel stack overflow (double-fault): 0000 [#1] SMP NOPTI[ 1190.159428] CPU: 1 PID: 3607 Comm: k_stackoverflow Kdump: loaded Tainted: G O 5.11.0-rc5+ #8[ 1190.159430] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS 1.13.0-1ubuntu1.1 04/01/2014[ 1190.159431] RIP: 0010:k_stackoverflow_thread+0x8/0x80 [stack_overflow][ 1190.159433] Code: Unable to access opcode bytes at RIP 0xffffffffc0238fde.[ 1190.159434] RSP: 0018:ffff9cc38089bec0 EFLAGS: 00010286[ 1190.159436] RAX: 0000000000000040 RBX: ffffffffc0239000 RCX: 0000000000000000[ 1190.159438] RDX: 0000000000000000 RSI: ffff8bc8bfc97fd0 RDI: 0000000000000000[ 1190.159439] RBP: 0000000000000000 R08: ffff8bc8bfc97fd0 R09: 0000000000000001[ 1190.159440] R10: 0000000000000001 R11: 0000000000000001 R12: ffff8bc857788040[ 1190.159441] R13: ffff9cc380907ba8 R14: 0000000000000000 R15: ffff8bc8843a0040[ 1190.159443] FS: 0000000000000000(0000) GS:ffff8bc8bfc80000(0000) knlGS:0000000000000000[ 1190.159444] CS: 0010 DS: 0000 ES: 0000 CR0: 0000000080050033[ 1190.159445] CR2: ffffffffc0238fde CR3: 000000001627c000 CR4: 00000000000006e0[ 1190.159446] Call Trace:[ 1190.159447] ? __lock_acquire+0x3be/0x28a0[ 1190.159448] ? __lock_acquire+0x3be/0x28a0[ 1190.159486] ? rcu_read_lock_sched_held+0x4d/0x80[ 1190.159487] ? __lock_acquire+0x3be/0x28a0[ 1190.159488] ? __lock_acquire+0x3be/0x28a0[ 1190.159489] ? desc_read_finalized_seq+0x26/0x80[ 1190.159490] ? find_held_lock+0x2b/0x80[ 1190.159491] ? console_unlock+0x35f/0x570[ 1190.159492] ? lockdep_hardirqs_on_prepare+0xd4/0x170[ 1190.159493] ? console_unlock+0x487/0x570[ 1190.159494] ? __irq_work_queue_local+0x48/0x50[ 1190.159495] ? irq_work_queue+0x20/0x30[ 1190.159495] ? wake_up_klogd.part.0+0x32/0x40[ 1190.159496] ? vprintk_emit+0x98/0x2a0[ 1190.159497] ? 0xffffffffc0239000[ 1190.159498] ? printk+0x53/0x6a[ 1190.159499] k_stackoverflow_thread.cold+0x16/0x20 [stack_overflow][ 1190.159500] k_stackoverflow_thread.cold+0x16/0x20 [stack_overflow][ 1190.159501] k_stackoverflow_thread.cold+0x16/0x20 [stack_overflow][ 1190.159502] kthread+0x10a/0x140[ 1190.159503] ? kthread_park+0x80/0x80[ 1190.159504] ret_from_fork+0x22/0x30[ 1190.159505] Modules linked in: stack_overflow(O)crash&gt; 可以很明显看到报错信息指出是 stack overflow 12[ 1190.159424] BUG: stack guard page was hit at 000000009af4b1c0 (stack is 0000000013046cba..00000000bab11d7d)[ 1190.159427] kernel stack overflow (double-fault): 0000 [#1] SMP NOPTI 反汇编一下就可以知道函数地址，结合 backtrace 就看可以知道具体原因 gcc 相关参数 默认开启的？我试了一下，有栈溢出，但是没有触发报错，后面有空再看…123456jenkins@server:~/workspace/linux_kernel_ltp$ cat debug_out/.config |grep STACKPROTECTORCONFIG_CC_HAS_SANE_STACKPROTECTOR=yCONFIG_HAVE_STACKPROTECTOR=yCONFIG_STACKPROTECTOR=yCONFIG_STACKPROTECTOR_STRONG=yjenkins@server:~/workspace/linux_kernel_ltp$ 总结 开启 CONFIG_SCHED_STACK_END_CHECK 是有一定 overhead的，每次sched的时候都会去检查栈顶的 magic num有无被修改过，且这个也不一定准确，也可能是虽然栈溢了，但是恰好没有改写到 magic num的数据，这种就比较难办。 开启 CONFIG_VMAP_STACK，相对来说 overhead 就少很多了，现在 64bit kernel都是默认打开这个宏定义的，32bit 猜测主要可能是 vmalloc区域太小。 參考知乎文章 参考文章1 參考文章2","link":"/2021/01/27/%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86/stack_overflow/"},{"title":"跟踪文件异常删除、修改","text":"场景实际工作中经常会遇到一些某个业务的重要的配置文件被删除了，或者某个重要的配置文件被串改了，而这个文件目录 往往又隐藏很深，不太可能被别的业务删除或者篡改，那么如何找到删除文件或者篡改文件的真凶呢。 bpftrace 工具使用首先删除一个文件我们一般会使用rm 来做，对应vfs层的就是 unlink方法，可以对unlink跟踪。这里当然可以使用 ftrace工具，但是我已经习惯使用 bpftrace了如下对 unlink的操作进行跟踪 123sh@ubuntu[root]:/home/rlk/workspace/test# bpftrace -e 'kprobe:vfs_unlink {printf(&quot;pid: [%d], comm: [%s]\\n&quot;, pid, comm)}'Attaching 1 probe...pid: [95301], comm: [rm] 这样的打印信息会很多，如果是在生产环境上，有用的信息往往来不及筛选就会被淹没掉，这时可以加上对删除文件名的限制过滤 123456789101112131415161718sh@ubuntu[root]:/sys/kernel/debug/tracing# cd events/syscalls/sys_enter_unlinkatsh@ubuntu[root]:/sys/kernel/debug/tracing/events/syscalls/sys_enter_unlinkat# lsenable filter format hist id triggersh@ubuntu[root]:/sys/kernel/debug/tracing/events/syscalls/sys_enter_unlinkat# cat formatname: sys_enter_unlinkatID: 722format: field:unsigned short common_type; offset:0; size:2; signed:0; field:unsigned char common_flags; offset:2; size:1; signed:0; field:unsigned char common_preempt_count; offset:3; size:1; signed:0; field:int common_pid; offset:4; size:4; signed:1; field:int __syscall_nr; offset:8; size:4; signed:1; field:int dfd; offset:16; size:8; signed:0; field:const char * pathname; offset:24; size:8; signed:0; field:int flag; offset:32; size:8; signed:0;print fmt: &quot;dfd: 0x%08lx, pathname: 0x%08lx, flag: 0x%08lx&quot;, ((unsigned long)(REC-&gt;dfd)), ((unsigned long)(REC-&gt;pathname)), ((unsigned long)(REC-&gt;flag)) 可以看出 pathname 就是文件名，下面我们添加对文件名是asdf文件删除的监控，可以清晰的看到pid, comm, 删除的文件名 123sh@ubuntu[root]:/home/rlk/workspace/test# bpftrace -e 'tracepoint:syscalls:sys_enter_unlinkat /str(args-&gt;pathname) == &quot;asdf&quot;/ {printf(&quot;pid: [%d], comm: [%s], rm [%s]\\n&quot;, pid, comm, str(args-&gt;pathname))}'Attaching 1 probe...pid: [95566], comm: [rm], rm [asdf] 这里需要注意 str(args-&gt;pathname) 的用法 这里我们使用的是tracepoint来跟踪的，如果你的机器上支持BTF，使用kprobe来跟踪也是可以的，过滤相关条件的时候比较复杂。 同理记录篡改文件也很好办，可以跟踪有哪些进程写入了这个文件或者打开了这个文件，都是有嫌疑的。 其他工具perffanotify 机制","link":"/2020/09/14/%E5%86%85%E6%A0%B8%E8%A7%82%E6%B5%8B/bpf%E7%9B%B8%E5%85%B3/%E8%B7%9F%E8%B8%AA%E6%96%87%E4%BB%B6%E5%BC%82%E5%B8%B8%E5%88%A0%E9%99%A4%E3%80%81%E4%BF%AE%E6%94%B9/"},{"title":"kprobes 原理与使用","text":"kprobe 原理开启 CONFIG_FUNCTION_TRACER 时在开启 CONFIG_FUNCTION_TRACER 情况下，不开启 kprobe 的情况下，产生crash 1234567891011121314151617181920212223crash&gt; dis kmem_cache_alloc0xffffffffbc089800 &lt;kmem_cache_alloc&gt;: nopl 0x0(%rax,%rax,1) [FTRACE NOP]0xffffffffbc089805 &lt;kmem_cache_alloc+5&gt;: push %r150xffffffffbc089807 &lt;kmem_cache_alloc+7&gt;: push %r140xffffffffbc089809 &lt;kmem_cache_alloc+9&gt;: mov %esi,%r14d0xffffffffbc08980c &lt;kmem_cache_alloc+12&gt;: push %r130xffffffffbc08980e &lt;kmem_cache_alloc+14&gt;: push %r120xffffffffbc089810 &lt;kmem_cache_alloc+16&gt;: mov 0x185c69d(%rip),%r12d # 0xffffffffbd8e5eb40xffffffffbc089817 &lt;kmem_cache_alloc+23&gt;: push %rbp0xffffffffbc089818 &lt;kmem_cache_alloc+24&gt;: mov %rdi,%rbp0xffffffffbc08981b &lt;kmem_cache_alloc+27&gt;: push %rbx0xffffffffbc08981c &lt;kmem_cache_alloc+28&gt;: and %esi,%r12d0xffffffffbc08981f &lt;kmem_cache_alloc+31&gt;: mov 0x1c(%rdi),%ebx0xffffffffbc089822 &lt;kmem_cache_alloc+34&gt;: mov %r12d,%edi0xffffffffbc089825 &lt;kmem_cache_alloc+37&gt;: mov 0x30(%rsp),%r150xffffffffbc08982a &lt;kmem_cache_alloc+42&gt;: callq 0xffffffffbc061b20 &lt;fs_reclaim_acquire&gt;0xffffffffbc08982f &lt;kmem_cache_alloc+47&gt;: mov %r12d,%edi0xffffffffbc089832 &lt;kmem_cache_alloc+50&gt;: callq 0xffffffffbc061a50 &lt;fs_reclaim_release&gt;0xffffffffbc089837 &lt;kmem_cache_alloc+55&gt;: test $0x400,%r12d0xffffffffbc08983e &lt;kmem_cache_alloc+62&gt;: jne 0xffffffffbc089c15 &lt;kmem_cache_alloc+1045&gt;0xffffffffbc089844 &lt;kmem_cache_alloc+68&gt;: mov %r12d,%esi0xffffffffbc089847 &lt;kmem_cache_alloc+71&gt;: mov %rbp,%rdicrash&gt; 可以看到 kmem_cache_alloc+0 位置的instruction是 nopl 在开启对 kmem_cache_alloc 的 kprobe之后，再出发一次 crash,然后看 函数反汇编 1234567891011121314151617181920212223crash&gt; dis kmem_cache_alloc0xffffffff8329ac80 &lt;kmem_cache_alloc&gt;: callq 0xffffffffc02270000xffffffff8329ac85 &lt;kmem_cache_alloc+5&gt;: push %r150xffffffff8329ac87 &lt;kmem_cache_alloc+7&gt;: mov %esi,%ecx0xffffffff8329ac89 &lt;kmem_cache_alloc+9&gt;: mov $0x1,%edx0xffffffff8329ac8e &lt;kmem_cache_alloc+14&gt;: push %r140xffffffff8329ac90 &lt;kmem_cache_alloc+16&gt;: push %r130xffffffff8329ac92 &lt;kmem_cache_alloc+18&gt;: mov %rdi,%r130xffffffff8329ac95 &lt;kmem_cache_alloc+21&gt;: push %r120xffffffff8329ac97 &lt;kmem_cache_alloc+23&gt;: push %rbp0xffffffff8329ac98 &lt;kmem_cache_alloc+24&gt;: push %rbx0xffffffff8329ac99 &lt;kmem_cache_alloc+25&gt;: sub $0x18,%rsp0xffffffff8329ac9d &lt;kmem_cache_alloc+29&gt;: mov 0x1c(%rdi),%ebx0xffffffff8329aca0 &lt;kmem_cache_alloc+32&gt;: mov %esi,0x4(%rsp)0xffffffff8329aca4 &lt;kmem_cache_alloc+36&gt;: lea 0x8(%rsp),%rsi0xffffffff8329aca9 &lt;kmem_cache_alloc+41&gt;: mov 0x48(%rsp),%r150xffffffff8329acae &lt;kmem_cache_alloc+46&gt;: mov %gs:0x28,%rax0xffffffff8329acb7 &lt;kmem_cache_alloc+55&gt;: mov %rax,0x10(%rsp)0xffffffff8329acbc &lt;kmem_cache_alloc+60&gt;: xor %eax,%eax0xffffffff8329acbe &lt;kmem_cache_alloc+62&gt;: movq $0x0,0x8(%rsp)0xffffffff8329acc7 &lt;kmem_cache_alloc+71&gt;: callq 0xffffffff83294dc0 &lt;slab_pre_alloc_hook.constprop.0&gt;0xffffffff8329accc &lt;kmem_cache_alloc+76&gt;: test %rax,%raxcrash&gt; 可以看到，此时kmem_cache_alloc+0 位置的instruction是 callq 0xffffffffc0227000，这是 x86平台 jmp优化之后的指令 关闭 CONFIG_FUNCTION_TRACER 时不开启 kprobe 的情况下，产生crash 1234567891011121314151617181920crash&gt; dis kmem_cache_alloc0xffffffffbb681430 &lt;kmem_cache_alloc&gt;: push %r150xffffffffbb681432 &lt;kmem_cache_alloc+2&gt;: mov %esi,%ecx0xffffffffbb681434 &lt;kmem_cache_alloc+4&gt;: mov $0x1,%edx0xffffffffbb681439 &lt;kmem_cache_alloc+9&gt;: push %r140xffffffffbb68143b &lt;kmem_cache_alloc+11&gt;: push %r130xffffffffbb68143d &lt;kmem_cache_alloc+13&gt;: mov %rdi,%r130xffffffffbb681440 &lt;kmem_cache_alloc+16&gt;: push %r120xffffffffbb681442 &lt;kmem_cache_alloc+18&gt;: push %rbp0xffffffffbb681443 &lt;kmem_cache_alloc+19&gt;: push %rbx0xffffffffbb681444 &lt;kmem_cache_alloc+20&gt;: sub $0x18,%rsp0xffffffffbb681448 &lt;kmem_cache_alloc+24&gt;: mov 0x1c(%rdi),%ebx0xffffffffbb68144b &lt;kmem_cache_alloc+27&gt;: mov %esi,0x4(%rsp)0xffffffffbb68144f &lt;kmem_cache_alloc+31&gt;: lea 0x8(%rsp),%rsi0xffffffffbb681454 &lt;kmem_cache_alloc+36&gt;: mov 0x48(%rsp),%r150xffffffffbb681459 &lt;kmem_cache_alloc+41&gt;: mov %gs:0x28,%rax0xffffffffbb681462 &lt;kmem_cache_alloc+50&gt;: mov %rax,0x10(%rsp)0xffffffffbb681467 &lt;kmem_cache_alloc+55&gt;: xor %eax,%eax0xffffffffbb681469 &lt;kmem_cache_alloc+57&gt;: movq $0x0,0x8(%rsp)crash&gt; 在开启对 kmem_cache_alloc 的 kprobe之后，再出发一次 crash,然后看 函数反汇编 1234567891011121314151617181920crash&gt; dis kmem_cache_alloc0xffffffff83a81430 &lt;kmem_cache_alloc&gt;: jmpq 0xffffffffc06080000xffffffff83a81435 &lt;kmem_cache_alloc+5&gt;: add %eax,(%rax)0xffffffff83a81437 &lt;kmem_cache_alloc+7&gt;: add %al,(%rax)0xffffffff83a81439 &lt;kmem_cache_alloc+9&gt;: push %r140xffffffff83a8143b &lt;kmem_cache_alloc+11&gt;: push %r130xffffffff83a8143d &lt;kmem_cache_alloc+13&gt;: mov %rdi,%r130xffffffff83a81440 &lt;kmem_cache_alloc+16&gt;: push %r120xffffffff83a81442 &lt;kmem_cache_alloc+18&gt;: push %rbp0xffffffff83a81443 &lt;kmem_cache_alloc+19&gt;: push %rbx0xffffffff83a81444 &lt;kmem_cache_alloc+20&gt;: sub $0x18,%rsp0xffffffff83a81448 &lt;kmem_cache_alloc+24&gt;: mov 0x1c(%rdi),%ebx0xffffffff83a8144b &lt;kmem_cache_alloc+27&gt;: mov %esi,0x4(%rsp)0xffffffff83a8144f &lt;kmem_cache_alloc+31&gt;: lea 0x8(%rsp),%rsi0xffffffff83a81454 &lt;kmem_cache_alloc+36&gt;: mov 0x48(%rsp),%r150xffffffff83a81459 &lt;kmem_cache_alloc+41&gt;: mov %gs:0x28,%rax0xffffffff83a81462 &lt;kmem_cache_alloc+50&gt;: mov %rax,0x10(%rsp)0xffffffff83a81467 &lt;kmem_cache_alloc+55&gt;: xor %eax,%eax0xffffffff83a81469 &lt;kmem_cache_alloc+57&gt;: movq $0x0,0x8(%rsp)crash&gt; 可以看到，此时kmem_cache_alloc+0 位置的instruction是 callq 0xffffffffc0227000，这是 x86平台 jmp优化之后的指令 代码分析debugfs 创建12345678910111213141516static int __init debugfs_kprobe_init(void){ struct dentry *dir; unsigned int value = 1; dir = debugfs_create_dir(&quot;kprobes&quot;, NULL); debugfs_create_file(&quot;list&quot;, 0400, dir, NULL, &amp;kprobes_fops); debugfs_create_file(&quot;enabled&quot;, 0600, dir, &amp;value, &amp;fops_kp); debugfs_create_file(&quot;blacklist&quot;, 0400, dir, NULL, &amp;kprobe_blacklist_fops); return 0;} 会在 /sys/kernel/debug/kprobes 创建几个文件 12root@rlk-Standard-PC-i440FX-PIIX-1996:/sys/kernel/debug/kprobes# lsblacklist enabled list register_kprobe123456789101112131415161718192021222324252627282930int register_kprobe(struct kprobe *p){ p-&gt;flags &amp;= KPROBE_FLAG_DISABLED; p-&gt;nmissed = 0; INIT_LIST_HEAD(&amp;p-&gt;list); old_p = get_kprobe(p-&gt;addr); if (old_p) { // 如果这个 addr 已经注册了 kprobe 了。 /* Since this may unoptimize old_p, locking text_mutex. */ ret = register_aggr_kprobe(old_p, p); goto out; } cpus_read_lock(); /* Prevent text modification */ mutex_lock(&amp;text_mutex); ret = prepare_kprobe(p); // 将 addr 地址处的指令 copy到 opcode，当做备份 mutex_unlock(&amp;text_mutex); cpus_read_unlock(); INIT_HLIST_NODE(&amp;p-&gt;hlist); // 将此 kprobe 添加到 kprobe_table 中 hlist_add_head_rcu(&amp;p-&gt;hlist, &amp;kprobe_table[hash_ptr(p-&gt;addr, KPROBE_HASH_BITS)]); // 如果系统已经将 所有 kprobe arm了，此时也需要顺便将此 kprobe arm if (!kprobes_all_disarmed &amp;&amp; !kprobe_disabled(p)) { ret = arm_kprobe(p); }} get_kprobe 可以通过地址addr，在 hlist kprobe_table 中确定 这个地址是否已经注册了 kprobe，如果已经注册，就需要 register_aggr_kprobe 注册 aggr 的kprobe。 12345678910111213struct kprobe *get_kprobe(void *addr){ struct hlist_head *head; struct kprobe *p; head = &amp;kprobe_table[hash_ptr(addr, KPROBE_HASH_BITS)]; hlist_for_each_entry_rcu(p, head, hlist, lockdep_is_held(&amp;kprobe_mutex)) { if (p-&gt;addr == addr) return p; } return NULL;} prepare_kprobe 会将 p 处指令 copy到 opcode 中，作为备份。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455static void __kprobes arch_prepare_ss_slot(struct kprobe *p){ /* prepare insn slot */ aarch64_insn_patch_text(addrs, insns, 2); // 替换指令 flush_icache_range((uintptr_t)addr, (uintptr_t)(addr + MAX_INSN_SIZE)); // 因为指令发生了改变，所以需要flush icahe /* * Needs restoring of return address after stepping xol. */ p-&gt;ainsn.api.restore = (unsigned long) p-&gt;addr + sizeof(kprobe_opcode_t);}int __kprobes arch_prepare_kprobe(struct kprobe *p){ unsigned long probe_addr = (unsigned long)p-&gt;addr; enum probe_insn insn; /* copy instruction */ p-&gt;opcode = le32_to_cpu(*p-&gt;addr); // 复制 kprobe 地址处的指令到 opcode中 /* decode instruction */ insn = arm_kprobe_decode_insn(p-&gt;addr, &amp;p-&gt;ainsn); // 解码 被替换的指令 pr_err(&quot;arch_prepare_kprobe 0x%p, insn: %d\\n&quot;, p-&gt;addr, insn); switch (insn) { case INSN_REJECTED: /* insn not supported */ return -EINVAL; case INSN_GOOD_NO_SLOT: /* insn need simulation */ p-&gt;ainsn.api.insn = NULL; break; case INSN_GOOD: /* instruction uses slot */ // 通过添加log，确定aarch64 中 nop 指令都是 INSN_GOOD 类型 p-&gt;ainsn.api.insn = get_insn_slot(); if (!p-&gt;ainsn.api.insn) return -ENOMEM; break; } /* prepare the instruction */ if (p-&gt;ainsn.api.insn) arch_prepare_ss_slot(p); // 替换掉 需要 probe的指令 else arch_prepare_simulate(p); return 0;}static inline int prepare_kprobe(struct kprobe *p){ return arch_prepare_kprobe(p);} objdump -d -l ./out/vmlinux 信息： 1234567891011121314151617ffff800010294e70 &lt;kmem_cache_alloc&gt;:kmem_cache_alloc():/home/ubuntu/workspace/arm64_linux/out/../mm/slub.c:5432ffff800010294e70: d503201f nopffff800010294e74: d503201f nop/home/ubuntu/workspace/arm64_linux/out/../mm/slub.c:3218ffff800010294e78: d503233f paciaspffff800010294e7c: a9b87bfd stp x29, x30, [sp, #-128]!ffff800010294e80: 910003fd mov x29, spffff800010294e84: a90363f7 stp x23, x24, [sp, #48]/home/ubuntu/workspace/arm64_linux/out/../mm/slub.c:3219ffff800010294e88: d00117f8 adrp x24, ffff800012592000 &lt;__boot_cpu_mode&gt;ffff800010294e8c: aa1e03f7 mov x23, x30ffff800010294e90: d50320ff xpaclri/home/ubuntu/workspace/arm64_linux/out/../mm/slub.c:3218ffff800010294e94: a90153f3 stp x19, x20, [sp, #16]ffff800010294e98: aa0003f3 mov x19, x0 这是添加的 log 输出 123[ 167.726444] register_kprobe 0x000000005af64ff2[ 167.726913] aarch64_insn_is_steppable 05[ 167.727012] aarch64_insn_is_steppable INSN_GOOD register_kprobe 中保存的指令1234567891011121314151617static void __kprobes arch_prepare_ss_slot(struct kprobe *p){ kprobe_opcode_t *addr = p-&gt;ainsn.api.insn; void *addrs[] = {addr, addr + 1}; u32 insns[] = {p-&gt;opcode, BRK64_OPCODE_KPROBES_SS}; // 在 slot 地址填充 opcode 地址 和 BRK64_OPCODE_KPROBES_SS 单步指令 /* prepare insn slot */ aarch64_insn_patch_text(addrs, insns, 2); flush_icache_range((uintptr_t)addr, (uintptr_t)(addr + MAX_INSN_SIZE)); /* * Needs restoring of return address after stepping xol. */ p-&gt;ainsn.api.restore = (unsigned long) p-&gt;addr + sizeof(kprobe_opcode_t);} 在 slot 地址填充 刚刚保存的opcode 和 自定义的BRK64_OPCODE_KPROBES_SS 单步指令 aarch64 使用 BRK 指令来 实现 kprobe, brk的 指令码是 AARCH64_BREAK_MON 0xd4200000 123456789101112131415161718192021222324252627282930313233/* * #imm16 values used for BRK instruction generation * 0x004: for installing kprobes * 0x005: for installing uprobes * 0x006: for kprobe software single-step * Allowed values for kgdb are 0x400 - 0x7ff * 0x100: for triggering a fault on purpose (reserved) * 0x400: for dynamic BRK instruction * 0x401: for compile time BRK instruction * 0x800: kernel-mode BUG() and WARN() traps * 0x9xx: tag-based KASAN trap (allowed values 0x900 - 0x9ff) */#define KPROBES_BRK_IMM 0x004#define UPROBES_BRK_IMM 0x005#define KPROBES_BRK_SS_IMM 0x006#define FAULT_BRK_IMM 0x100#define KGDB_DYN_DBG_BRK_IMM 0x400#define KGDB_COMPILED_DBG_BRK_IMM 0x401#define BUG_BRK_IMM 0x800#define KASAN_BRK_IMM 0x900#define KASAN_BRK_MASK 0x0ff/* * BRK instruction encoding * The #imm16 value should be placed at bits[20:5] within BRK ins */#define AARCH64_BREAK_MON 0xd4200000/* kprobes BRK opcodes with ESR encoding */#define BRK64_OPCODE_KPROBES (AARCH64_BREAK_MON | (KPROBES_BRK_IMM &lt;&lt; 5))#define BRK64_OPCODE_KPROBES_SS (AARCH64_BREAK_MON | (KPROBES_BRK_SS_IMM &lt;&lt; 5)) enable_kprobekprobe 在 register 之后，只是在 某个地址处保存了两条指令而已： opcode： 保存的是被 probe 地址处的 指令 KPROBES_BRK_SS_IMM：kprobe 单步指令kprobe 真正的work,还需要将它使能12345678910111213141516171819202122232425262728293031/* arm kprobe: install breakpoint in text */void __kprobes arch_arm_kprobe(struct kprobe *p){ void *addr = p-&gt;addr; u32 insn = BRK64_OPCODE_KPROBES; aarch64_insn_patch_text(&amp;addr, &amp;insn, 1); // 将 被probe地址处的指令 替换成 BRK64_OPCODE_KPROBES}#define __arm_kprobe(p) arch_arm_kprobe(p)/* Arm a kprobe with text_mutex */static int arm_kprobe(struct kprobe *kp){ __arm_kprobe(kp);}/* Enable one kprobe */int enable_kprobe(struct kprobe *kp){ int ret = 0; struct kprobe *p; mutex_lock(&amp;kprobe_mutex); if (!kprobes_all_disarmed &amp;&amp; kprobe_disabled(p)) { p-&gt;flags &amp;= ~KPROBE_FLAG_DISABLED; ret = arm_kprobe(p); } mutex_unlock(&amp;kprobe_mutex); return ret;} 可以看到 真正的使能只是将 被probe地址处的指令 替换成 BRK64_OPCODE_KPROBES所以一个地址被 kprobe 之后，这个地址附近指令执行顺序是： 1BRK64_OPCODE_KPROBES --&gt; opcode -&gt; BRK64_OPCODE_KPROBES_SS kprobe 的执行上面讲到 kprobe 的执行顺序：主要和 struct break_hook 有关： 123456789static struct break_hook kprobes_break_ss_hook = { .imm = KPROBES_BRK_SS_IMM, .fn = kprobe_breakpoint_ss_handler,};static struct break_hook kprobes_break_hook = { .imm = KPROBES_BRK_IMM, .fn = kprobe_breakpoint_handler,}; 在遇到 BRK 指令时 123456789101112131415161718static int call_break_hook(struct pt_regs *regs, unsigned int esr){ list_for_each_entry_rcu(hook, list, node) { unsigned int comment = esr &amp; ESR_ELx_BRK64_ISS_COMMENT_MASK; if ((comment &amp; ~hook-&gt;mask) == hook-&gt;imm) fn = hook-&gt;fn; } return fn ? fn(regs, esr) : DBG_HOOK_ERROR;}static int brk_handler(unsigned long unused, unsigned int esr, struct pt_regs *regs){ if (call_break_hook(regs, esr) == DBG_HOOK_HANDLED) return 0;} handler 初始化 12345void __init debug_traps_init(void){ hook_debug_fault_code(DBG_ESR_EVT_BRK, brk_handler, SIGTRAP, TRAP_BRKPT, &quot;BRK handler&quot;);} 可以看出 在执行 BRK64_OPCODE_KPROBES 时，(comment &amp; ~hook-&gt;mask) == KPROBES_BRK_IMM, 执行的函数是 kprobe_breakpoint_handler。 可以看出 在执行 BRK64_OPCODE_KPROBES_SS 时，(comment &amp; ~hook-&gt;mask) == KPROBES_BRK_SS_IMM, 执行的函数是 kprobe_breakpoint_ss_handler。 其中 kprobe_breakpoint_handler, 里面会执行 -&gt;pre_handler() 12345678910111213141516171819202122232425262728293031323334static void __kprobes kprobe_handler(struct pt_regs *regs){ struct kprobe *p, *cur_kprobe; struct kprobe_ctlblk *kcb; unsigned long addr = instruction_pointer(regs); kcb = get_kprobe_ctlblk(); cur_kprobe = kprobe_running(); p = get_kprobe((kprobe_opcode_t *) addr); if (p) { if (cur_kprobe) { if (reenter_kprobe(p, regs, kcb)) return; } else { /* Probe hit */ set_current_kprobe(p); kcb-&gt;kprobe_status = KPROBE_HIT_ACTIVE; if (!p-&gt;pre_handler || !p-&gt;pre_handler(p, regs)) { setup_singlestep(p, regs, kcb, 0); } else reset_current_kprobe(); } }}static int __kprobeskprobe_breakpoint_handler(struct pt_regs *regs, unsigned int esr){ kprobe_handler(regs); return DBG_HOOK_HANDLED;} 其中 kprobe_breakpoint_ss_handler, 里面会执行 -&gt;post_handler() 1234567891011121314151617181920212223242526272829303132333435363738static void __kprobespost_kprobe_handler(struct kprobe *cur, struct kprobe_ctlblk *kcb, struct pt_regs *regs){ /* return addr restore if non-branching insn */ if (cur-&gt;ainsn.api.restore != 0) instruction_pointer_set(regs, cur-&gt;ainsn.api.restore); /* restore back original saved kprobe variables and continue */ if (kcb-&gt;kprobe_status == KPROBE_REENTER) { restore_previous_kprobe(kcb); return; } /* call post handler */ kcb-&gt;kprobe_status = KPROBE_HIT_SSDONE; if (cur-&gt;post_handler) cur-&gt;post_handler(cur, regs, 0); reset_current_kprobe();}static int __kprobeskprobe_breakpoint_ss_handler(struct pt_regs *regs, unsigned int esr){ struct kprobe_ctlblk *kcb = get_kprobe_ctlblk(); unsigned long addr = instruction_pointer(regs); struct kprobe *cur = kprobe_running(); if (cur &amp;&amp; (kcb-&gt;kprobe_status &amp; (KPROBE_HIT_SS | KPROBE_REENTER)) &amp;&amp; ((unsigned long)&amp;cur-&gt;ainsn.api.insn[1] == addr)) { kprobes_restore_local_irqflag(kcb, regs); post_kprobe_handler(cur, kcb, regs); return DBG_HOOK_HANDLED; } /* not ours, kprobes should ignore it */ return DBG_HOOK_ERROR;} 到这里 aarch64 的 kprobe 执行过程就已经分析完成了，可以看到 kprobe 可以对内核任何地址进行 probe(除了blacklist)，并不需要 CONFIG_FUNCTION_TRACE，将函数第一个地址 编译成为 NOP。 disable_kprobe与 enable_kprobe 对应的是，disable_kprobe 只是将 将p-&gt;addr地址处的 BRK64_OPCODE_KPROBES 指令替换为 原来的 p-&gt;opcode 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253void __kprobes arch_disarm_kprobe(struct kprobe *p){ void *addr = p-&gt;addr; aarch64_insn_patch_text(&amp;addr, &amp;p-&gt;opcode, 1); // 将 被probe地址处的指令替换为 opcode}#define __disarm_kprobe(p, o) arch_disarm_kprobe(p)/* Disarm a kprobe with text_mutex */static int disarm_kprobe(struct kprobe *kp, bool reopt){ cpus_read_lock(); mutex_lock(&amp;text_mutex); __disarm_kprobe(kp, reopt); mutex_unlock(&amp;text_mutex); cpus_read_unlock(); return 0;}/* Disable one kprobe: Make sure called under kprobe_mutex is locked */static struct kprobe *__disable_kprobe(struct kprobe *p){ if (!kprobe_disabled(p)) { /* Disable probe if it is a child probe */ if (p != orig_p) p-&gt;flags |= KPROBE_FLAG_DISABLED; /* Try to disarm and disable this/parent probe */ if (p == orig_p || aggr_kprobe_disabled(orig_p)) { if (!kprobes_all_disarmed) { ret = disarm_kprobe(orig_p, true); } orig_p-&gt;flags |= KPROBE_FLAG_DISABLED; } } return orig_p;}/* Disable one kprobe */int disable_kprobe(struct kprobe *kp){ mutex_lock(&amp;kprobe_mutex); /* Disable this kprobe */ p = __disable_kprobe(kp); mutex_unlock(&amp;kprobe_mutex); return ret;}EXPORT_SYMBOL_GPL(disable_kprobe); unregister_kprobe与 register_kprobe 不同的是，只需要 free slot内存就可以了。 1234void unregister_kprobe(struct kprobe *p){ unregister_kprobes(&amp;p, 1);} kprobes使用最基本的方式就是写一个 kpobes 相关的 module，编译安装到 kernel里面，实现想要的功能，但是由于以下原因，一般使用中不常用 module的方式： kernel module 对于仅仅想使用的人来说难度较大 kernel module 有问题的话直接会导致 panic等严重问题 kernel module 编译安装，相比与其他方式资源消耗较大 除了直接使用 kernel module 的方式外，还可以使用 ftrace 框架，perf，bpftrace等集成好的工具框架。 通过kernel module 使用参考写的test case后面发现 kernel 的samples也实现了这个example。。编译安装之后，随便运行一些命令，dmesg看 12345678910111213141516[ 282.874169] Planted kprobe at 00000000ef62436b[ 282.899730] &lt;kernel_clone&gt; pre_handler: p-&gt;addr = 00000000ef62436b, ip = ffffffffafe69fa1, flags = 0x206[ 282.899732] &lt;kernel_clone&gt; post_handler: p-&gt;addr = 00000000ef62436b, flags = 0x206[ 288.039369] &lt;kernel_clone&gt; pre_handler: p-&gt;addr = 00000000ef62436b, ip = ffffffffafe69fa1, flags = 0x206[ 288.039373] &lt;kernel_clone&gt; post_handler: p-&gt;addr = 00000000ef62436b, flags = 0x206[ 289.401793] &lt;kernel_clone&gt; pre_handler: p-&gt;addr = 00000000ef62436b, ip = ffffffffafe69fa1, flags = 0x206[ 289.401794] &lt;kernel_clone&gt; post_handler: p-&gt;addr = 00000000ef62436b, flags = 0x206[ 294.421145] &lt;kernel_clone&gt; pre_handler: p-&gt;addr = 00000000ef62436b, ip = ffffffffafe69fa1, flags = 0x206[ 294.421148] &lt;kernel_clone&gt; post_handler: p-&gt;addr = 00000000ef62436b, flags = 0x206[ 294.428760] &lt;kernel_clone&gt; pre_handler: p-&gt;addr = 00000000ef62436b, ip = ffffffffafe69fa1, flags = 0x206[ 294.428764] &lt;kernel_clone&gt; post_handler: p-&gt;addr = 00000000ef62436b, flags = 0x206[ 297.028993] &lt;kernel_clone&gt; pre_handler: p-&gt;addr = 00000000ef62436b, ip = ffffffffafe69fa1, flags = 0x206[ 297.028998] &lt;kernel_clone&gt; post_handler: p-&gt;addr = 00000000ef62436b, flags = 0x206[ 299.595467] &lt;kernel_clone&gt; pre_handler: p-&gt;addr = 00000000ef62436b, ip = ffffffffafe69fa1, flags = 0x206[ 299.595471] &lt;kernel_clone&gt; post_handler: p-&gt;addr = 00000000ef62436b, flags = 0x206[ 1137.315517] kprobe at 00000000ef62436b unregistered 可以看到每次运行到 kernel_fork 位置，都会触发 pre_handler，结束也会触发post_handler。 与 trace 框架结合使用trace 框架中，与 kprobe相关的节点有如下节点 12345/sys/kernel/debug/tracing/kprobe_events-----------------------配置kprobe事件属性，增加事件之后会在kprobes下面生成对应目录。/sys/kernel/debug/tracing/kprobe_profile----------------------kprobe事件统计属性文件。/sys/kernel/debug/tracing/kprobes/&lt;GRP&gt;/&lt;EVENT&gt;/enabled-------使能kprobe事件/sys/kernel/debug/tracing/kprobes/&lt;GRP&gt;/&lt;EVENT&gt;/filter--------过滤kprobe事件/sys/kernel/debug/tracing/kprobes/&lt;GRP&gt;/&lt;EVENT&gt;/format--------查询kprobe事件显示格式 可以通过如下方式配置 kprobe 123p[:[GRP/]EVENT] [MOD:]SYM[+offs]|MEMADDR [FETCHARGS]-------------------设置一个probe探测点r[:[GRP/]EVENT] [MOD:]SYM[+0] [FETCHARGS]------------------------------设置一个return probe探测点-:[GRP/]EVENT----------------------------------------------------------删除一个探测点 examples: 添加 12echo 'p:myprobe do_sys_open dfd=%ax filename=%dx flags=%cx mode=+4($stack)' &gt; /sys/kernel/debug/tracing/kprobe_eventsecho 'r:myretprobe do_sys_open ret=$retval' &gt;&gt; /sys/kernel/debug/tracing/kprobe_events 开始 12echo 1 &gt; /sys/kernel/debug/tracing/events/kprobes/myprobe/enableecho 1 &gt; /sys/kernel/debug/tracing/events/kprobes/myretprobe/enable 结果 1234567891011121314151617181920212223242526272829root@rlk-Standard-PC-i440FX-PIIX-1996:/sys/kernel/debug/tracing# cat /sys/kernel/debug/tracing/trace# tracer: nop## entries-in-buffer/entries-written: 103/103 #P:4## _-----=&gt; irqs-off# / _----=&gt; need-resched# | / _---=&gt; hardirq/softirq# || / _--=&gt; preempt-depth# ||| / delay# TASK-PID CPU# |||| TIMESTAMP FUNCTION# | | | |||| | | irqbalance-344 [003] ...1 6185.081692: myprobe: (do_sys_open+0x0/0x80) dfd=0xffffffffb0069510 filename=0x8000 flags=0x0 mode=0xff irqbalance-344 [003] ...1 6185.082130: myprobe: (do_sys_open+0x0/0x80) dfd=0xffffffffb0069510 filename=0x8000 flags=0x0 mode=0xff bash-3542 [001] ...1 6186.142775: myprobe: (do_sys_open+0x0/0x80) dfd=0xffffffffb0069510 filename=0x8241 flags=0x1b6 mode=0f bash-3542 [001] ...1 6188.258576: myprobe: (do_sys_open+0x0/0x80) dfd=0xffffffffb0069510 filename=0x98800 flags=0x0 mode=0xf bash-3542 [001] ...1 6188.258652: myretprobe: (do_syscall_64+0x33/0x40 &lt;- do_sys_open) ret=0x3 cat-5421 [000] ...1 6188.662021: myprobe: (do_sys_open+0x0/0x80) dfd=0xffffffffb0069510 filename=0x88000 flags=0x0 mode=0xf cat-5421 [000] ...1 6188.662087: myretprobe: (do_syscall_64+0x33/0x40 &lt;- do_sys_open) ret=0x3 cat-5421 [000] ...1 6188.662117: myprobe: (do_sys_open+0x0/0x80) dfd=0xffffffffb0069510 filename=0x88000 flags=0x0 mode=0xf cat-5421 [000] ...1 6188.662145: myretprobe: (do_syscall_64+0x33/0x40 &lt;- do_sys_open) ret=0x3 cat-5421 [000] ...1 6188.662866: myprobe: (do_sys_open+0x0/0x80) dfd=0xffffffffb0069510 filename=0x88000 flags=0x0 mode=0xf cat-5421 [000] ...1 6188.662910: myretprobe: (do_syscall_64+0x33/0x40 &lt;- do_sys_open) ret=0x3 cat-5421 [000] ...1 6188.663082: myprobe: (do_sys_open+0x0/0x80) dfd=0xffffffffb0069510 filename=0x8000 flags=0x0 mode=0xff cat-5421 [000] ...1 6188.663108: myretprobe: (do_syscall_64+0x33/0x40 &lt;- do_sys_open) ret=0x3 ls-5422 [000] ...1 6192.453887: myprobe: (do_sys_open+0x0/0x80) dfd=0xffffffffb0069510 filename=0x88000 flags=0x0 mode=0xf ls-5422 [000] ...1 6192.453928: myretprobe: (do_syscall_64+0x33/0x40 &lt;- do_sys_open) ret=0x3 ls-5422 [000] ...1 6192.453951: myprobe: (do_sys_open+0x0/0x80) dfd=0xffffffffb0069510 filename=0x88000 flags=0x0 mode=0xf ls-5422 [000] ...1 6192.453972: myretprobe: (do_syscall_64+0x33/0x40 &lt;- do_sys_open) ret=0x3 关闭 12echo 0 &gt; /sys/kernel/debug/tracing/events/kprobes/myprobe/enableecho 0 &gt; /sys/kernel/debug/tracing/events/kprobes/myretprobe/enable 删除 12echo '-:myprobe' &gt;&gt; /sys/kernel/debug/tracing/kprobe_eventsecho '-:myretprobe' &gt;&gt; /sys/kernel/debug/tracing/kprobe_events 这个方式和 kernel module 的操作方法流程一样，也需要手动添加，开启，最后关闭整个流程最好写到bash 脚本中去执行比较好，但也比较麻烦 其实还可以在enable之前设置过滤选项 1echo 'filename==0x8241' &gt; /sys/kernel/debug/tracing/events/kprobes/myprobe/filter 与 perf 结合使用123456789101112131415161718192021222324252627282930313233343536root@ubuntu-Inspiron-5548:~# perf probe --add do_sys_openAdded new event: probe:do_sys_open (on do_sys_open)You can now use it in all perf tools, such as: perf record -e probe:do_sys_open -aR sleep 1root@ubuntu-Inspiron-5548:~# perf record -e probe:do_sys_open -aR sleep 5[ perf record: Woken up 1 times to write data ][ perf record: Captured and wrote 1.377 MB perf.data (21 samples) ]root@ubuntu-Inspiron-5548:~# perf script perf 75934 [003] 281578.918936: probe:do_sys_open: (ffffffff8a4fe540) sleep 75937 [000] 281578.919380: probe:do_sys_open: (ffffffff8a4fe540) sleep 75937 [000] 281578.919399: probe:do_sys_open: (ffffffff8a4fe540) sleep 75937 [000] 281578.919659: probe:do_sys_open: (ffffffff8a4fe540) chrome 44220 [003] 281581.327113: probe:do_sys_open: (ffffffff8a4fe540) thermald 815 [000] 281582.138797: probe:do_sys_open: (ffffffff8a4fe540) thermald 815 [000] 281582.138888: probe:do_sys_open: (ffffffff8a4fe540) thermald 815 [000] 281582.138926: probe:do_sys_open: (ffffffff8a4fe540) irqbalance 704 [000] 281583.566773: probe:do_sys_open: (ffffffff8a4fe540) irqbalance 704 [000] 281583.567130: probe:do_sys_open: (ffffffff8a4fe540) irqbalance 704 [000] 281583.567239: probe:do_sys_open: (ffffffff8a4fe540) irqbalance 704 [000] 281583.567274: probe:do_sys_open: (ffffffff8a4fe540) irqbalance 704 [000] 281583.567300: probe:do_sys_open: (ffffffff8a4fe540) irqbalance 704 [000] 281583.567324: probe:do_sys_open: (ffffffff8a4fe540) irqbalance 704 [000] 281583.567348: probe:do_sys_open: (ffffffff8a4fe540) irqbalance 704 [000] 281583.567372: probe:do_sys_open: (ffffffff8a4fe540) irqbalance 704 [000] 281583.567395: probe:do_sys_open: (ffffffff8a4fe540) irqbalance 704 [000] 281583.567418: probe:do_sys_open: (ffffffff8a4fe540) irqbalance 704 [000] 281583.567442: probe:do_sys_open: (ffffffff8a4fe540) irqbalance 704 [000] 281583.567465: probe:do_sys_open: (ffffffff8a4fe540) irqbalance 704 [000] 281583.567488: probe:do_sys_open: (ffffffff8a4fe540)root@ubuntu-Inspiron-5548:~# perf probe -d do_sys_openRemoved event: probe:do_sys_openroot@ubuntu-Inspiron-5548:~# 虽然不必要像 trace框架那样在各个目录间来回echo cat了，但是也还是需要手动去操作 add， record，script，d 的 perf 设置过滤选项 --filter 与 bpftrace 结合使用bpftrace 可以一行做到以上很多行才能做到的事情，且异常退出也不需要手动清楚 kprobe 点。 1234567root@ubuntu-Inspiron-5548:~# bpftrace -e 'kprobe:do_sys_open {printf(&quot;[%s]: do_sys_open\\n&quot;, comm)}'Attaching 1 probe...[chrome]: do_sys_open[thermald]: do_sys_open[thermald]: do_sys_open[thermald]: do_sys_open^C 更关键的是，他的参数管理更加方便 12345678root@ubuntu-Inspiron-5548:~# bpftrace -e 'kprobe:do_sys_open {printf(&quot;[pid-%d:%s]: do_sys_open %s\\n&quot;, pid,comm, str(arg1))}'Attaching 1 probe...[pid-42080:gsd-housekeepin]: do_sys_open /etc/fstab[pid-42080:gsd-housekeepin]: do_sys_open /proc/self/mountinfo[pid-42080:gsd-housekeepin]: do_sys_open /run/mount/utab[pid-42080:gsd-housekeepin]: do_sys_open /proc/self/mountinfo[pid-42080:gsd-housekeepin]: do_sys_open /run/mount/utab^C bpftrace 可以通过 / xxx / 来设置过滤选项。 参考Linux kprobe调试技术使用","link":"/2021/01/29/%E5%86%85%E6%A0%B8%E8%A7%82%E6%B5%8B/%E6%80%A7%E8%83%BD%E7%A8%B3%E5%AE%9A%E6%80%A7/kprobes%20%E5%8E%9F%E7%90%86%E4%B8%8E%E4%BD%BF%E7%94%A8/"},{"title":"perf基本使用","text":"arm64 版本perf编译 123cd toolsmake ARCH=arm64 CROSS_COMPILE=/usr/bin/aarch64-linux-gnu- perf LDFLAGS+=--staticls ./perf/ -al | grep perf perf功能可以追踪 tracepoint syscall kprobes perf支持的 events知道系统支持那些events，对遇到问题之后怎么分析十分有帮助。 12perf list [--no-desc] [--long-desc] [hw|sw|cache|tracepoint|pmu|sdt|metric|metricgroup|event_glob] 看看支持的 tracepoint 123456789Inspiron-5548@ubuntu: ~/workspace# sudo perf list tracepoint | wc -l2591Inspiron-5548@ubuntu: ~/workspace# sudo perf list tracepoint | grep &quot;sched:&quot; sched:sched_kthread_stop [Tracepoint event] sched:sched_kthread_stop_ret [Tracepoint event] sched:sched_migrate_task [Tracepoint event] sched:sched_move_numa [Tracepoint event] sched:sched_pi_setprio [Tracepoint event] sched:sched_process_exec [Tracepoint event] 看看支持的 hw 12345678910Inspiron-5548@ubuntu: ~/workspace# sudo perf list hwList of pre-defined events (to be used in -e): branch-instructions OR branches [Hardware event] branch-misses [Hardware event] bus-cycles [Hardware event] cache-misses [Hardware event] cache-references [Hardware event] cpu-cycles OR cycles [Hardware event] instructions [Hardware event] ref-cycles [Hardware event] perf支持的选项-e 事件选择器-p 只跟踪制定线程-P 采样周期，就是一共采样多久？-F 频率，就是1s之内记录多少次-g call_graph，打开调用栈追踪，火焰图就是利用的此特性-a 从所有cpu搜集数据-D 频率，就是1s之内记录多少次-G 只采用 cgroup name内的信息-C 指定CPU的数据 123456789101112131415161718192021root@ubuntu-Inspiron-5548:/home/ubuntu/workspace# perf record -e&quot;sched:*&quot;^C[ perf record: Woken up 0 times to write data ][ perf record: Captured and wrote 1.503 MB perf.data (41 samples) ]root@ubuntu-Inspiron-5548:/home/ubuntu/workspace# perf script perf 61734 [000] 203358.640055: sched:sched_stat_runtime: comm=perf pid=61734 runtime=168226 [ns] vruntime=27117216697&gt; perf 61734 [000] 203358.640059: sched:sched_waking: comm=migration/0 pid=12 prio=0 target_cpu=000 perf 61734 [000] 203358.640060: sched:sched_wakeup: comm=migration/0 pid=12 prio=0 target_cpu=000 perf 61734 [000] 203358.640061: sched:sched_stat_runtime: comm=perf pid=61734 runtime=6351 [ns] vruntime=271172173327 &gt; perf 61734 [000] 203358.640063: sched:sched_switch: prev_comm=perf prev_pid=61734 prev_prio=120 prev_state=R+ ==&gt; migration/0 12 [000] 203358.640066: sched:sched_migrate_task: comm=perf pid=61734 prio=120 orig_cpu=0 dest_cpu=1 migration/0 12 [000] 203358.640069: sched:sched_wake_idle_without_ipi: cpu=1root@ubuntu-Inspiron-5548:/home/ubuntu/workspace# perf reportAvailable samples0 sched:sched_kthread_stop ◆0 sched:sched_kthread_stop_ret ▒5 sched:sched_waking ▒5 sched:sched_wakeup ▒0 sched:sched_wakeup_new ▒13 sched:sched_switch ▒4 sched:sched_migrate_task ▒0 sched:sched_process_free perf sched的支持可以用 sudo perf sched 看看perf sched的支持行为 123456Inspiron-5548@1ubuntu: ~/workspace# sudo perf sched Usage: perf sched [&lt;options&gt;] {record|latency|map|replay|script|timehist} -D, --dump-raw-trace dump raw trace in ASCII -f, --force don't complain, do it -i, --input &lt;file&gt; input file name -v, --verbose be more verbose (show symbol address, etc) 首先需要 perf sched record 12345Inspiron-5548@141ubuntu: ~/workspace# sudo perf sched record^C[ perf record: Woken up 1 times to write data ][ perf record: Captured and wrote 1.924 MB perf.data (4820 samples) ]Inspiron-5548@130ubuntu: ~/workspace# 后续可以使用 latency map replay script timehist 去分析 调度延迟如果想分析调度延迟，可以制定按照进程还是线程来分析，也可以按照 sudo perf sched latency --sort max 找到最大调度延迟的task. 123456Inspiron-5548@ubuntu: ~/workspace# sudo perf sched latency --help Usage: perf sched latency [&lt;options&gt;] -C, --CPU &lt;n&gt; CPU to profile on -p, --pids latency stats per pid instead of per comm -s, --sort &lt;key[,key2...]&gt; sort by key(s): runtime, switch, avg, max 123456789101112Inspiron-5548@129ubuntu: ~/workspace# sudo perf sched latency --sort max ----------------------------------------------------------------------------------------------------------------- Task | Runtime ms | Switches | Average delay ms | Maximum delay ms | Maximum delay at | ----------------------------------------------------------------------------------------------------------------- ibus-extension-:44142 | 0.053 ms | 1 | avg: 0.788 ms | max: 0.788 ms | max at: 189462.300642 s ibus-x11:44144 | 0.054 ms | 1 | avg: 0.743 ms | max: 0.743 ms | max at: 189462.300589 s kworker/3:1-eve:48187 | 2.245 ms | 107 | avg: 0.024 ms | max: 0.666 ms | max at: 189459.728139 s chrome:(4) | 50.852 ms | 127 | avg: 0.021 ms | max: 0.645 ms | max at: 189462.300483 s JS Helper:(4) | 2.367 ms | 25 | avg: 0.046 ms | max: 0.626 ms | max at: 189462.300654 s gsd-wacom:42110 | 0.063 ms | 1 | avg: 0.481 ms | max: 0.481 ms | max at: 189462.300409 s kworker/0:1-eve:48272 | 5.959 ms | 95 | avg: 0.024 ms | max: 0.456 ms | max at: 189462.343798 s 12345678Inspiron-5548@130ubuntu: ~/workspace# sudo perf sched latency --sort switch ----------------------------------------------------------------------------------------------------------------- Task | Runtime ms | Switches | Average delay ms | Maximum delay ms | Maximum delay at | ----------------------------------------------------------------------------------------------------------------- kworker/1:1-eve:44912 | 5.260 ms | 166 | avg: 0.019 ms | max: 0.141 ms | max at: 189461.969262 s chrome:(4) | 50.852 ms | 127 | avg: 0.021 ms | max: 0.645 ms | max at: 189462.300483 s kworker/3:1-eve:48187 | 2.245 ms | 107 | avg: 0.024 ms | max: 0.666 ms | max at: 189459.728139 s kworker/0:1-eve:48272 | 5.959 ms | 95 | avg: 0.024 ms | max: 0.456 ms | max at: 189462.343798 s 调度行为详细分析可以使用 sudo perf sched script 具体分析 线程何时 wakeup，何时 switch 等信息 12345678910111213141516171819Inspiron-5548@141ubuntu: ~/workspace# sudo perf sched script perf 50314 [000] 189458.979541: sched:sched_stat_runtime: comm=perf pid=50314 runtime=41043 [ns] vruntime=46286470740 [ns] perf 50314 [000] 189458.979545: sched:sched_wakeup: comm=migration/0 pid=12 prio=0 target_cpu=000 perf 50314 [000] 189458.979546: sched:sched_stat_runtime: comm=perf pid=50314 runtime=4675 [ns] vruntime=46286475415 [ns] perf 50314 [000] 189458.979547: sched:sched_switch: prev_comm=perf prev_pid=50314 prev_prio=120 prev_state=R+ ==&gt; next_co&gt; migration/0 12 [000] 189458.979551: sched:sched_migrate_task: comm=perf pid=50314 prio=120 orig_cpu=0 dest_cpu=1 migration/0 12 [000] 189458.979570: sched:sched_switch: prev_comm=migration/0 prev_pid=12 prev_prio=0 prev_state=S ==&gt; next_c&gt; swapper 0 [001] 189459.531154: sched:sched_switch: prev_comm=swapper/1 prev_pid=0 prev_prio=120 prev_state=R ==&gt; next_co&gt; chrome 44220 [001] 189459.531182: sched:sched_stat_runtime: comm=chrome pid=44220 runtime=33768 [ns] vruntime=358082171091 [ns] chrome 44220 [001] 189459.531184: sched:sched_switch: prev_comm=chrome prev_pid=44220 prev_prio=120 prev_state=S ==&gt; next_c&gt; Xorg 41740 [002] 189459.531198: sched:sched_wakeup: comm=chrome pid=44220 prio=120 target_cpu=001 swapper 0 [001] 189459.531202: sched:sched_switch: prev_comm=swapper/1 prev_pid=0 prev_prio=120 prev_state=R ==&gt; next_co&gt; Xorg 41740 [002] 189459.531231: sched:sched_stat_runtime: comm=Xorg pid=41740 runtime=566128 [ns] vruntime=22634838876 [ns] Xorg 41740 [002] 189459.531234: sched:sched_switch: prev_comm=Xorg prev_pid=41740 prev_prio=120 prev_state=S ==&gt; next_com&gt; chrome 44220 [001] 189459.531296: sched:sched_stat_runtime: comm=chrome pid=44220 runtime=98522 [ns] vruntime=358082269613 [ns] chrome 44220 [001] 189459.531299: sched:sched_switch: prev_comm=chrome prev_pid=44220 prev_prio=120 prev_state=S ==&gt; next_c&gt; swapper 0 [001] 189459.537012: sched:sched_wakeup: comm=kworker/1:1 pid=44912 prio=120 target_cpu=001 swapper 0 [001] 189459.537019: sched:sched_switch: prev_comm=swapper/1 prev_pid=0 prev_prio=120 prev_state=R ==&gt; next_co&gt; swapper 0 [000] 189459.537022: sched:sched_wakeup: comm=kworker/0:1 pid=48272 prio=120 target_cpu=000 调度延迟可能原因 唤醒线程的 target cpu 上正在运行的线程的 vruntime与 唤醒线程的 vruntime 差值较小，导致调度器，认为不能切换。 唤醒线程的 target cpu 上当前正在运行的线程已运行的时间还小于最小运行时间（sched_min_granularity_ns），为防止切换太频繁，所以调度器认为u不能切换线程； 唤醒抢占的feature关闭，即sched_features中WAKEUP_PREEMPT特性关闭，不允许唤醒抢占 唤醒线程的 target cpu 被hard irq占用较多，导致无法进行 sched_switch 唤醒线程的 target cpu 被 softirq占用较多（net soft_irq）。 唤醒线程的 target cpu， cpu上唤醒的线程实在太多，唤醒的线程在 rbtree上不是最左边的线程。 非抢占内核上， 唤醒线程的 target cpu 上正在运行的线程一直处于内核态(有bug)，导致一直未退出到用户空间，所以一直不能切换？ perf 对动态追踪的支持上面的sudo perf list tracepoint sudo perf sched record 都是基于 static tracepoint的，假如我想追踪一个函数，但是没有相关的 static tracepoint点，应该如何做呢？ perf 提供了 sudo perf probe 的方法 动态添加删除 probe点sudo perf probe --add 'xxx' 添加sudo perf probe -d 'xxx' 删除sudo perf probe --list 展示sudo perf record -e probe:xxx -aR sleep 5 采样这个 probe 5s.sudo perf record -e probe:xxx -aR slep5 --filter 'yy' 采样这个 probe 5s，且满足yyy条件 1234567891011121314151617Inspiron-5548@ubuntu: ~/workspace# sudo perf probe --add tcp_sendmsgAdded new event: probe:tcp_sendmsg (on tcp_sendmsg)You can now use it in all perf tools, such as: perf record -e probe:tcp_sendmsg -aR sleep 1Inspiron-5548@ubuntu: ~/workspace# sudo perf probe --list probe:tcp_sendmsg (on tcp_sendmsg)Inspiron-5548@ubuntu: ~/workspace# sudo perf probe -d tcp_sendmsgRemoved event: probe:tcp_sendmsgInspiron-5548@ubuntu: ~/workspace# sudo perf probe --listInspiron-5548@ubuntu: ~/workspace# add添加的时候也可以加上表达式，然后在record，在最后script 解析时，会自动打印 1234567891011121314151617181920212223Inspiron-5548@130ubuntu: ~/workspace# sudo perf probe --add 'tcp_sendmsg bytes=%cx'Added new event: probe:tcp_sendmsg (on tcp_sendmsg with bytes=%cx)You can now use it in all perf tools, such as: perf record -e probe:tcp_sendmsg -aR sleep 1Inspiron-5548@ubuntu: ~/workspace# sudo perf record -e probe:tcp_sendmsg -aR sleep 5[ perf record: Woken up 1 times to write data ][ perf record: Captured and wrote 1.396 MB perf.data (10 samples) ]Inspiron-5548@ubuntu: ~/workspace# sudo perf script ssh 59966 [001] 199745.500983: probe:tcp_sendmsg: (ffffffff8ac617a0) bytes=0xffff970b347ee268 code 50459 [000] 199745.501228: probe:tcp_sendmsg: (ffffffff8ac617a0) bytes=0xffff970b346ae910 ssh 59966 [001] 199746.651139: probe:tcp_sendmsg: (ffffffff8ac617a0) bytes=0xffff970b347ee268 code 50459 [000] 199746.651380: probe:tcp_sendmsg: (ffffffff8ac617a0) bytes=0xffff970b346ae910 code 50432 [003] 199747.361044: probe:tcp_sendmsg: (ffffffff8ac617a0) bytes=0xffff970b346ae600 code 50459 [000] 199747.361322: probe:tcp_sendmsg: (ffffffff8ac617a0) bytes=0xffff970b346ae910 ssh 59966 [001] 199747.361489: probe:tcp_sendmsg: (ffffffff8ac617a0) bytes=0xffff970b347ee268 code 50432 [000] 199748.724615: probe:tcp_sendmsg: (ffffffff8ac617a0) bytes=0xffff970b346ae600 code 50459 [001] 199748.724872: probe:tcp_sendmsg: (ffffffff8ac617a0) bytes=0xffff970b346ae910 ssh 59966 [002] 199748.725028: probe:tcp_sendmsg: (ffffffff8ac617a0) bytes=0xffff970b347ee268Inspiron-5548@ubuntu: ~/workspace# 关于%ax %bx %cx解释 123456789101112131415PROBE ARGUMENT Each probe argument follows below syntax. [NAME=]LOCALVAR|$retval|%REG|@SYMBOL[:TYPE][@user] NAME specifies the name of this argument (optional). You can use the name of local variable, local data structure member (e.g. var→field, var.field2), local array with fixed index (e.g. array[1], var→array[0], var→pointer[2]), or kprobe-tracer argument format (e.g. $retval, %ax, etc). Note that the name of this argument will be set as the last member name if you specify a local data structure member (e.g. field2 for var→field1.field2.) $vars and $params special arguments are also available for NAME, $vars is expanded to the local variables (including function parameters) which can access at given probe point. $params is expanded to only the function parameters. TYPE casts the type of this argument (optional). If omitted, perf probe automatically set the type based on debuginfo (*). Currently, basic types (u8/u16/u32/u64/s8/s16/s32/s64), hexadecimal integers (x/x8/x16/x32/x64), signedness casting (u/s), &quot;string&quot; and bitfield are supported. (see TYPES for detail) On x86 systems %REG is always the short form of the register: for example %AX. %RAX or %EAX is not valid. &quot;@user&quot; is a special attribute which means the LOCALVAR will be treated as a user-space memory. This is only valid for kprobe event. perf ftrace 的使用man 手册 12345678910111213141516171819202122232425perf ftrace --helpPERF-FTRACE(1) perf Manual PERF-FTRACE(1)NAME perf-ftrace - simple wrapper for kernel's ftrace functionalitySYNOPSIS perf ftrace &lt;command&gt;DESCRIPTION The perf ftrace command is a simple wrapper of kernel’s ftrace functionality. It only supports single thread tracing currently and just reads trace_pipe in text and then write it to stdout. The following options apply to perf ftrace.OPTIONS -t, --tracer= Tracer to use when neither -G nor -F option is not specified: function_graph or function. -v, --verbose= Verbosity level. -F, --funcs List available functions to trace. It accepts a pattern to only list interested functions. 查看有哪些函数可以用于跟踪 12345ubuntu@zeku_server:~/workspace/share/test_modules $ sudo perf ftrace -F | grep vfs_readvfs_readvfs_readvvfs_readlinkubuntu@zeku_server:~/workspace/share/test_modules $ 用于跟踪 graph-ftrace 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647ubuntu@zeku_server:~/workspace/share/test_modules $ sudo perf ftrace -a -G vfs_read# tracer: function_graph## CPU DURATION FUNCTION CALLS# | | | | | | | 3) | vfs_read() { 3) | rw_verify_area() { 3) | security_file_permission() { 3) | apparmor_file_permission() { 3) | aa_file_perm() { 3) 0.102 us | rcu_read_unlock_strict(); 3) 0.321 us | } 3) 0.508 us | } 3) 0.095 us | __fsnotify_parent(); 3) 0.900 us | } 3) 1.130 us | } 3) | new_sync_read() { 3) | ext4_file_read_iter() { 3) | generic_file_read_iter() { 3) | generic_file_buffered_read() { 3) | _cond_resched() { 3) 0.086 us | rcu_all_qs(); 3) 0.255 us | } 3) | generic_file_buffered_read_get_pages() { 3) | find_get_pages_contig() { 3) 0.088 us | PageHuge(); 3) 0.084 us | rcu_read_unlock_strict(); 3) 0.566 us | } 3) 0.747 us | } 3) 0.191 us | mark_page_accessed(); 3) | _cond_resched() { 3) 0.214 us | rcu_all_qs(); 3) 0.646 us | } 3) | touch_atime() { 3) | atime_needs_update() { 3) | current_time() { 3) 0.084 us | ktime_get_coarse_real_ts64(); 3) 0.266 us | } 3) 0.435 us | } 3) 0.607 us | } 3) 3.294 us | } 3) 3.486 us | } 3) 3.659 us | } 3) 3.873 us | } 3) 0.088 us | __fsnotify_parent(); 3) 5.884 us | } 3) | vfs_read() { perf lock 的使用为了给系统中造成一定压力，先 起10个 vm thread 分配内存 12stable_kernel@kernel: ~/workspace/fs# stress --vm 10stress: info: [4746] dispatching hogs: 0 cpu, 0 io, 10 vm, 0 hdd 12345678910111213141516171819202122232425262728293031stable_kernel@kernel: ~/workspace/linux/tools/perf# sudo ./perf lock record^C[ perf record: Woken up 0 times to write data ]Warning:Processed 301597 events and lost 4 chunks!Check IO/CPU overload![ perf record: Captured and wrote 30.069 MB perf.data (295337 samples) ]stable_kernel@130kernel: ~/workspace/linux/tools/perf# sudo ./perf lock report Name acquired contended avg wait (ns) total wait (ns) max wait (ns) min wait (ns) rcu_read_lock 884617 0 0 0 0 0 &amp;mm-&gt;mmap_lock 9652 0 0 0 0 0 &amp;mm-&gt;mmap_lock 8229 0 0 0 0 0 &amp;mm-&gt;mmap_lock 7863 0 0 0 0 0 &amp;mm-&gt;mmap_lock 7827 0 0 0 0 0 &amp;mm-&gt;mmap_lock 7782 0 0 0 0 0 &amp;mm-&gt;mmap_lock 7549 0 0 0 0 0 &amp;mm-&gt;mmap_lock 7364 0 0 0 0 0 &amp;mm-&gt;mmap_lock 6710 0 0 0 0 0 &amp;mm-&gt;mmap_lock 6676 0 0 0 0 0 &amp;xa-&gt;xa_lock 5485 171 1075 183872 4457 705 &amp;mm-&gt;mmap_lock 5482 0 0 0 0 0 hrtimer_bases.lo... 1160 0 0 0 0 0 hrtimer_bases.lo... 1146 0 0 0 0 0 hrtimer_bases.lo... 1119 0 0 0 0 0 hrtimer_bases.lo... 1118 0 0 0 0 0 &amp;zone-&gt;lock 975 6 2009 12054 3671 805 ptlock_ptr(page)... 966 0 0 0 0 0 ptlock_ptr(page)... 933 0 0 0 0 0 可以看到这种 memory 压力比较大的情况下，xa-&gt;xa_lock 与 zone-&gt;lock 的争用情况比较激烈 Name：内核锁的名字。 aquired：该锁被直接获得的次数，因为没有其它内核路径占用该锁，此时不用等待。 contended：该锁等待后获得的次数，此时被其它内核路径占用，需要等待。 total wait：为了获得该锁，总共的等待时间。 max wait：为了获得该锁，最大的等待时间。 min wait：为了获得该锁，最小的等待时间 perf 的 overhead为了对比明显，这里加上strace 对比，由于strace 会使用ptrace（2）附加到目标进程，并在系统调用期间将其停止，例如调试器。 这很猛烈，并可能导致严重的开销。为了证明这一点，下面的系统调用繁重程序是通过perf和strace本身运行的。 123456789101112root@ubuntu-Inspiron-5548:/home/ubuntu/workspace# dd if=/dev/zero of=/dev/null bs=512 count=10000k记录了10240000+0 的读入记录了10240000+0 的写出5242880000 bytes (5.2 GB, 4.9 GiB) copied, 14.3584 s, 365 MB/sroot@ubuntu-Inspiron-5548:/home/ubuntu/workspace# perf stat -e 'syscalls:sys_enter_*' dd if=/dev/zero of=/dev/null bs=512 count=10000k记录了10240000+0 的读入记录了10240000+0 的写出5242880000 bytes (5.2 GB, 4.9 GiB) copied, 15.4485 s, 339 MB/sroot@ubuntu-Inspiron-5548:/home/ubuntu/workspace# strace -c dd if=/dev/zero of=/dev/null bs=512 count=10000k 记录了10240000+0 的读入记录了10240000+0 的写出5242880000 bytes (5.2 GB, 4.9 GiB) copied, 504.601 s, 10.4 MB/s perf 慢了1MB/s大概7%，但是strace慢了36倍。。与 strace 这类会停止线程的debugger方式比， perf的 overhead 就显得微不足道了。 参考博客参考[perf example]](http://www.brendangregg.com/perf.html)","link":"/2021/01/28/%E5%86%85%E6%A0%B8%E8%A7%82%E6%B5%8B/perf%E7%9B%B8%E5%85%B3/perf%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/"},{"title":"node zone 相关问题","text":"NUMA概念在NUMA出现之前，CPU朝着高频率的方向发展遇到了天花板，转而向着多核心的方向发展。 在一开始，内存控制器还在北桥中，所有CPU对内存的访问都要通过北桥来完成。此时所有CPU访问内存都是“一致的”，如下图所示： NUMA 全称 Non-Uniform Memory Access，译为“非一致性内存访问”。这种构架下，不同的内存器件和CPU核心从属不同的 Node，每个 Node 都有自己的集成内存控制器（IMC，Integrated Memory Controller）。 在 Node 内部，架构类似SMP，使用 IMC Bus 进行不同核心间的通信；不同的 Node 间通过QPI（Quick Path Interconnect）进行通信，如下图所示：一般来说，一个内存插槽对应一个 Node。需要注意的一个特点是，QPI的延迟要高于IMC Bus，也就是说CPU访问内存有了远近（remote/local）之别，而且实验分析来看，这个差别非常明显。 NUMA概念 是想对于CPU来说的，系统中主存不同node对于不同CPU来说访问速度不一样，就应该可以理解成为NUMA。 nodenode 就是 一个节点，不同节点对于一个CPU来讲访问延迟 是不一样的。在linux上用 pg_data_t 来表示，同时 pg_data_t 也包含了struct lruvec lru链表。同时 kswapd kcompactd 线程也都是基于 pg_data_t的。 zonezone 是基于 node的，一般分为如下 123456Inspiron-5548@ubuntu: ~/workspace/linux-stable# cat /proc/zoneinfo | grep &quot;Node 0&quot;Node 0, zone DMANode 0, zone DMA32Node 0, zone NormalNode 0, zone MovableNode 0, zone Device 其中 _watermark struct per_cpu_pageset struct free_area 都是基于 zone的。 虽然 kcompactd是基于node的，但是 compaction 行为是基于zone的。 12345678910111213141516171819202122232425262728293031struct zone { /* zone watermarks, access with *_wmark_pages(zone) macros */ unsigned long _watermark[NR_WMARK]; unsigned long watermark_boost; ......}struct zoneref { struct zone *zone; /* Pointer to actual zone */ int zone_idx; /* zone_idx(zoneref-&gt;zone) */};struct zonelist { struct zoneref _zonerefs[MAX_ZONES_PER_ZONELIST + 1];};typedef struct pglist_data { /* * node_zones contains just the zones for THIS node. Not all of the * zones may be populated, but it is the full list. It is referenced by * this node's node_zonelists as well as other node's node_zonelists. */ struct zone node_zones[MAX_NR_ZONES]; /* * node_zonelists contains references to all zones in all nodes. * Generally the first zones will be references to this node's * node_zones. */ struct zonelist node_zonelists[MAX_ZONELISTS];} node是如何建立起来的？arch/x86/mm/numa.c中定义了 指针数组，可以系统bring up的时候初始化，也可以是 memory_hotplug时候初始化。 1struct pglist_data *node_data[MAX_NUMNODES] __read_mostly; 如何查找 不同的node节点？根据nid查找 1#define NODE_DATA(nid) (node_data[nid]) 根据zone查找node节点 1234567struct zone { ......#ifdef CONFIG_NUMA int node;#endif ......} 根据page查找node节点，在 include/linux/mm.h 12345678910111213#define NODES_WIDTH NODES_SHIFT#define NODES_MASK ((1UL &lt;&lt; NODES_WIDTH) - 1)static inline void set_page_node(struct page *page, unsigned long node) { page-&gt;flags &amp;= ~(NODES_MASK &lt;&lt; NODES_PGSHIFT); page-&gt;flags |= (node &amp; NODES_MASK) &lt;&lt; NODES_PGSHIFT;}static inline int page_to_nid(const struct page *page) { struct page *p = (struct page *)page; return (PF_POISONED_CHECK(p)-&gt;flags &gt;&gt; NODES_PGSHIFT) &amp; NODES_MASK;} zone是如何建立起来的？start_kernel中 12345asmlinkage __visible void __init __no_sanitize_address start_kernel(void){ build_all_zonelists(NULL); ......} 在 mm/memory_hotplug.c 的 hotadd_new_pgdat函数中 123456/* we are OK calling __meminit stuff here - we have CONFIG_MEMORY_HOTPLUG */static pg_data_t __ref *hotadd_new_pgdat(int nid){ build_all_zonelists(pgdat); ......} 在 build_all_zonelists 中 根据是否处于 SYSTEM_BOOTING 进行初始化，最终到__build_all_zonelists，如果是 memory hot plug 只需要进行当前self pgdata的初始化，如果是 SYSTEM_BOOTING 需要遍历所有在线的 node节点，都需要初始化。 1234567891011121314151617181920212223242526272829303132333435363738394041static void build_zonelists(pg_data_t *pgdat){ ...... while ((node = find_next_best_node(local_node, &amp;used_mask)) &gt;= 0) { if (node_distance(local_node, node) != node_distance(local_node, prev_node)) node_load[node] = load; node_order[nr_nodes++] = node; prev_node = node; load--; } build_zonelists_in_node_order(pgdat, node_order, nr_nodes); build_thisnode_zonelists(pgdat);}static void __build_all_zonelists(void *data){ /* * This node is hotadded and no memory is yet present. So just * building zonelists is fine - no need to touch other nodes. */ if (self &amp;&amp; !node_online(self-&gt;node_id)) { // hotplug build_zonelists(self); } else { for_each_online_node(nid) { //system init build_zonelists(NODE_DATA(nid)); } } .......}void __ref build_all_zonelists(pg_data_t *pgdat){ if (system_state == SYSTEM_BOOTING) { build_all_zonelists_init(); } else { __build_all_zonelists(pgdat); }} 这里又涉及到 pgdat-&gt;node_zonelists[ZONELIST_FALLBACK] 这个 fallback 机制的问题了。 1234567891011enum { ZONELIST_FALLBACK, /* zonelist with fallback */#ifdef CONFIG_NUMA /* * The NUMA zonelists are doubled because we need zonelists that * restrict the allocations to a single node for __GFP_THISNODE. */ ZONELIST_NOFALLBACK, /* zonelist without fallback (__GFP_THISNODE) */#endif MAX_ZONELISTS}; pgdat-&gt;node_zonelists[]包含了2个zonelist，一个是由本node的zones组成，另一个是由从本node分配不到内存时可选的备用zones组成，相当于是选择了一个退路，所以叫fallback。 具体初始化是由 build_zonelists_in_node_order 与 build_thisnode_zonelists完成的 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566static void zoneref_set_zone(struct zone *zone, struct zoneref *zoneref){ zoneref-&gt;zone = zone; zoneref-&gt;zone_idx = zone_idx(zone);}//可以看到赋值顺序是：// zonerefs[nr_zones++]// zone_type--;static int build_zonerefs_node(pg_data_t *pgdat, struct zoneref *zonerefs){ struct zone *zone; enum zone_type zone_type = MAX_NR_ZONES; int nr_zones = 0; do { zone_type--; zone = pgdat-&gt;node_zones + zone_type; if (managed_zone(zone)) { zoneref_set_zone(zone, &amp;zonerefs[nr_zones++]); check_highest_zone(zone_type); } } while (zone_type); return nr_zones;}/* * Build zonelists ordered by node and zones within node. * This results in maximum locality--normal zone overflows into local * DMA zone, if any--but risks exhausting DMA zone. */static void build_zonelists_in_node_order(pg_data_t *pgdat, int *node_order, unsigned nr_nodes){ struct zoneref *zonerefs; int i; zonerefs = pgdat-&gt;node_zonelists[ZONELIST_FALLBACK]._zonerefs; for (i = 0; i &lt; nr_nodes; i++) { int nr_zones; pg_data_t *node = NODE_DATA(node_order[i]); nr_zones = build_zonerefs_node(node, zonerefs); zonerefs += nr_zones; } zonerefs-&gt;zone = NULL; zonerefs-&gt;zone_idx = 0;}/* * Build gfp_thisnode zonelists */static void build_thisnode_zonelists(pg_data_t *pgdat){ struct zoneref *zonerefs; int nr_zones; zonerefs = pgdat-&gt;node_zonelists[ZONELIST_NOFALLBACK]._zonerefs; nr_zones = build_zonerefs_node(pgdat, zonerefs); zonerefs += nr_zones; zonerefs-&gt;zone = NULL; zonerefs-&gt;zone_idx = 0;} 如何查找 不同的zone区？ 根据page查找zone区，在 include/linux/mm.h 1234567891011#define ZONES_WIDTH ZONES_SHIFT#define ZONES_MASK ((1UL &lt;&lt; ZONES_WIDTH) - 1)static inline void set_page_zone(struct page *page, enum zone_type zone) { page-&gt;flags &amp;= ~(ZONES_MASK &lt;&lt; ZONES_PGSHIFT); page-&gt;flags |= (zone &amp; ZONES_MASK) &lt;&lt; ZONES_PGSHIFT;}static inline int page_zone_id(struct page *page) { return (page-&gt;flags &gt;&gt; ZONEID_PGSHIFT) &amp; ZONEID_MASK;} zone相关参数 在 alloc_pages 中如何起作用的alloc_pages 最后调用到 __alloc_pages_nodemask 根据 gfp 和 mempolicy等生成了两个参数 preferred_nid nodemask，需要探究一下这两个参数生成背后逻辑 12345678910111213141516171819202122232425262728293031struct page *__alloc_pages_nodemask(gfp_t gfp_mask, unsigned int order, int preferred_nid, nodemask_t *nodemask);struct page *alloc_pages_current(gfp_t gfp, unsigned order){ struct mempolicy *pol = &amp;default_policy; struct page *page; if (!in_interrupt() &amp;&amp; !(gfp &amp; __GFP_THISNODE)) pol = get_task_policy(current); /* * No reference counting needed for current-&gt;mempolicy * nor system default_policy */ if (pol-&gt;mode == MPOL_INTERLEAVE) page = alloc_page_interleave(gfp, order, interleave_nodes(pol)); else page = __alloc_pages_nodemask(gfp, order, policy_node(gfp, pol, numa_node_id()), policy_nodemask(gfp, pol)); return page;}static inline struct page *alloc_pages(gfp_t gfp_mask, unsigned int order){ return alloc_pages_current(gfp_mask, order);} policy_node 代码： 12345678910111213141516171819202122232425262728DECLARE_PER_CPU(int, numa_node);static inline int numa_node_id(void){ return raw_cpu_read(numa_node);}static inline void set_numa_node(int node){ this_cpu_write(numa_node, node);}/* Return the node id preferred by the given mempolicy, or the given id */static int policy_node(gfp_t gfp, struct mempolicy *policy, int nd){ if (policy-&gt;mode == MPOL_PREFERRED &amp;&amp; !(policy-&gt;flags &amp; MPOL_F_LOCAL)) nd = policy-&gt;v.preferred_node; else { /* * __GFP_THISNODE shouldn't even be used with the bind policy * because we might easily break the expectation to stay on the * requested node and not break the policy. */ WARN_ON_ONCE(policy-&gt;mode == MPOL_BIND &amp;&amp; (gfp &amp; __GFP_THISNODE)); } return nd;} policy_nodemask 代码： 1234567891011121314/* * Return a nodemask representing a mempolicy for filtering nodes for * page allocation */nodemask_t *policy_nodemask(gfp_t gfp, struct mempolicy *policy){ /* Lower zones don't get a nodemask applied for MPOL_BIND */ if (unlikely(policy-&gt;mode == MPOL_BIND) &amp;&amp; apply_policy_zone(policy, gfp_zone(gfp)) &amp;&amp; cpuset_nodemask_valid_mems_allowed(&amp;policy-&gt;v.nodes)) return &amp;policy-&gt;v.nodes; return NULL;} 跟踪发现 policy_nodemask 返回值基本都是 NULL，除非特殊需求，使用过mbind才会返回非NULL。 123456789Inspiron-5548@ubuntu: ~/workspace/linux-stable# sudo bpftrace -e 'kretprobe:policy_nodemask {printf(&quot;[pid-%d:%s]: policy_nodemask retval %p\\n&quot;, pid,comm, retval)}'Attaching 1 probe...[pid-2326:gnome-terminal-]: policy_nodemask retval (nil)[pid-1290:gnome-shell]: policy_nodemask retval (nil)[pid-2326:gnome-terminal-]: policy_nodemask retval (nil)[pid-22642:bpftrace]: policy_nodemask retval (nil)[pid-1290:gnome-shell]: policy_nodemask retval (nil)[pid-1290:gnome-shell]: policy_nodemask retval (nil)","link":"/2021/02/04/memory/node%20zone%20%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98/"},{"title":"vmstat 统计了什么资源","text":"mm/vmstat.c 提供了很多对内存方面的统计，在/proc/vmstat 上有体现。 vmstat 在不同维度上的统计先看 drain_zonestat 函数 1234567891011121314151617181920212223void drain_zonestat(struct zone *zone, struct per_cpu_pageset *pset){ int i; for (i = 0; i &lt; NR_VM_ZONE_STAT_ITEMS; i++) if (pset-&gt;vm_stat_diff[i]) { int v = pset-&gt;vm_stat_diff[i]; pset-&gt;vm_stat_diff[i] = 0; atomic_long_add(v, &amp;zone-&gt;vm_stat[i]); atomic_long_add(v, &amp;vm_zone_stat[i]); }#ifdef CONFIG_NUMA for (i = 0; i &lt; NR_VM_NUMA_STAT_ITEMS; i++) if (pset-&gt;vm_numa_stat_diff[i]) { int v = pset-&gt;vm_numa_stat_diff[i]; pset-&gt;vm_numa_stat_diff[i] = 0; atomic_long_add(v, &amp;zone-&gt;vm_numa_stat[i]); atomic_long_add(v, &amp;vm_numa_stat[i]); }#endif} 可以发现 struct per_cpu_pageset-&gt;vm_stat_diff[NR_VM_ZONE_STAT_ITEMS] zone-&gt;vm_stat[NR_VM_ZONE_STAT_ITEMS] vm_zone_stat[NR_VM_ZONE_STAT_ITEMS] 是不同维度上，对同一类信息的统计 – vm 虚拟内存的使用情况的监控 123456789101112131415161718enum zone_stat_item { /* First 128 byte cacheline (assuming 64 bit words) */ NR_FREE_PAGES, NR_ZONE_LRU_BASE, /* Used only for compaction and reclaim retry */ NR_ZONE_INACTIVE_ANON = NR_ZONE_LRU_BASE, NR_ZONE_ACTIVE_ANON, NR_ZONE_INACTIVE_FILE, NR_ZONE_ACTIVE_FILE, NR_ZONE_UNEVICTABLE, NR_ZONE_WRITE_PENDING, /* Count of dirty, writeback and unstable pages */ NR_MLOCK, /* mlock()ed pages found and moved off LRU */ /* Second 128 byte cacheline */ NR_BOUNCE,#if IS_ENABLED(CONFIG_ZSMALLOC) NR_ZSPAGES, /* allocated in zsmalloc */#endif NR_FREE_CMA_PAGES, NR_VM_ZONE_STAT_ITEMS }; 对应实际系统中/proc/vmstat的文件 123456789Inspiron-5548@ubuntu: ~/workspace# cat /proc/vmstat | grep zonenr_zone_inactive_anon 6569nr_zone_active_anon 317170nr_zone_inactive_file 211371nr_zone_active_file 138953nr_zone_unevictable 84539nr_zone_write_pending 35zone_reclaim_failed 0Inspiron-5548@ubuntu: ~/workspace# struct per_cpu_pageset-&gt;vm_numa_stat_diff[NR_VM_NUMA_STAT_ITEMS] zone-&gt;vm_numa_stat[NR_VM_NUMA_STAT_ITEMS] vm_numa_stat[NR_VM_NUMA_STAT_ITEMS] 也是不同维度上，对同一类信息的统计 – numa 内存分配行为的监控 123456789enum numa_stat_item { NUMA_HIT, /* allocated in intended node */ NUMA_MISS, /* allocated in non intended node */ NUMA_FOREIGN, /* was intended here, hit elsewhere */ NUMA_INTERLEAVE_HIT, /* interleaver preferred this zone */ NUMA_LOCAL, /* allocation from local node */ NUMA_OTHER, /* allocation from other node */ NR_VM_NUMA_STAT_ITEMS}; 对应实际系统中/proc/vmstat的文件 12345678910111213Inspiron-5548@ubuntu: ~/workspace# cat /proc/vmstat | grep numanuma_hit 2605582numa_miss 0numa_foreign 0numa_interleave 45536numa_local 2605582numa_other 0numa_pte_updates 0numa_huge_pte_updates 0numa_hint_faults 0numa_hint_faults_local 0numa_pages_migrated 0Inspiron-5548@ubuntu: ~/workspace# 上面/proc/vmstat 是整个系统的情况，对应 vm_zone_stat 与 vm_numa_stat。 /proc/zoneinfo是 对应的是zone 和 pcp的情况，对应的是 zone-&gt;vm_stat 与 zone-&gt;vm_numa_stat， 和 per_cpu_pageset-&gt;vm_stat_diff 与 per_cpu_pageset-&gt;vm_numa_stat_diff 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152Inspiron-5548@ubuntu: ~/workspace# cat /proc/zoneinfoNode 0, zone Normal pages free 9557 min 6807 low 8508 high 10209 spanned 389120 present 389120 managed 370822 protection: (0, 0, 0, 0, 0) nr_free_pages 9557 nr_zone_inactive_anon 224 nr_zone_active_anon 114422 nr_zone_inactive_file 120358 nr_zone_active_file 58547 nr_zone_unevictable 7191 nr_zone_write_pending 3 nr_mlock 4 nr_page_table_pages 2879 nr_kernel_stack 8016 nr_bounce 0 nr_zspages 0 nr_free_cma 0 numa_hit 870583 numa_miss 0 numa_foreign 0 numa_interleave 45534 numa_local 870583 numa_other 0 pagesets cpu: 0 count: 293 high: 378 batch: 63 vm stats threshold: 30 cpu: 1 count: 293 high: 378 batch: 63 vm stats threshold: 30 cpu: 2 count: 234 high: 378 batch: 63 vm stats threshold: 30 cpu: 3 count: 338 high: 378 batch: 63 vm stats threshold: 30 node_unreclaimable: 0 start_pfn: 1048576 全局统计的数据可能和上面有些许冲突 123atomic_long_t vm_zone_stat[NR_VM_ZONE_STAT_ITEMS] __cacheline_aligned_in_smp;atomic_long_t vm_numa_stat[NR_VM_NUMA_STAT_ITEMS] __cacheline_aligned_in_smp;atomic_long_t vm_node_stat[NR_VM_NODE_STAT_ITEMS] __cacheline_aligned_in_smp; 分别是 zone numa node 的统计信息，zone numa 上面都已经分析了这个 node 对应着 /proc/zoneinfo 开头信息 12345678910111213141516171819202122232425262728293031323334353637Inspiron-5548@ubuntu: ~/workspace# cat /proc/zoneinfoNode 0, zone DMA per-node stats nr_inactive_anon 6591 nr_active_anon 315599 nr_inactive_file 212218 nr_active_file 139142 nr_unevictable 84780 nr_slab_reclaimable 23493 nr_slab_unreclaimable 26794 nr_isolated_anon 0 nr_isolated_file 0 workingset_nodes 0 workingset_refault 0 workingset_activate 0 workingset_restore 0 workingset_nodereclaim 0 nr_anon_pages 332534 nr_mapped 104733 nr_file_pages 425795 nr_dirty 7 nr_writeback 0 nr_writeback_temp 0 nr_shmem 95530 nr_shmem_hugepages 0 nr_shmem_pmdmapped 0 nr_file_hugepages 0 nr_file_pmdmapped 0 nr_anon_transparent_hugepages 0 nr_vmscan_write 0 nr_vmscan_immediate_reclaim 0 nr_dirtied 81310 nr_written 72963 nr_kernel_misc_reclaimable 0 nr_foll_pin_acquired 0 nr_foll_pin_released 0 pages free 3788 与定义 也十分相符 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950enum node_stat_item { NR_LRU_BASE, NR_INACTIVE_ANON = NR_LRU_BASE, /* must match order of LRU_[IN]ACTIVE */ NR_ACTIVE_ANON, /* &quot; &quot; &quot; &quot; &quot; */ NR_INACTIVE_FILE, /* &quot; &quot; &quot; &quot; &quot; */ NR_ACTIVE_FILE, /* &quot; &quot; &quot; &quot; &quot; */ NR_UNEVICTABLE, /* &quot; &quot; &quot; &quot; &quot; */ NR_SLAB_RECLAIMABLE_B, NR_SLAB_UNRECLAIMABLE_B, NR_ISOLATED_ANON, /* Temporary isolated pages from anon lru */ NR_ISOLATED_FILE, /* Temporary isolated pages from file lru */ WORKINGSET_NODES, WORKINGSET_REFAULT_BASE, WORKINGSET_REFAULT_ANON = WORKINGSET_REFAULT_BASE, WORKINGSET_REFAULT_FILE, WORKINGSET_ACTIVATE_BASE, WORKINGSET_ACTIVATE_ANON = WORKINGSET_ACTIVATE_BASE, WORKINGSET_ACTIVATE_FILE, WORKINGSET_RESTORE_BASE, WORKINGSET_RESTORE_ANON = WORKINGSET_RESTORE_BASE, WORKINGSET_RESTORE_FILE, WORKINGSET_NODERECLAIM, NR_ANON_MAPPED, /* Mapped anonymous pages */ NR_FILE_MAPPED, /* pagecache pages mapped into pagetables. only modified from process context */ NR_FILE_PAGES, NR_FILE_DIRTY, NR_WRITEBACK, NR_WRITEBACK_TEMP, /* Writeback using temporary buffers */ NR_SHMEM, /* shmem pages (included tmpfs/GEM pages) */ NR_SHMEM_THPS, NR_SHMEM_PMDMAPPED, NR_FILE_THPS, NR_FILE_PMDMAPPED, NR_ANON_THPS, NR_VMSCAN_WRITE, NR_VMSCAN_IMMEDIATE, /* Prioritise for reclaim when writeback ends */ NR_DIRTIED, /* page dirtyings since bootup */ NR_WRITTEN, /* page writings since bootup */ NR_KERNEL_MISC_RECLAIMABLE, /* reclaimable non-slab kernel pages */ NR_FOLL_PIN_ACQUIRED, /* via: pin_user_page(), gup flag: FOLL_PIN */ NR_FOLL_PIN_RELEASED, /* pages returned via unpin_user_page() */ NR_KERNEL_STACK_KB, /* measured in KiB */#if IS_ENABLED(CONFIG_SHADOW_CALL_STACK) NR_KERNEL_SCS_KB, /* measured in KiB */#endif NR_PAGETABLE, /* used for pagetables */ NR_VM_NODE_STAT_ITEMS}; atomic_long_t vm_zone_stat[NR_VM_ZONE_STAT_ITEMS] __cacheline_aligned_in_smp;atomic_long_t vm_numa_stat[NR_VM_NUMA_STAT_ITEMS] __cacheline_aligned_in_smp;atomic_long_t vm_node_stat[NR_VM_NODE_STAT_ITEMS] __cacheline_aligned_in_smp; vm_zone_stat 主要是 统计各个zone的各种 anon file active inactive page的数量。vm_numa_stat 主要是 统计各个 node 节点上内存分配行为，是从本节点分配的，还是跨节点分配了内存。vm_node_stat 主要是 统计各个 node节点 的各种 anon file active inactive page的数量，还有各种类型 shmem dirty writeback的page数量 migrate type 是来定义什么的？12345678enum migratetype { MIGRATE_UNMOVABLE, MIGRATE_MOVABLE, MIGRATE_RECLAIMABLE, MIGRATE_PCPTYPES, /* the number of types on the pcp lists */ MIGRATE_HIGHATOMIC = MIGRATE_PCPTYPES, MIGRATE_TYPES}; 很明显 migratetype 是用来定义 伙伴系统中页面的，order相同情况下根据是否可以 移动，分成了几条 list，方便了后面各种 内存迁移，内存规整。 1234struct free_area { struct list_head free_list[MIGRATE_TYPES]; unsigned long nr_free;}; 123struct zone { struct free_area free_area[MAX_ORDER];} lruvec 是在哪个维度上？2016年之前 lru 是基于zone这个维度上的，现在都是基于 node这个维度的 1234567commit 75ef7184053989118d3814c558a9af62e7376a58Author: Mel Gorman &lt;mgorman@techsingularity.net&gt;Date: Thu Jul 28 15:45:24 2016 -0700 mm, vmstat: add infrastructure for per-node vmstats Patchset: &quot;Move LRU page reclaim from zones to nodes v9&quot; 先看定义 123456789101112131415161718192021enum lru_list { LRU_INACTIVE_ANON = LRU_BASE, LRU_ACTIVE_ANON = LRU_BASE + LRU_ACTIVE, LRU_INACTIVE_FILE = LRU_BASE + LRU_FILE, LRU_ACTIVE_FILE = LRU_BASE + LRU_FILE + LRU_ACTIVE, LRU_UNEVICTABLE, NR_LRU_LISTS};struct lruvec { struct list_head lists[NR_LRU_LISTS]; /* per lruvec lru_lock for memcg */ spinlock_t lru_lock;#ifdef CONFIG_MEMCG struct pglist_data *pgdat;#endif};typedef struct pglist_data { struct lruvec __lruvec;} pg_data_t; 代码定义大多来自于Linux 5.10","link":"/2021/02/01/memory/vmstat%20%E7%BB%9F%E8%AE%A1%E4%BA%86%E4%BB%80%E4%B9%88%E8%B5%84%E6%BA%90/"},{"title":"搭建f2fs测试环境","text":"发现 f2fs 相关代码有一些优化点，想着提个patch该一下，但是手中没有使用f2fs的设备，不好验证，然后我就想使用 mkfs.f2fs初始化一个文件，然后通过mount的方式去挂载，最后在文件系统上做一些测试读写等 环境搭建 安装f2fs 工具源码地址我是编译安装的，也可以尝试从命令行安装 1Inspiron-5548@ubuntu: ~/workspace# sudo apt install f2fs-tools 生成块文件，然后格式化块文件 123456789101112131415161718192021222324stable_kernel@kernel: ~/workspace/f2fs# dd if=/dev/zero of=./test_f2fs bs=1M count=40964096+0 records in4096+0 records out4294967296 bytes (4.3 GB, 4.0 GiB) copied, 6.36491 s, 675 MB/sstable_kernel@kernel: ~/workspace/f2fs# mkfs.f2fs test_f2fs F2FS-tools: mkfs.f2fs Ver: 1.9.0 (2017-09-21)Info: Disable heap-based policyInfo: Debug level = 0Info: Label =Info: Trim is enabledInfo: Segments per section = 1Info: Sections per zone = 1Info: sector size = 512Info: total sectors = 8388608 (4096 MB)Info: zone aligned segment0 blkaddr: 512Info: format version with &quot;Linux version 5.11.0-rc6+ (root@server) (gcc (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0, GNU ld (GNU Binutils for Ubuntu) 2.34) #50 SMP Wed Feb 3 11:04:13 CST 2021&quot;Info: [test_f2fs] Discarding deviceInfo: Overprovision ratio = 3.150%Info: Overprovision segments = 132 (GC reserved = 71)Info: format successfulstable_kernel@kernel: ~/workspace/f2fs# mkdir test_dir 使用 -t f2fs 选项挂载这个 f2fs disk在没有使能f2fs的内核上，尝试安装，会出现如下报错123stable_kernel@1kernel: ~/workspace/f2fs# sudo mount -t f2fs ./test_file ./test_dirmount: /home/rlk/workspace/f2fs/test_dir: unknown filesystem type 'f2fs'. 需要使能如下选项重新编译 kernel 12345678910if [ $debug_fsf2 == 1 ]then## debug_fsf2 enable start echo &quot;CONFIG_F2FS_FS=y&quot; &gt;&gt; /tmp/.config echo &quot;CONFIG_F2FS_STAT_FS=y&quot; &gt;&gt; /tmp/.config echo &quot;CONFIG_F2FS_FS_XATTR=y&quot; &gt;&gt; /tmp/.config echo &quot;CONFIG_F2FS_FS_POSIX_ACL=y&quot; &gt;&gt; /tmp/.config echo &quot;CONFIG_CRYPTO_CRC32=y&quot; &gt;&gt; /tmp/.config## debug_fsf2 enable endfi 重新挂载 1234567891011121314151617stable_kernel@kernel: ~/workspace/f2fs# df -aFilesystem 1K-blocks Used Available Use% Mounted on/dev/root 32377648 19964388 10745524 66% /....../dev/sda1 523248 4 523244 1% /boot/efitmpfs 174684 24 174660 1% /run/user/1000binfmt_misc 0 0 0 - /proc/sys/fs/binfmt_miscstable_kernel@kernel: ~/workspace/f2fs# sudo mount -t f2fs ./test_f2fs ./test_dirstable_kernel@kernel: ~/workspace/f2fs# df -aFilesystem 1K-blocks Used Available Use% Mounted on/dev/root 32377648 19964388 10745524 66% /....../dev/sda1 523248 4 523244 1% /boot/efitmpfs 174684 24 174660 1% /run/user/1000binfmt_misc 0 0 0 - /proc/sys/fs/binfmt_misc/dev/loop0 4192256 307208 3885048 8% /home/rlk/workspace/f2fs/test_dirstable_kernel@kernel: ~/workspace/f2fs# 读写 test_dir 相关目录 1 跟踪相关 probe，触发 slab回收，观察 跟踪 kprobe 123456stable_kernel@130kernel: ~/workspace/f2fs# sudo bpftrace -e 'kretprobe:f2fs_shrink_scan {printf(&quot;[pid-%d:%s]: f2fs_shrink_scan retval %d\\n&quot;, pid,comm, retval)}'Attaching 1 probe...[pid-3577:bash]: f2fs_shrink_scan retval 128[pid-3577:bash]: f2fs_shrink_scan retval 128[pid-3577:bash]: f2fs_shrink_scan retval 128 触发 slab 回收 12root@rlk-Standard-PC-i440FX-PIIX-1996:/proc/sys/vm# sudo echo 3 &gt; drop_cachesroot@rlk-Standard-PC-i440FX-PIIX-1996:/proc/sys/vm# 发patch最后发现是我自己天真了。。。 收获 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051unsigned long f2fs_shrink_scan(struct shrinker *shrink, struct shrink_control *sc){ unsigned long nr = sc-&gt;nr_to_scan; struct f2fs_sb_info *sbi; struct list_head *p; unsigned int run_no; unsigned long freed = 0; spin_lock(&amp;f2fs_list_lock); do { run_no = ++shrinker_run_no; } while (run_no == 0); p = f2fs_list.next; while (p != &amp;f2fs_list) { sbi = list_entry(p, struct f2fs_sb_info, s_list); if (sbi-&gt;shrinker_run_no == run_no) break; /* stop f2fs_put_super */ if (!mutex_trylock(&amp;sbi-&gt;umount_mutex)) { p = p-&gt;next; continue; } spin_unlock(&amp;f2fs_list_lock); sbi-&gt;shrinker_run_no = run_no; /* shrink extent cache entries */ freed += f2fs_shrink_extent_tree(sbi, nr &gt;&gt; 1); /* shrink clean nat cache entries */ if (freed &lt; nr) freed += f2fs_try_to_free_nats(sbi, nr - freed); /* shrink free nids cache entries */ if (freed &lt; nr) freed += f2fs_try_to_free_nids(sbi, nr - freed); spin_lock(&amp;f2fs_list_lock); p = p-&gt;next; list_move_tail(&amp;sbi-&gt;s_list, &amp;f2fs_list); mutex_unlock(&amp;sbi-&gt;umount_mutex); if (freed &gt;= nr) break; } spin_unlock(&amp;f2fs_list_lock); return freed;} 在 f2fs_shrink_scan 中，为什么需要list_move_tail()？看 fs/ubifs/shrinker.c 中有注释： 1234/* * Move this one to the end of the list to provide some * fairness. */ 主要是让 回收过程更加公平，如果当前slab有1000个 object可以回收，只需要 回收80个object，如果不将已经回收的 sbi move到tail，就会导致每次都是 head的那几个sbi 一直被回收，但是靠近 tail的 sbi 虽然有很多 slab object，但是一直未被回收过。 参考文档 操作 1234567891011dd if=/dev/zero of=./test_f2fs2 bs=1M count=128sudo mount -t f2fs ./test_f2fs ./test_dirsudo bpftrace -e 'kretprobe:f2fs_shrink_scan {printf(&quot;[pid-%d:%s]: f2fs_shrink_scan retval %d\\n&quot;, pid,comm, retval)}'cd /proc/sys/vm &amp;&amp; sudo su rootsudo echo 3 &gt; drop_caches","link":"/2021/02/04/%E6%B5%8B%E8%AF%95/%E6%90%AD%E5%BB%BAf2fs%E6%B5%8B%E8%AF%95%E7%8E%AF%E5%A2%83/"},{"title":"kvmtool 入门","text":"参考博客参考博客参考博客","link":"/2021/02/01/%E8%99%9A%E6%8B%9F%E5%8C%96/kvmtool/kvmtool%20%E5%85%A5%E9%97%A8/"},{"title":"ext2文件系统","text":"","link":"/2021/02/01/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/ext2%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/"},{"title":"mount 流程分析","text":"","link":"/2021/02/01/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/mount%20%E6%B5%81%E7%A8%8B%E5%88%86%E6%9E%90/"},{"title":"文件系统的 xattr 是做什么用的","text":"简介与 xatter 相关的系统调用有 1getxattr(2), listxattr(2), removexattr(2), setxattr(2) 也可以直接使用 shell 去控制 1chattr(1),lsattr(1), btrfs(5), ext4(5), xfs(5). 目前还没找到这个 xatter 相关的应用，后续再补充。","link":"/2021/02/01/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E7%9A%84%20xattr%20%E6%98%AF%E5%81%9A%E4%BB%80%E4%B9%88%E7%94%A8%E7%9A%84/"},{"title":"task_struct 和 thread_info结构","text":"本文基于 Linux 5.10 代码，由于相关结构变化较大，不去考古以前代码了。 什么是内核栈？进程在内核态运行时需要自己的堆栈信息，因此linux内核为每个进程都提供了一个内核栈kernel stack(这里的stack就是这个进程在内核态的堆栈信息！！！)。 1234567891011struct task_struct {#ifdef CONFIG_THREAD_INFO_IN_TASK /* * For reasons of header soup (see current_thread_info()), this * must be the first element of task_struct. */ struct thread_info thread_info;#endif void *stack; ......} 这个 void *stack 就是栈指针，指针指向哪里呢？ 内核态的进程访问处于内核数据段的栈，这个栈不同于用户态的进程所用的栈。用户态进程所用的栈，是在进程线性地址空间中； 而内核栈是当进程从用户空间进入内核空间时，特权级发生变化，需要切换堆栈，内核空间中使用的就是这个内核栈。因为内核控制路径使用很少的栈空间，所以只需要几千个字节的内核态堆栈。 thread_info内核栈还需要存储每个进程的PCB信息,linux内核是支持不同体系的,但是不同的体系结构可能进程需要存储的信息不尽相同,这就需要我们实现一种通用的方式,我们将体系结构相关的部分和无关的部分进行分离。 用一种通用的方式来描述进程, 这就是struct task_struct 而thread_info就保存了特定体系结构的汇编代码段需要访问的那部分进程的数据目前基本的配置中（x86_64、我们公司arm64平台都是开启了这个CONFIG_THREAD_INFO_IN_TASK） 想通过 从 task_struct 找到 thread_info 就很简单了，直接结构体访问变量： 12345678#ifdef CONFIG_THREAD_INFO_IN_TASKstatic inline struct thread_info *task_thread_info(struct task_struct *task){ return &amp;task-&gt;thread_info;}#endif#define current_thread_info() ((struct thread_info *)current) x86 架构中 12345struct thread_info { unsigned long flags; /* low level flags */ unsigned long syscall_work; /* SYSCALL_WORK_ flags */ u32 status; /* thread synchronous flags */}; arm64架构中 1234567891011121314151617struct thread_info { unsigned long flags; /* low level flags */#ifdef CONFIG_ARM64_SW_TTBR0_PAN u64 ttbr0; /* saved TTBR0_EL1 */#endif union { u64 preempt_count; /* 0 =&gt; preemptible, &lt;0 =&gt; bug */ struct { u32 count; u32 need_resched; } preempt; };#ifdef CONFIG_SHADOW_CALL_STACK void *scs_base; void *scs_sp;#endif}; 如何直接找到 task_struct这就涉及到thread_union这个结构了 123456union thread_union {#ifndef CONFIG_ARCH_TASK_STRUCT_ON_STACK struct task_struct task;#endif unsigned long stack[THREAD_SIZE/sizeof(long)];}; 我们大部分平台都是开启这个CONFIG_ARCH_TASK_STRUCT_ON_STACK宏定义的arm64 x86 栈都是向下生长，这个结构大概是这样： 1234567891011121314151617181920212223242526272829303132333435363738 +---------------------+ stack[8k] |---------------------| + || || | || || | || || | || || | Grow Down || || | || || | || || | || || | || || | || || | sp&lt;---- +---------------------+ | | | | | | | | | | | | | | | | | | v | | | | | | | | | +-----------------+ | | | | | | | | | | | | | | | | | +------&gt; struct task_struct | | | | | | | | | | | | | | | | | | | | | +-----------------+ | | | | | | | | | +------&gt; struct thread_infostack[0] | +-----------------+ | +---------------------+ 有个细节没有体现出来，就是task-&gt;stack应该指向 最下面。 我们可以通过 current() 获得当前正在运行的线程的 task_struct 结构：arm64 平台 12345678910static __always_inline struct task_struct *get_current(void){ unsigned long sp_el0; asm (&quot;mrs %0, sp_el0&quot; : &quot;=r&quot; (sp_el0)); return (struct task_struct *)sp_el0;}#define current get_current() ARM64增加了很多通用寄存器，使用寄存器传递进程描述符显然效率更高。因此在ARM64架构里，current宏不再通过栈偏移量得到进程描述符地址，而是借用专门的寄存器 sp_el0。 ARM64使用sp_el0，在进程切换时暂存进程描述符地址。 可以看arm64 进程切换arch相关代码(arch/arm64/kernel/entry.S) 1234567891011121314151617181920212223242526272829303132333435/* * Register switch for AArch64. The callee-saved registers need to be saved * and restored. On entry: * x0 = previous task_struct (must be preserved across the switch) * x1 = next task_struct * Previous and next are guaranteed not to be the same. * */SYM_FUNC_START(cpu_switch_to) mov x10, #THREAD_CPU_CONTEXT add x8, x0, x10 mov x9, sp stp x19, x20, [x8], #16 // store callee-saved registers stp x21, x22, [x8], #16 stp x23, x24, [x8], #16 stp x25, x26, [x8], #16 stp x27, x28, [x8], #16 stp x29, x9, [x8], #16 str lr, [x8] add x8, x1, x10 ldp x19, x20, [x8], #16 // restore callee-saved registers ldp x21, x22, [x8], #16 ldp x23, x24, [x8], #16 ldp x25, x26, [x8], #16 ldp x27, x28, [x8], #16 ldp x29, x9, [x8], #16 ldr lr, [x8] mov sp, x9 msr sp_el0, x1 ptrauth_keys_install_kernel x1, x8, x9, x10 scs_save x0, x8 scs_load x1, x8 retSYM_FUNC_END(cpu_switch_to)NOKPROBE(cpu_switch_to) 线程切换的时候THREAD_CPU_CONTEXT –&gt; x10x8 保存的压栈大小x10 + x8 –&gt; x1x1 –&gt; sp_el0 THREAD_CPU_CONTEXT 是什么？ 123456789101112131415161718192021222324252627282930313233343536373839404142434445DEFINE(THREAD_CPU_CONTEXT, offsetof(struct task_struct, thread.cpu_context));struct task_struct { /* CPU-specific state of this task: */ struct thread_struct thread; /* * WARNING: on x86, 'thread_struct' contains a variable-sized * structure. It *MUST* be at the end of 'task_struct'. * * Do not put anything below here! */}struct thread_struct { struct cpu_context cpu_context; /* cpu context */ /* * Whitelisted fields for hardened usercopy: * Maintainers must ensure manually that this contains no * implicit padding. */ struct { unsigned long tp_value; /* TLS register */ unsigned long tp2_value; struct user_fpsimd_state fpsimd_state; } uw; ......}struct cpu_context { unsigned long x19; unsigned long x20; unsigned long x21; unsigned long x22; unsigned long x23; unsigned long x24; unsigned long x25; unsigned long x26; unsigned long x27; unsigned long x28; unsigned long fp; unsigned long sp; unsigned long pc;}; 后面需要更加详细 分析，有空再学习一下然后写。 struct thread_struct &amp; struct pt_regs的区别thread_struct结构体主要是在内核态两个进程发生切换时，thread_struct用来保存上一个进程的相关寄存器。pt_regs结构体主要是当用户态的进程陷入到内核态时，需要使用pt_regs来保存用户态进程的寄存器状态。 x86 平台 12345678DECLARE_PER_CPU(struct task_struct *, current_task);static __always_inline struct task_struct *get_current(void){ return this_cpu_read_stable(current_task);}#define current get_current() 在 x86 平台中，直接使用了 一个 percpu 变量来缓存 current_task 指针 example: (x86 架构) 123456789101112131415161718192021222324252627282930313233343536WARNING: kernel relocated [512MB]: patching 137921 gdb minimal_symbol values [26/162] KERNEL: vmlinux DUMPFILE: dump.202103011631 [PARTIAL DUMP] CPUS: 4 DATE: Mon Mar 1 16:31:27 CST 2021 UPTIME: 00:02:00LOAD AVERAGE: 1.63, 0.53, 0.19 TASKS: 449 NODENAME: rlk-Standard-PC-i440FX-PIIX-1996 RELEASE: 5.12.0-rc1 VERSION: #24 SMP Mon Mar 1 14:25:47 CST 2021 MACHINE: x86_64 (3693 Mhz) MEMORY: 4 GB PANIC: &quot;Kernel panic - not syncing: softlockup: hung tasks&quot; PID: 13 COMMAND: &quot;kworker/0:1&quot; TASK: ffff91344081b240 [THREAD_INFO: ffff91344081b240] CPU: 0 STATE: TASK_RUNNING (PANIC)crash&gt; btPID: 13 TASK: ffff91344081b240 CPU: 0 COMMAND: &quot;kworker/0:1&quot;bt: seek error: kernel virtual address: ffff98357ffffff8 type: &quot;stack contents&quot;bt: read of stack at ffff98357ffffff8 failedcrash&gt; struct task_struct ffff91344081b240struct task_struct { thread_info = { flags = 16392, syscall_work = 0, status = 0 }, state = 0, stack = 0xffff983580070000, usage = {...... 可以看到 task thread_info 地址都是 ffff91344081b240。stack地址是0xffff983580070000。 参考linux 进程内核栈参考Linux调度——神奇的current参考Linux syscall过程分析参考do_fork实现–下","link":"/2021/02/19/%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/task_struct%20%E5%92%8C%20thread_info%E7%BB%93%E6%9E%84/"},{"title":"psi 相关patch","text":"PSI简介psi 是 pressure stall information缩写，主要是观测 系统压力的一系列指标 下面是 Kernel Documention 翻译：当系统CPU，内存或IO设备有竞争风险时，workload会遇到较大的sched latency，吞吐量减少，并有导致OOM被杀死的风险。如果没有这种到底是哪种资源竞争的度量，在发生资源紧张 或者 性能问题的时候很难确定到底是那种资源导致的问题（毕竟 内存问题可能导致CPU占满，也会导致IO读取慢等问题），很难找出木桶中最短的那块木板。 psi功能可以识别并量化由 木桶中最短缺的资源 造成的中断及其对复杂工作负载乃至整个系统的时间影响。准确衡量由资源短缺导致的生产力损失，可以帮助用户将工作负载调整为硬件大小，或者根据工作负载需求配置硬件。 当psi实时汇总此信息时，可以使用诸如减载，将作业迁移到其他系统或数据中心之类的技术来动态管理系统，或者从战略上暂停或杀死低优先级或可重新启动的批处理作业。这样可以最大化硬件利用率，且不会牺牲工作负载的运行状况 或冒重大破坏（如OOM终止）的风险。 关于用户空间接口，在我ubuntu 服务器上（Linux 5.8）可以看到： 123456789amd_server@ubuntu: ~/workspace/tmp# cat /proc/pressure/cpusome avg10=0.00 avg60=0.00 avg300=0.00 total=2344622763amd_server@ubuntu: ~/workspace/tmp# cat /proc/pressure/iosome avg10=0.00 avg60=0.00 avg300=0.00 total=454836681full avg10=0.00 avg60=0.00 avg300=0.00 total=444813857amd_server@ubuntu: ~/workspace/tmp# cat /proc/pressure/memorysome avg10=0.00 avg60=0.00 avg300=0.00 total=31812811full avg10=0.00 avg60=0.00 avg300=0.00 total=15845529amd_server@ubuntu: ~/workspace/tmp# PSI 的优点如上所说，可以定位系统中的 短板资源 是哪个。在没有 PSI 之前的情况是怎么样的呢？之前可以使用 loadavg 与 vmpressure 来衡量一些资源压力的情况： loadavg：主要是衡量 一定时间内 运行队列上的进程数（running + uninterruptible 进程数量）1234567891011121314long calc_load_fold_active(struct rq *this_rq, long adjust){ long nr_active, delta = 0; nr_active = this_rq-&gt;nr_running - adjust; nr_active += (long)this_rq-&gt;nr_uninterruptible; if (nr_active != this_rq-&gt;calc_load_active) { delta = nr_active - this_rq-&gt;calc_load_active; this_rq-&gt;calc_load_active = nr_active; } return delta;} 用户空间接口/proc/loadavg 123amd_server@ubuntu: /proc# cat /proc/loadavg0.22 0.14 0.10 2/1013 752563amd_server@ubuntu: /proc# vmpressure可以告知此时内存压力，但是这个需要配置 CONFIG_MEMCG。后面再写。。代码主要在 mm/vmpressure.c 缺点嘛，很明显：vmpressure 也有一些缺陷：结果仅体现内存回收压力，不能反映系统在申请内存上的资源等待时间；计算周期比较粗；粗略的几个等级通知，无法精细化管理。 PSI实现1234567891011121314151617181920212223+--------------------------------------------------------------+| || LKMD OOMD等 应用层模块 || |+-----------+------------------------------------+-------------+ | | v v System 接口 Per group 接口 | | | |+-----------+------------------------------------+--------------+| || PSI 值 计 算 PER CPUTASK和 状态时间统计 |+-----------+------------------------------------+--------------+ Enqueue| Dequeue Fast| path | | Tick , | Wakeup Slow | Path | |+-----------v--------------+ +-----------v--------------+| | | || SCHEDULE | | MEMORY || | | |+--------------------------+ +--------------------------+ 对于用户空间，PSI 模块通过文件系统节点向用户空间开放两种形态的接口。一种是系统级别的接口/proc/pressure/。另外一种是结合 control group，进行更精细化的分组后面再写。 对于内核空间实现，PSI 模块通过在内存管理模块以及调度器模块中插桩，我们可以跟踪每一个任务由于 memory、io 以及 CPU 资源而进入等待状态的信息。例如系统中处于 iowait 状态的 task 数目、由于等待 memory 资源而处于阻塞状态的任务数目。 代码主要在 kernel/sched/psi.c，kernel/sched/Makefile中 1obj-$(CONFIG_PSI) += psi.o 这是实现为一个 kernel module的： 1234567891011static int __init psi_proc_init(void){ if (psi_enable) { proc_mkdir(&quot;pressure&quot;, NULL); proc_create(&quot;pressure/io&quot;, 0, NULL, &amp;psi_io_proc_ops); proc_create(&quot;pressure/memory&quot;, 0, NULL, &amp;psi_memory_proc_ops); proc_create(&quot;pressure/cpu&quot;, 0, NULL, &amp;psi_cpu_proc_ops); } return 0;}module_init(psi_proc_init); 初始化123456789101112131415161718192021222324252627282930313233343536373839404142434445static void psi_avgs_work(struct work_struct *work) { mutex_lock(&amp;group-&gt;avgs_lock); now = sched_clock(); collect_percpu_times(group, PSI_AVGS, &amp;changed_states); nonidle = changed_states &amp; (1 &lt;&lt; PSI_NONIDLE); /* * If there is task activity, periodically fold the per-cpu * times and feed samples into the running averages. If things * are idle and there is no data to process, stop the clock. * Once restarted, we'll catch up the running averages in one * go - see calc_avgs() and missed_periods. */ if (now &gt;= group-&gt;avg_next_update) group-&gt;avg_next_update = update_averages(group, now); if (nonidle) { schedule_delayed_work(dwork, nsecs_to_jiffies( group-&gt;avg_next_update - now) + 1); } mutex_unlock(&amp;group-&gt;avgs_lock);}static void group_init(struct psi_group *group){ int cpu; for_each_possible_cpu(cpu) seqcount_init(&amp;per_cpu_ptr(group-&gt;pcpu, cpu)-&gt;seq); group-&gt;avg_last_update = sched_clock(); group-&gt;avg_next_update = group-&gt;avg_last_update + psi_period; INIT_DELAYED_WORK(&amp;group-&gt;avgs_work, psi_avgs_work); mutex_init(&amp;group-&gt;avgs_lock); ......}void __init psi_init(void) { psi_period = jiffies_to_nsecs(PSI_FREQ); group_init(&amp;psi_system);}void __init sched_init(void) { psi_init(); ...... scheduler_running = 1;} 初始化分为俩部分:a. 模块初始化b. 还有部分是跟随 schedule_init的初始化 如果支持 cgroup（需要 mount cgroup2 文件系统），那么系统中会有多个 PSI group，形成层级结构。 事件触发 统计其实就是在各个事件发生的时候，在代码路径中埋点。整个 PSI 技术的核心难点其实在于如何准确捕捉到任务状态的变化，并统计状态持续时间。状态的标记主要通过函数 psi_task_change()，相关的flag保存在 task_struct-&gt;flag 中。如12345/* Task state bitmasks */#define TSK_IOWAIT (1 &lt;&lt; NR_IOWAIT)#define TSK_MEMSTALL (1 &lt;&lt; NR_MEMSTALL)#define TSK_RUNNING (1 &lt;&lt; NR_RUNNING)#define TSK_ONCPU (1 &lt;&lt; NR_ONCPU) 在 enqueue_task() dequeue_task() 中都会调用到 psi hook的函数，最后调用到 psi_task_change()， 周期性统计周期性统计任务是在 group_init() 中 开启的， 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970static void calc_avgs(unsigned long avg[3], int missed_periods, u64 time, u64 period){ unsigned long pct; /* Fill in zeroes for periods of no activity */ if (missed_periods) { avg[0] = calc_load_n(avg[0], EXP_10s, 0, missed_periods); avg[1] = calc_load_n(avg[1], EXP_60s, 0, missed_periods); avg[2] = calc_load_n(avg[2], EXP_300s, 0, missed_periods); } /* Sample the most recent active period */ pct = div_u64(time * 100, period); pct *= FIXED_1; avg[0] = calc_load(avg[0], EXP_10s, pct); avg[1] = calc_load(avg[1], EXP_60s, pct); avg[2] = calc_load(avg[2], EXP_300s, pct);}static u64 update_averages(struct psi_group *group, u64 now){ expires = group-&gt;avg_next_update; if (now - expires &gt;= psi_period) missed_periods = div_u64(now - expires, psi_period); avg_next_update = expires + ((1 + missed_periods) * psi_period); period = now - (group-&gt;avg_last_update + (missed_periods * psi_period)); group-&gt;avg_last_update = now; for (s = 0; s &lt; NR_PSI_STATES - 1; s++) { u32 sample; sample = group-&gt;total[PSI_AVGS][s] - group-&gt;avg_total[s]; if (sample &gt; period) sample = period; group-&gt;avg_total[s] += sample; calc_avgs(group-&gt;avg[s], missed_periods, sample, period); } return avg_next_update;}static void psi_avgs_work(struct work_struct *work) { mutex_lock(&amp;group-&gt;avgs_lock); now = sched_clock(); collect_percpu_times(group, PSI_AVGS, &amp;changed_states); nonidle = changed_states &amp; (1 &lt;&lt; PSI_NONIDLE); /* * If there is task activity, periodically fold the per-cpu * times and feed samples into the running averages. If things * are idle and there is no data to process, stop the clock. * Once restarted, we'll catch up the running averages in one * go - see calc_avgs() and missed_periods. */ if (now &gt;= group-&gt;avg_next_update) group-&gt;avg_next_update = update_averages(group, now); if (nonidle) { schedule_delayed_work(dwork, nsecs_to_jiffies( group-&gt;avg_next_update - now) + 1); } mutex_unlock(&amp;group-&gt;avgs_lock);}static void group_init(struct psi_group *group){ ...... INIT_DELAYED_WORK(&amp;group-&gt;avgs_work, psi_avgs_work);} 运行时机是 psi_period = jiffies_to_nsecs(PSI_FREQ); 每2s。 从底层看，一个 psi group 的 PSI 值是基于任务数目统计的，当一个任务状态发生变化的时候，首先需要遍历该任务所属的 PSI group（如果不支持 cgroup，那么系统只有一个全局的 PSI group），更新 PSI group 的 task counter。 一旦 task counter 发生了变化，那么我们需要进一步更新对应 CPU 上的时间统计信息。例如 iowait task count 从 0 变成 1，那么 SOME 维度的 io wait time 需要更新。具体的 per-CPU PSI 状态时间统计信息如下： 具体分析可以根据如下 标志 来跟踪内核中的埋点 12345/* Task state bitmasks */#define TSK_IOWAIT (1 &lt;&lt; NR_IOWAIT)#define TSK_MEMSTALL (1 &lt;&lt; NR_MEMSTALL)#define TSK_RUNNING (1 &lt;&lt; NR_RUNNING)#define TSK_ONCPU (1 &lt;&lt; NR_ONCPU) 各种资源状态 12345678910111213141516/* * Pressure states for each resource: * * SOME: Stalled tasks &amp; working tasks * FULL: Stalled tasks &amp; no working tasks */enum psi_states { PSI_IO_SOME, PSI_IO_FULL, PSI_MEM_SOME, PSI_MEM_FULL, PSI_CPU_SOME, /* Only per-CPU, to weigh the CPU in the global average: */ PSI_NONIDLE, NR_PSI_STATES = 6,}; memory 相关埋点 12345678static void psi_flags_change(struct task_struct *task, int clear, int set);void psi_task_change(struct task_struct *task, int clear, int set);void psi_task_switch(struct task_struct *prev, struct task_struct *next, bool sleep);void psi_memstall_enter(unsigned long *flags);void psi_memstall_leave(unsigned long *flags); 输出到 用户空间 1234567891011121314static int psi_io_show(struct seq_file *m, void *v){ return psi_show(m, &amp;psi_system, PSI_IO);}static int psi_memory_show(struct seq_file *m, void *v){ return psi_show(m, &amp;psi_system, PSI_MEM);}static int psi_cpu_show(struct seq_file *m, void *v){ return psi_show(m, &amp;psi_system, PSI_CPU);} 有了 PSI 对系统资源压力的准确评估，可以做很多有意义的功能来最大化系统资源的利用。比如 facebook 开发的 cgroup2 和 oomd。oomd 是一个用户态的 out of memory 监控管理服务。 Android 早期在 kernel 新增了一个功能叫 lmk(low memory killer)，在有了 PSI 之后，android 将默认的 LMK 替换成了用户态的 LMKD。其代码存放于 android/system/core/lmkd/。 其核心思想是给 /proc/pressure/memory 的 SOME 和 FULL 设定阈值，当延时超过阈值时，触发 lmkd daemon 进程选择进程杀死。同时，还可以结合 meminfo 的剩余内存大小来判断需要清理的程度和所选进程的优先级。 参考文章","link":"/2021/02/08/%E5%86%85%E6%A0%B8%E8%A7%82%E6%B5%8B/%E6%80%A7%E8%83%BD%E7%A8%B3%E5%AE%9A%E6%80%A7/psi%20%E7%9B%B8%E5%85%B3patch/"},{"title":"hrtimer","text":"hrtimer没有使用时间轮对定时器进行管理，而是选用了更加通用、性能稳定的红黑树。红黑树查找，插入和删除平均时间复杂度是O(logN)。虽然查找的时间复杂度达不到O(1)，但是避免了时间轮的迁移。 因此在平均性能上红黑树和时间轮会较为接近。 hrtimer红黑树是在红黑树的基础上做了简单的封装，hrtimer红黑树节点使用数据结构struct timerqueue_node，比rb_node多了一个expires字段，用于记录超时时刻。hrtimer的数据结构与动态定时器的数据结构类似。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849struct timerqueue_node { struct rb_node node; ktime_t expires;};// include/linux/hrtimer.h/** * struct hrtimer - the basic hrtimer structure * @node: timerqueue node, which also manages node.expires, * the absolute expiry time in the hrtimers internal * representation. The time is related to the clock on * which the timer is based. Is setup by adding * slack to the _softexpires value. For non range timers * identical to _softexpires. * @_softexpires: the absolute earliest expiry time of the hrtimer. * The time which was given as expiry time when the timer * was armed. * @function: timer expiry callback function * @base: pointer to the timer base (per cpu and per clock) * @state: state information (See bit values above) * @is_rel: Set if the timer was armed relative * @is_soft: Set if hrtimer will be expired in soft interrupt context. * @is_hard: Set if hrtimer will be expired in hard interrupt context * even on RT. * * The hrtimer structure must be initialized by hrtimer_init() */struct hrtimer { struct timerqueue_node node; ktime_t _softexpires; enum hrtimer_restart (*function)(struct hrtimer *); struct hrtimer_clock_base *base; u8 state; u8 is_rel; u8 is_soft; u8 is_hard;};/** * struct hrtimer_sleeper - simple sleeper structure * @timer: embedded timer structure * @task: task to wake up * * task is set to NULL, when the timer expires. */struct hrtimer_sleeper { struct hrtimer timer; struct task_struct *task;}; hrtimer和动态定时器一样拥有超时计数字段_softexpires和超时后的回调函数。 通过比较当前时间是否大于_softexpires来决定定时器超时，执行回调函数。 在红黑树的node中同样保存了一份expires，node.expires大于等于_softexpires。 这样做的目的是在高精度模式下，在_softexpires到node.expires期间设定定时器被触发的时间。 hrtimer管理器结构 123456789101112131415161718192021struct hrtimer_cpu_base { raw_spinlock_t lock; unsigned int cpu; unsigned int active_bases; unsigned int clock_was_set_seq; unsigned int hres_active : 1, in_hrtirq : 1, hang_detected : 1, softirq_activated : 1;#ifdef CONFIG_HIGH_RES_TIMERS unsigned int nr_events; unsigned short nr_retries; unsigned short nr_hangs; unsigned int max_hang_time;#endif ktime_t expires_next; struct hrtimer *next_timer; ktime_t softirq_expires_next; struct hrtimer *softirq_next_timer; struct hrtimer_clock_base clock_base[HRTIMER_MAX_CLOCK_BASES];} ____cacheline_aligned; running 字段指向正在执行的 hrtimer。 next_timer字段指向第一个超时的hrtimer。 clock_base字段是当前CPU维护的定时器树。 目前每个CPU都维护了若干组定时器树，其中包括单条递增时间HRTIMER_BASE_MONOTONIC,上墙时间HRTIMER_BASE_REALTIME等。。 hrtimer有两种精度模式，高精度模式和低精度模式。那么同样的数据设计，如何实现两种精度呢？ 这里的关键就是调用检查hrtimer函数的地方。我们来看看两种精度的调用栈。 低精度 1timer_interrupt() -&gt; update_process_times() -&gt; run_local_timers() -&gt; hrtimer_run_queues() -&gt; __hrtimer_run_queues() 高精度 1hrtimer_interrupt() -&gt; __hrtimer_run_queues() 当hrtimer处于低精度模式时，每次irq0上的时间中断，即每次tick事件，调用一次检查hrtimer函数。 tick的频率是1000hz，此时，hrtimer处于低精度模式。 在第一篇文章中，我们提到TSC、HPET都是可以提供纳秒级的时间设备。 当hrtimer处于高精度模式时，Linux把hrtimer_interrupt()绑定到高精度的时间设备，这时就可以提供纳秒级的定时器服务了。 但是频繁的调用检查hrtimer函数会非常消耗机器性能。 为了避免这个缺陷，hrtimer_interrupt()的调用采用了one-shot的模式。每一次调用都会设定下一次调用的时间。 参考高精度定时器Hrtimer","link":"/2021/03/04/interrupt/hrtimer/"},{"title":"interrupt storm","text":"记录一下hrtimer使用问题导致的 interrupt storm （中断风暴）。 hrtimer使用 只想使用一次 1init -&gt; start -&gt; [return HRTIMER_NORESTART;] 想使用多次，timer 一直存在 1init -&gt; start -&gt; [hrtimer_forward_now; return HRTIMER_NORESTART;] 参考代码 init: 12hrtimer_init( &amp;hr_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);hr_timer.function = &amp;my_hrtimer_callback; start: 1hrtimer_start(&amp;hr_timer, ms_to_ktime(500), HRTIMER_MODE_REL); callback: (产生中断风暴) 12345678910111213141516int once_timer = 1;int interrupt_storm = 0;enum hrtimer_restart my_hrtimer_callback( struct hrtimer *hr_timer){ printk(&quot;my_hrtimer_callback called (%ld).\\n&quot;, jiffies ); if (interrupt_storm) { if (once_timer) { return HRTIMER_NORESTART; } hrtimer_forward_now(hr_timer, ms_to_ktime(500)); } return HRTIMER_RESTART;} 中断风暴安装这个模块 1stable_kernel@kernel: /tmp/share/test_modules/misc/hrtimer# sudo insmod hrtimer.ko 产生如下警告： 12345678[ 1130.893739] rcu: INFO: rcu_sched detected stalls on CPUs/tasks:[ 1130.893750] rcu: 3-...0: (5 GPs behind) idle=b3e/0/0x1 softirq=17149/17149 fqs=6500[ 1208.887926] rcu: INFO: rcu_sched detected stalls on CPUs/tasks:[ 1208.888716] rcu: 3-...0: (5 GPs behind) idle=b3e/0/0x1 softirq=17149/17149 fqs=24703rlk-Standard-PC-i440FX-PIIX-1996 login:rlk-Standard-PC-i440FX-PIIX-1996 login: [ 1286.883226] rcu: INFO: rcu_sched detected stalls on CPUs/tasks:[ 1286.884781] rcu: 3-...0: (5 GPs behind) idle=b3e/0/0x1 softirq=17149/17149 fqs=42700 然后crash,看现场信息 1","link":"/2021/03/01/interrupt/interrupt%20storm/"},{"title":"rcu简介","text":"RCU顾名思义–Read Copy Update，我的理解是：读者直接读，没有任何开销，写者需要先Copy，然后等待合适的时机(宽限期)，去更新 Update。 记得之前有一道笔试题叫 无锁编程？现在想想应该是有以下几类吧 12341. 原子操作2. CAS操作3. 使用rcu编程...... WHY RCU?在RCU的实现过程中，我们主要解决以下问题： 在读取过程中，另外一个线程删除了一个节点。删除线程可以把这个节点从链表中移除，但它不能直接销毁这个节点，必须等到所有的读取线程读取完成以后，才进行销毁操作。RCU中把这个过程称为宽限期（Grace period）。 在读取过程中，另外一个线程插入了一个新节点，而读线程读到了这个节点，那么需要保证读到的这个节点是完整的。这里涉及到了发布-订阅机制（Publish-Subscribe Mechanism）。 保证读取链表的完整性。新增或者删除一个节点，不至于导致遍历一个链表从中间断开。但是RCU并不保证一定能读到新增的节点或者不读到要被删除的节点。 关于 RCU 和 其他锁的性能对比: RCU 的性能优势（scalblity）往往体现在多核上 相对于 rwlock等锁来说，RCU的效率可以高到10 - 100倍 rcu 的编程APIFor reader: 1234rcu_read_lock(void);rcu_read_unlock(void);rcu_read_lock_bh(void);rcu_read_unlock_bh(void); For writer: 123456void call_rcu(struct rcu_head *head, rcu_callback_t func);void synchronize_rcu(void);#define rcu_assign_pointer(p, v)#define rcu_replace_pointer(rcu_ptr, ptr, c)rcu_dereferencercu_read_unlock(void); 和调度器相关的 API 123456static void __sched notrace __schedule(bool preempt){ ...... rcu_note_context_switch(preempt); ......} RCU类型TINY RCU 一般只有arm32 UP系统使用了，使用场景很少。TREE RCU 从4核 到 1024 核心都可以很好的支持。 arm64平台 和 x86平台 defconfig 都是使用的是 TREE RCU。所以重点还是放在 TREE RCU，同时需要关注 SRCU。 CONFIG_PREEMPT_RCU抢占式 RCU含义是？","link":"/2021/02/26/rcu/rcu%E7%AE%80%E4%BB%8B/"},{"title":"arm64 kernel 编译 + vscode解析","text":"最近要换坑位了，下一家公司是做arm64手机芯片的，所以先熟悉一下arm64 kernel 开发环境的配置。 准备工作 安装 aarch64 架构的gcc编译器。 12345678910111213141516Inspiron-5548@100ubuntu: ~/workspace/linux-stable# sudo apt install gcc-aarch64-linux-gnu正在读取软件包列表... 完成正在分析软件包的依赖关系树正在读取状态信息... 完成下列软件包是自动安装的并且现在不需要了： gir1.2-keybinder-3.0 libfprint-2-tod1 libkeybinder-3.0-0 python3-configobj python3-psutil使用'sudo apt autoremove'来卸载它(它们)。将会同时安装下列软件： cpp-aarch64-linux-gnu建议安装： cpp-doc gdb-aarch64-linux-gnu gcc-doc下列【新】软件包将被安装： cpp-aarch64-linux-gnu gcc-aarch64-linux-gnu升级了 0 个软件包，新安装了 2 个软件包，要卸载 0 个软件包，有 80 个软件包未被升级。需要下载 4,852 B 的归档。解压缩后会消耗 56.3 kB 的额外空间。 准备 defconfig 12Inspiron-5548@130ubuntu: ~/workspace/linux-stable# cp arch/arm64/configs/defconfig .configInspiron-5548@ubuntu: ~/workspace/linux-stable# 编译 配置 config make ARCH=arm64 CROSS_COMPILE=/usr/bin/aarch64-linux-gnu- menuconfig 12345678Inspiron-5548@ubuntu: ~/workspace/linux-stable# make ARCH=arm64 CROSS_COMPILE=/usr/bin/aarch64-linux-gnu- menuconfig.config:745:warning: symbol value 'm' invalid for KVMconfiguration written to .config*** End of the configuration.*** Execute 'make' to start the build or try 'make help'.Inspiron-5548@ubuntu: ~/workspace/linux-stable# 编译make ARCH=arm64 CROSS_COMPILE=/usr/bin/aarch64-linux-gnu- all -j4 12345678Inspiron-5548@ubuntu: ~/workspace/linux-stable# make ARCH=arm64 CROSS_COMPILE=/usr/bin/aarch64-linux-gnu- all -j4 O=./out SYNC include/config/auto.conf.cmd HOSTCC scripts/kconfig/conf.o HOSTLD scripts/kconfig/conf WRAP arch/arm64/include/generated/uapi/asm/kvm_para.h WRAP arch/arm64/include/generated/uapi/asm/errno.h ...... 生成compile_commands.json 1234# 4.19 - 5.8 之前可以用./scripts/gen_compile_commands.py -d ./out/# 5.9 之后可以用./scripts/clang-tools/gen_compile_commands.py -d ./out/ 配置 .vscode/c_cpp_properties.json 12345678910{# 4.19 - 5.8 之前可以用 ..... &quot;compileCommands&quot;: &quot;${WorkspaceFolder}/out/compile_commands.json&quot; .....# 5.9 之后可以用 ..... &quot;compileCommands&quot;: &quot;${WorkspaceFolder}/compile_commands.json&quot; .....} 到此为止，基本上在浏览内核代码上不会有困难了","link":"/2021/03/18/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/arm64%20kernel%20%E7%BC%96%E8%AF%91%20+%20vscode%E8%A7%A3%E6%9E%90/"},{"title":"sched concepts","text":"EASwhat? 12This patch series introduces Energy Aware Scheduling (EAS) for CFS taskson platforms with asymmetric CPU topologies (e.g. Arm big.LITTLE). 参考 lkml EAS cover letter参考 lkml EAS overall design V5 misfit应该是指 task运行位置不合适：运行 此 task需要的算力 比 当前 CPU or rq的最大算力还要高，实际代码里面使用的是task需求算力 &lt; （CPU 最大算力 * 1.2）？ task需求算力如何计算：uclamp_task_util(p) 12345678910111213141516171819/* * The margin used when comparing utilization with CPU capacity. * * (default: ~20%) */#define fits_capacity(cap, max) ((cap) * 1280 &lt; (max) * 1024)static inline int task_fits_capacity(struct task_struct *p, long capacity){ return fits_capacity(uclamp_task_util(p), capacity);} if (task_fits_capacity(p, capacity_of(cpu_of(rq)))) { rq-&gt;misfit_task_load = 0; return; } utilization clampingThe main use-cases for utilization clamping are: boosting: better interactive response for small tasks whichare affecting the user experience. Consider for example the case of a small control thread for an externalaccelerator (e.g. GPU, DSP, other devices). Here, from the task utilizationthe scheduler does not have a complete view of what the task’s requirementsare and, if it’s a small utilization task, it keeps selecting a more energyefficient CPU, with smaller capacity and lower frequency, thus negativelyimpacting the overall time required to complete task activations. capping: increase energy efficiency for background tasks not affecting theuser experience. Since running on a lower capacity CPU at a lower frequency is more energyefficient, when the completion time is not a main goal, then capping theutilization considered for certain (maybe big) tasks can have positiveeffects, both on energy consumption and thermal headroom.This feature allows also to make RT tasks more energy friendly on mobilesystems where running them on high capacity CPUs and at the maximumfrequency is not required. 参考 LWN 文章-Add utilization clamping support isolationWHY? 1Dedicating a CPU to a specific performance-critical process/task is desired. 需要将某些CPU 专用于 特定的性能关键的进程/任务。 HOW?需要在boot的时候设置启动参数。四核SMP系统上，加入需要 isolate CPU 0,1,2,3 中的 CPU2 与 CPU3，可以配置 kernel的启动参数 1isolcpus=2,3 完整启动参数是 123456789101112 sudo qemu-system-x86_64 \\-kernel /home/ubuntu/workspace/share/stable/bzImage \\-hda /home/ubuntu/myspace/qemu_build/stable_ubuntu.img \\-append &quot;root=/dev/sda5 console=ttyS0 crashkernel=256M isolcpus=2,3&quot; \\-smp 4 \\-m 4096 \\--enable-kvm \\-net nic \\-net user,hostfwd=tcp::2222-:22 \\--nographic \\-fsdev local,id=fs1,path=/home/ubuntu/workspace/share,security_model=none \\-device virtio-9p-pci,fsdev=fs1,mount_tag=host_share 一旦系统用这个参数启动，进程/任务将不会被分配给或从指定的CPU，除非通过taskset或cset命令分配进程到 isolate的CPU。在Linux上，可以使用taskset命令设置进程的CPU亲和度，使用cset命令设置进程的CPU亲和度。 可以使用 taskset 设置task的 cpu 亲和性查看某个进程的亲和性 12345ubuntu@wsl:~/workspace/linux-stable $ ps -aux | grep sleepubuntu 4445 0.0 0.0 15276 824 tty2 S 11:19 0:00 sleep 1000ubuntu 4448 0.0 0.0 16208 1284 pts/0 S 11:19 0:00 grep --color=auto sleepubuntu@wsl:~/workspace/linux-stable $ taskset -c -p 4445pid 4445's current affinity list: 0-11 设置某个进程的亲和性 123456ubuntu@wsl:~/workspace/linux-stable $ taskset -cp 1 4445pid 4445's current affinity list: 0-11pid 4445's new affinity list: 1ubuntu@wsl:~/workspace/linux-stable $ taskset -c -p 4445pid 4445's current affinity list: 1ubuntu@wsl:~/workspace/linux-stable $ 在server上实际测试结果： 12345测试环境：0-3共有四核 CPU，启动时 isolate出2-3俩核心。1. 开机之后，0-1核 有正常task在跑，2-3核无任何 task。（此时task的 allow_cpumask都是 0-1）2. userspace &amp; kernelspace 的 cfs可以正常绑定到2-3核心，且可以正常跑3. userspace 与 kernelspace 的rt 进程均可绑定到2-3核心，且可以正常跑4. usespace 的 cfs绑定到0-3核心，可以跑到2-3核心上。 参考 Suse 的官方文档","link":"/2021/04/12/schedule/sched%20concepts/"},{"title":"qemu启动的ubuntu img制作","text":"基础img是从哪里来？直接 wget 链接 1wget https://cloud-images.ubuntu.com/focal/20210407/focal-server-cloudimg-arm64.img 参考博客 不替换kernel，如何启动这个kernel ???123456789qemu-system-aarch64 \\ -hda /home/ubuntu/myspace/qemu_build/aarch64/arm64_ubuntu.img \\ -m 2048M \\ -smp 4 \\ -M virt \\ -cpu cortex-a72 \\ -net nic \\ -net user,hostfwd=tcp::2222-:22 \\ -nographic 如何使用这个 img这个img是直接可以启动的 1234567891011qemu-system-aarch64 \\ -hda /home/ubuntu/myspace/qemu_build/aarch64/arm64_ubuntu.img \\ -kernel /home/ubuntu/workspace/linux-stable/out/arch/arm64/boot/Image \\ -append &quot;console=ttyAMA0 root=/dev/vda1&quot; \\ -m 2048M \\ -smp 4 \\ -M virt \\ -cpu cortex-a72 \\ -net nic \\ -net user,hostfwd=tcp::2222-:22 \\ -nographic 但是由于是 cloud img，所以一开始无法使用账号密码登陆，也没法通过ssh登陆。但是可以通过 mount 整个 img，将主机的 ssh的 id_rsa.pub 添加到 cloud img的 authorized_keys中，然后就可以通过 ssh 登陆了。 How to mount a qcow2 disk image- 方法一This is a quick guide to mounting a qcow2 disk images on your host server. This is useful to reset passwords, edit files, or recover something without the virtual machine running. Step 1 - Enable NBD on the Host1modprobe nbd max_part=8 Step 2 - Connect the QCOW2 as network block device1qemu-nbd --connect=/dev/nbd0 /var/lib/vz/images/100/vm-100-disk-1.qcow2 Step 3 - Find The Virtual Machine Partitions1fdisk /dev/nbd0 -l Step 4 - Mount the partition from the VM1mount /dev/nbd0p1 /mnt/somepoint/ Step 5 - After you done, unmount and disconnect123umount /mnt/somepoint/qemu-nbd --disconnect /dev/nbd0rmmod nbd How to mount a qcow2 disk image- 方法二安装 libguestfs-tools1sudo apt-get install libguestfs-tools help libguestfs-tools1234567891011121314151617181920ubuntu@zeku_server:~/workspace/share/simpleapp_hw_breakpoints $ guestmount --helpguestmount: FUSE module for libguestfsguestmount lets you mount a virtual machine filesystemCopyright (C) 2009-2020 Red Hat Inc.Usage: guestmount [--options] mountpointOptions: -a|--add image Add image --blocksize[=512|4096] Set sector size of the disk for -a option -c|--connect uri Specify libvirt URI for -d option --dir-cache-timeout Set readdir cache timeout (default 5 sec) -d|--domain guest Add disks from libvirt guest --echo-keys Don't turn off echo for passphrases --fd=FD Write to pipe FD when mountpoint is ready --format[=raw|..] Force disk format for -a option --fuse-help Display extra FUSE options -i|--inspector Automatically mount filesystems --help Display help message and exit --key selector Specify a LUKS key 挂载1sudo guestmount -a userdata-qemu.img.qcow2 -m /dev/sda qcow2_mount_point 卸载1sudo guestunmount qcow2_mount_point 如何连接 到 img ssh添加key之后，可以直接使用 ssh连接 1ssh -v ubuntu@127.0.0.1 -p 2222 连接之后，可以添加账号密码，修改默认账号密码 账号密码 如何加大 img size在长时间运行 安装软件 编译软件之后， img的size会越来越大，这时候最好可以扩展 img的大小 1qemu-img resize ubuntu.img +10G 如何开启 numa 架构支持1sudo qemu-system-x86_64 -kernel /home/ubuntu/workspace/linux/out/arch/x86/boot/bzImage -hda /home/ubuntu/myspace/qemu_build/stable_ubuntu.img -append 'root=/dev/sda5 console=ttyS0 crashkernel=256M systemd.unified_cgroup_hierarchy=1' -smp cores=4,threads=1,sockets=2 -m 4G -object memory-backend-ram,id=mem0,size=2G -object memory-backend-ram,id=mem1,size=2G -numa node,memdev=mem0,cpus=0-3,nodeid=0 -numa node,memdev=mem1,cpus=4-7,nodeid=1 --enable-kvm -net nic -net user,hostfwd=tcp::2222-:22 --nographic -fsdev local,id=fs1,path=/home/ubuntu/workspace/share,security_model=none -device virtio-9p-pci,fsdev=fs1,mount_tag=host_share","link":"/2021/04/09/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/qemu%E5%90%AF%E5%8A%A8%E7%9A%84ubuntu%20img%E5%88%B6%E4%BD%9C/"},{"title":"Capacity awareness for the deadline scheduler","text":"What is CPU Capacity?What is Capacity awarenessCapacity Awareness refers to the fact that on heterogeneous systems(like Arm big.LITTLE), the capacity of the CPUs is not uniform, hencewhen placing tasks we need to be aware of this difference of CPUcapacities.容量意识是指在异构系统上（例如Arm big.LITTLE），CPU的容量不一致，因此在放置任务时，我们需要意识到CPU的这种差异能力。 RT scheduler 遇到的问题参考sched/rt: Make RT capacity-aware 在ARM big.LITTLE情况下，我们要确保所选的CPU有足够的Capacity，满足运行任务运行的要求，可以理解为： 1capacity_orig_of（cpu）&gt; = task.requirement。 对于CFS来说： 1capacity_orig_of(cpu) &gt;= cfs_task.util 对于 deadline来说：(虽然还没有这个机制，但是可以使用bandwidthreservation实现) 1capacity_orig_of(cpu)/SCHED_CAPACITY &gt;= dl_deadline/dl_runtime 对于RT，我们没有跟踪Per-task的CPU利用率，并且我们没有任何需求 去跟踪 RT-task对性能的要求。 但是随着uclamp的引入，RT任务现在可以控制通过设置uclamp_min来保证最低性能点。 Deadline scheduler 遇到的问题参考LWN文章linux deadline scheduler 将cpu分配给deadline 调度类的task，确保每个task不会逾期，这在SMP系统上很容易办到，但是在arm的 big.LITTLE系统中这变得很复杂，因为一个task假设在big core上需要运行50ms，在LITTLE core上可能需要运行100ms。在scheduler 不能感知到这些 big.LITTLE core的算力差异的时候，deadline的选核操作就有很大问题，会导致一些task被调度到小核上去，task由于小核算力不足导致最后逾期。 deadline schedule中缺少的信息是对CPU容量（CPU Capacity）的了解-在给定时间内可以执行的指令数。CPU capacity已经在 load balancing和其他场景中使用了。 For example: 1231. [Telling the scheduler about thermal pressure](https://lwn.net/Articles/788380/)2. [CPU capacity Usage in rt scheduler](https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=804d402fb6f6) Eggemann’s的工作主要是让 task placement 算法 考虑了不通CPU的 Capacity的差异，加上他的patch之后，deadline scheduler不管是选择到了big core还是 LITTLE core，都会满足 task的算力要求。 具体改了啥？admission-control 主要是基于系统中CPU总算力决定的。 在SMP系统中，单个CPU 容量是c, 系统中有x个cpu，系统总算力C = x * c。 AMP系统中，x个big core，单个容量是c1，y个LITTLE core，单个容量是c2，系统总算力C = x * c1 + y * c2。 deadline scheduler的task-placement 还必须更好地了解系统的CPU拓扑。 在将任务移至新CPU之前，调度程序需要确保新CPU可以处理该任务。 在AMP中，需要有新的方法检查从一个cpu a迁移出的task到 cpu b，b的CPU Capacity满足task的要求，使用以下公式执行 fitness check： 12345678910111213(CPU capacity) / 1024 &gt;= (task runtime) / (task deadline) |cap &gt;&gt; 10 &gt;= (p-&gt;dl.dl_runtime) / (p-&gt;dl.dl_deadline) |(cap &gt;&gt; 10) * p-&gt;dl.dl_deadline &gt;= p-&gt;dl.dl_runtime |(cap * p-&gt;dl.dl_deadline) &gt;&gt; 10 &gt;= p-&gt;dl.dl_runtime |cap_scale(p-&gt;dl.dl_deadline, cap) &gt;= p-&gt;dl.dl_runtime; 对应kernel/sched/sched.h 12345678#define cap_scale(v, s) ((v)*(s) &gt;&gt; SCHED_CAPACITY_SHIFT)static inline bool dl_task_fits_capacity(struct task_struct *p, int cpu){ unsigned long cap = arch_scale_cpu_capacity(cpu); return cap_scale(p-&gt;dl.dl_deadline, cap) &gt;= p-&gt;dl.dl_runtime;} 这个改动会影响啥 waking up a deadline task moving a deadline task to a different CPU migrating a task out of a CPU that is going offline 都会检查 target cpu的 Capacity 是否满足task 的deadline条件。","link":"/2021/04/12/schedule/%E7%BF%BB%E8%AF%91/Capacity%20awareness%20for%20the%20deadline%20scheduler/"},{"title":"freq governer","text":"参考内核文档","link":"/2021/04/15/schedule/freq%20governer/"},{"title":"sched latency + memory direct reclaim","text":"现象UAV在空中悬停，同时下载拍摄的视频到手机中，会导致飞机漂移较多。飞控反馈说是感知的深度图数据频率不对，导致姿态问题。感知深度图传输流程是linux侧的一个 hrtimer 中释放一个信号量给一个线程，驱动内核线程中会处理相关的数据然后给Cortex M核发送一个中断，飞控从中断中获得数据。异构SOC上，Cortex-M核心反馈说 linux上送出去的数据延迟不确定，有时候会很大。 case 首先看了 hrtimer的配置，基本没有问题，排除hrtimer频率不稳定导致的问题 怀疑是 hrtimer中，release sema之后，由于系统调度延迟很大，导致的线程没有及时投入运行，最后导致时间很不稳定 使用 perf sched抓取实际的调度相关数据，发现在未在空中飞行的时候sched latency是一个比较正常的值，由于 perf 运行时的overhead 较大，也不能直接将 perf部署之后飞上天。 寻找了 bytedance的开源工具trace_noschedule，其实就是一个内核模块；原理主要是一个hrtimer一直在采样，可以设置一个阈值a，可以将调度延迟大于多少的进程stack直接记录下来。 部署之后看了调度延迟之后，同场景测试之后，还是会出现飞机漂移，但是没有调度延迟特别高的线程。 只能关注系统其他类型的资源了，比如 memory io等，使用 sar -B 1在实际场景中测试发现他的直接内存回收指标非常高 怀疑是 处理的驱动线程，在处理图像数据的时候，会申请较多内存导致的内存分配延迟较高，最后处理深度图时间从原来2ms，直接延长到10ms以上，导致linux -&gt; rtos中断不及时。 bpftrace vmscan_direct_reclaim_[begin,end] 相关接口，果然是 触发了内存直接回收，且看时间戳，延迟时间有10ms以上，找到问题根源 可以调整 vm_free_kbytes，使得 min 与 low水位之间间距变大，更早触发 kswapd 内存回收，更难触发 direct reclaim。","link":"/2021/02/22/mycase/sched%20latency%20+%20memory%20direct%20reclaim/"},{"title":"softlockup + rcu + oom","text":"现象产生oom，业务A被干掉了 case 根据oom现场 dump的信息查看 各个进程都没有占用太多内存，排除用户空间进程内存泄漏 查看 slab 信息，发现确实有某些slab 变得很大，发现是图传驱动创建的一个slab。 严重 怀疑是 slab 内存泄漏，最后导致的内存泄漏，排查相关create alloc free 代码之后，没什么收获。 使用 bpftrace 跟踪 申请释放的kprobe点，跟踪几分钟，十几分钟，看 输出几乎是成对出现的，应该不存在 slab内存泄漏 查看驱动自己封装的接口，发现释放slab 是直接使用 call_rcu()的一个接口，怀疑可能是跟 call_rcu接口有关系。 rcu 涉及到 宽限期的问题，怀疑可能是某个 cpu 发生了 softlockup 或者不调度，部署panic_on_oom 抓取现场。 看到确实有 一个驱动线程 一直占用cpu,看这个线程的 backtrace发现是在一个驱动的重新初始化上 一直 while，且之前获取过 spin_lock()，这样就基本构成了发生 softlockup的必要条件，之所以没有看到softlockup的原因就是时间还没到。 到这一步，大概基本80～90%怀疑是 驱动代码 + 外设导致的 softlockup，最后导致rcu的宽限期不能通过，导致slab不能延迟释放，最后导致的 oom。 然后尝试手动触发一个 softlockup，来尝试复现这个问题，但是一直只能的到 softlockup的警告，并不能复现 oom的情况？ 最后发现slab积累 释放的速率 和 业务场景有很大关系，研发机器经常烧录，在烧录之后参数区丢失，导致图传模式一直是默认模式，slab申请释放速率较低，但是QA都是OTA升级，一般使用 高画质，slab 申请释放 会比默认模式快很多，这也是为什么我本地不能复现这个 oom bug的原因。 大概流程是： 123456789101112131415161718192021222324252627282930313233343536+---------------------------------+| 驱动先spin_lock之后，while等待 || 一个初始化信号，由于外设异常，导致 || 一直在while中,无法退出 |+-----------+---------------------+ | | | | v+-----------+----------------------------+| || 具备产生 softlockup 的条件, || 但是时间还是比较短，所以还未发生 || softlockup |+----------+-----------------------------+ | | |+----------v-----------------------------+| || 系统中rcu的宽限期过不了， || 导致业务驱动使用频繁的slab积累 || 太多看起来和slab内存泄漏一样 || |+---------+------------------------------+ | | | |+---------v-------------------+| || 系统内存不足，导致oom || |+-----------------------------+","link":"/2021/02/25/mycase/softlockup%20+%20rcu%20+%20oom/"},{"title":"ipi_sched","text":"WHAT?IPI 中断 (Inter-Processor Interrupts) ，核间中断可以通过向这个寄存器写入需要的值来产生。若硬件线程 A 想要发送一个核间中断给硬件线程 B，它只需要向寄存器 IPIBase 中写入 B的 Thread ID、中断向量、中断类型等值就可以了，PIC 会通知 B 所在的核挂起它当前的执行序列，并根据中断向量跳转到中断服务例程 ISR 的入口 HOW?linux上将 IPI 中断分为以下几类 123456789enum ipi_msg_type { IPI_RESCHEDULE, IPI_CALL_FUNC, IPI_CPU_STOP, IPI_CPU_CRASH_STOP, IPI_TIMER, IPI_IRQ_WORK, IPI_WAKEUP}; 通过接口 smp_cross_call 调用，For example: 1234void smp_send_reschedule(int cpu){ smp_cross_call(cpumask_of(cpu), IPI_RESCHEDULE);} CPUa 给 CPUb 发送一个 IPI_RESCHEDULE 信号，然后 CPUb执行相关函数 1234567891011121314151617181920212223/* * Main handler for inter-processor interrupts */void handle_IPI(int ipinr, struct pt_regs *regs){ unsigned int cpu = smp_processor_id(); struct pt_regs *old_regs = set_irq_regs(regs); switch (ipinr) { case IPI_RESCHEDULE: scheduler_ipi(); break; case IPI_CALL_FUNC: irq_enter(); generic_smp_call_function_interrupt(); irq_exit(); break; } ...... set_irq_regs(old_regs);} scheduler_ipi 主要工作？12345678910111213141516171819202122232425262728293031323334353637void scheduler_ipi(void){ /* * Fold TIF_NEED_RESCHED into the preempt_count; anybody setting * TIF_NEED_RESCHED remotely (for the first time) will also send * this IPI. */ preempt_fold_need_resched(); if (llist_empty(&amp;this_rq()-&gt;wake_list) &amp;&amp; !got_nohz_idle_kick()) return; /* * Not all reschedule IPI handlers call irq_enter/irq_exit, since * traditionally all their work was done from the interrupt return * path. Now that we actually do some work, we need to make sure * we do call them. * * Some archs already do call them, luckily irq_enter/exit nest * properly. * * Arguably we should visit all archs and update all handlers, * however a fair share of IPIs are still resched only so this would * somewhat pessimize the simple resched case. */ irq_enter(); sched_ttwu_pending(); /* * Check if someone kicked us for doing the nohz idle load balance. */ if (unlikely(got_nohz_idle_kick())) { this_rq()-&gt;idle_balance = 1; raise_softirq_irqoff(SCHED_SOFTIRQ); } irq_exit();} 如果 llist_empty(&amp;this_rq()-&gt;wake_list) 成立，意味着rq上没有需要wake_up的thread，且同时 !got_nohz_idle_kick()也成立的话，就直接返回了，这个 IPI就是无效的。 正常都会继续往下继续走，重点是 sched_ttwu_pending 123456789101112131415161718void sched_ttwu_pending(void){ struct rq *rq = this_rq(); struct llist_node *llist = llist_del_all(&amp;rq-&gt;wake_list); struct task_struct *p, *t; struct rq_flags rf; if (!llist) return; rq_lock_irqsave(rq, &amp;rf); update_rq_clock(rq); // 一次性唤醒所有 `wake_list` 上的task。 llist_for_each_entry_safe(p, t, llist, wake_entry) ttwu_do_activate(rq, p, p-&gt;sched_remote_wakeup ? WF_MIGRATED : 0, &amp;rf); rq_unlock_irqrestore(rq, &amp;rf);} 然后查看是否需要做 load_balance 什么情况下会发送 IPI_RESCHEDULE？ resched_curr()如果cpu == smp_processor_id()，(这里分为俩种情况，1. UP架构只有一个CPU 2. SMP架构上恰好resched_curr()的是本cpu)那仅仅是设置 当前 curr的thread_info的 need_resched标志位。如果是需要其他cpu执行 resched_curr()，就需要 smp_send_reschedule(cpu) 来发送 IPI_RESCHEDULE了。123456789101112131415161718192021222324252627282930/* * resched_curr - mark rq's current task 'to be rescheduled now'. * * On UP this means the setting of the need_resched flag, on SMP it * might also involve a cross-CPU call to trigger the scheduler on * the target CPU. */void resched_curr(struct rq *rq){ struct task_struct *curr = rq-&gt;curr; int cpu; lockdep_assert_held(&amp;rq-&gt;lock); if (test_tsk_need_resched(curr)) return; cpu = cpu_of(rq); if (cpu == smp_processor_id()) { set_tsk_need_resched(curr); set_preempt_need_resched(); return; } if (set_nr_and_not_polling(curr)) smp_send_reschedule(cpu); else trace_sched_wake_idle_without_ipi(cpu);} wake_up_idle_cpu() 在唤醒一个 idle的cpu时， 123456789101112static void wake_up_idle_cpu(int cpu){ struct rq *rq = cpu_rq(cpu); if (cpu == smp_processor_id()) return; if (set_nr_and_not_polling(rq-&gt;idle)) smp_send_reschedule(cpu); else trace_sched_wake_idle_without_ipi(cpu);}s kick_process() 让一个 task立刻进入 kernel mode（without any delay），从而来处理 signal信号。 1234567891011121314151617181920212223242526272829303132333435363738/*** * kick_process - kick a running thread to enter/exit the kernel * @p: the to-be-kicked thread * * Cause a process which is running on another CPU to enter * kernel-mode, without any delay. (to get signals handled.) * * NOTE: this function doesn't have to take the runqueue lock, * because all it wants to ensure is that the remote task enters * the kernel. If the IPI races and the task has been migrated * to another CPU then no harm is done and the purpose has been * achieved as well. */void kick_process(struct task_struct *p){ int cpu; preempt_disable(); cpu = task_cpu(p); if ((cpu != smp_processor_id()) &amp;&amp; task_curr(p)) smp_send_reschedule(cpu); preempt_enable();}EXPORT_SYMBOL_GPL(kick_process);void signal_wake_up_state(struct task_struct *t, unsigned int state){ set_tsk_thread_flag(t, TIF_SIGPENDING); /* * TASK_WAKEKILL also means wake it up in the stopped/traced/killable * case. We don't check t-&gt;state here because there is a race with it * executing another processor and just now entering stopped state. * By using wake_up_state, we ensure the process will wake up and * handle its death signal. */ if (!wake_up_state(t, state | TASK_INTERRUPTIBLE)) kick_process(t);} ttwu_queue_remote() try_to_wake_up() 一个thread之后，需要将 这个task入队a. 如果是 task不是在 本cpu 唤醒的，那就需要IPI中断smp_send_reschedule来搞了b. 如果是在 本地cpu 唤醒的，就直接 lock_rq() 之后进行 ttwu_do_activate 1234567891011121314151617181920212223242526272829303132static void ttwu_queue_remote(struct task_struct *p, int cpu, int wake_flags){ struct rq *rq = cpu_rq(cpu); p-&gt;sched_remote_wakeup = !!(wake_flags &amp; WF_MIGRATED); if (llist_add(&amp;p-&gt;wake_entry, &amp;cpu_rq(cpu)-&gt;wake_list)) { if (!set_nr_if_polling(rq-&gt;idle)) smp_send_reschedule(cpu); else trace_sched_wake_idle_without_ipi(cpu); }}static void ttwu_queue(struct task_struct *p, int cpu, int wake_flags){ struct rq *rq = cpu_rq(cpu); struct rq_flags rf;#if defined(CONFIG_SMP) if (sched_feat(TTWU_QUEUE) &amp;&amp; !cpus_share_cache(smp_processor_id(), cpu)) { sched_clock_cpu(cpu); /* Sync clocks across CPUs */ ttwu_queue_remote(p, cpu, wake_flags); return; }#endif rq_lock(rq, &amp;rf); update_rq_clock(rq); ttwu_do_activate(rq, p, wake_flags, &amp;rf); rq_unlock(rq, &amp;rf);} wake_up_if_idle()wake_up_if_idle() 只有在 wake_up_all_idle_cpus() 中被调用，主要是看此 cpu 是否idle空闲，就是看他 is_idle_task(cpu_rq(cpu)-&gt;curr) 是否成立，成立的话就需要 IPI中断去唤醒。 12345678910111213141516171819202122232425262728293031323334353637void wake_up_if_idle(int cpu){ struct rq *rq = cpu_rq(cpu); struct rq_flags rf; rcu_read_lock(); if (!is_idle_task(rcu_dereference(rq-&gt;curr))) goto out; if (set_nr_if_polling(rq-&gt;idle)) { trace_sched_wake_idle_without_ipi(cpu); } else { rq_lock_irqsave(rq, &amp;rf); if (is_idle_task(rq-&gt;curr)) smp_send_reschedule(cpu); /* Else CPU is not idle, do nothing here: */ rq_unlock_irqrestore(rq, &amp;rf); }out: rcu_read_unlock();}void wake_up_all_idle_cpus(void){ int cpu; preempt_disable(); for_each_online_cpu(cpu) { if (cpu == smp_processor_id()) continue; wake_up_if_idle(cpu); } preempt_enable();} 參考linux-5.4.61代码","link":"/2021/04/21/schedule/ipi_sched/"},{"title":"pelt","text":"参考wowo文章 WHY?为了让调度器更加的聪明，我们总是希望系统满足最大吞吐量同时又最大限度的降低功耗。虽然可能有些矛盾，但是现实总是这样。PELT算法是Linux 3.8合入的，那么在此之前，我们存在什么问题才引入PELT算法呢？在Linux 3.8之前，CFS以每个运行队列（runqueue，简称rq）为基础跟踪负载。但是这种方法，我们无法确定当前负载的来源。同时，即使工作负载相对稳定的情况下，在rq级别跟踪负载，其值也会产生很大变化。为了解决以上的问题，PELT算法会跟踪每个调度实体（per-scheduling entity）的负载情况。 HOW?为了做到Per-entity的负载跟踪，时间（物理时间，不是虚拟时间）被分成了1024us的序列，在每一个1024us的周期中，一个entity对系统负载的贡献可以根据该实体处于runnable状态（正在CPU上运行或者等待cpu调度运行）的时间进行计算。 如果在该周期内，runnable的时间是x，那么对系统负载的贡献就是（x/1024）。当然，一个实体在一个计算周期内的负载可能会超过1024us，这是因为我们会累积在过去周期中的负载，当然，对于过去的负载我们在计算的时候需要乘一个衰减因子。如果我们让Li表示在周期pi中该调度实体的对系统负载贡献，那么一个调度实体对系统负荷的总贡献可以表示为： 123L = L0 + L1 * y + L2 * y2 + L3 * y3 + ... + Ln * yn其中：y32 = 0.5, y = 0.97857206 如果有一个task，从第一次加入rq后开始一直运行4096us后一直睡眠，那么在1023us、2047us、3071us、4095us、5119us、6143us、7167us和8191us时间的每一个时刻负载贡献分别是多少呢？ / | 1|———————–| | | 0| |————————————————————————————————————————-&gt; | | | | | | | | 1023 2047 4095 5119 7167 8191 可以计算： 1234L0 = 1023 * y0 = 1023L1 = 1023 * y0 + 1024 * y1 = 1023 + (L0 + 1) * y = 2025L2 = 1023 * y0 + 1024 * y1 + 1024 * y2 = 1023 + (L1 + 1) * y1 = 3005...... 可以得到： 1Ln = Load(This) + Ln-1 * y = 当前周期负载 + 上一周期负载 * 衰减系数y 需要经常计算 Ln-1 * y的值，但是内核并不是 简单计算 Ln-1 * y，也可能需要计算Ln-m * ym，kernel中使用decay_laod()函数。为了提高精度和计算速度，采用乘法 和 位移算法进行计算，输入是: val: 前n个负载计算周期的负载 n: 衰减n个周期之后的实际负载123decay_load(val, n) = val * yn = (val * yn) * (232 &gt;&gt;32) = val * (yn * 232) &gt;&gt; 32 可以看到 yn * 232的值其实是固定的（n确定的情况下是一个数组）runnable_avg_yN_inv[n] = yn*232, n &gt; 0 &amp;&amp; n &lt; 32kernel中使用了一个数组来保存这些值： 12345678static const u32 runnable_avg_yN_inv[] = { 0xffffffff, 0xfa83b2da, 0xf5257d14, 0xefe4b99a, 0xeac0c6e6, 0xe5b906e6, 0xe0ccdeeb, 0xdbfbb796, 0xd744fcc9, 0xd2a81d91, 0xce248c14, 0xc9b9bd85, 0xc5672a10, 0xc12c4cc9, 0xbd08a39e, 0xb8fbaf46, 0xb504f333, 0xb123f581, 0xad583ee9, 0xa9a15ab4, 0xa5fed6a9, 0xa2704302, 0x9ef5325f, 0x9b8d39b9, 0x9837f050, 0x94f4efa8, 0x91c3d373, 0x8ea4398a, 0x8b95c1e3, 0x88980e80, 0x85aac367, 0x82cd8698,}; 下面是decay_load实际代码： 如果衰减周期大于 LOAD_AVG_PERIOD * 63，那么我们认为LOAD_AVG_PERIOD * 63之前的负载val对现在的贡献是 0 如果n &gt;= 32，由于 runnable_avg_yN_inv 只包含了y31的值，所以 必须将n 归一化到 &lt; 32，每归一化一次，负载贡献就减半 (val = val &gt;&gt; 1) 最后衰减的负载是 val * runnable_avg_yN_inv[local_n] &gt;&gt; 321234567891011121314151617181920212223242526272829/* * Approximate: * val * y^n, where y^32 ~= 0.5 (~1 scheduling period) */static u64 decay_load(u64 val, u64 n){ unsigned int local_n; if (unlikely(n &gt; LOAD_AVG_PERIOD * 63)) return 0; /* after bounds checking we can collapse to 32-bit */ local_n = n; /* * As y^PERIOD = 1/2, we can combine * y^n = 1/2^(n/PERIOD) * y^(n%PERIOD) * With a look-up table which covers y^n (n&lt;PERIOD) * * To achieve constant time decay_load. */ if (unlikely(local_n &gt;= LOAD_AVG_PERIOD)) { val &gt;&gt;= local_n / LOAD_AVG_PERIOD; local_n %= LOAD_AVG_PERIOD; } val = mul_u64_u32_shr(val, runnable_avg_yN_inv[local_n], 32); return val;} 负载信息如何记录？首先负载计算是相对于调度实体se 和 就绪队列rq的：对于 struct sched_entity struct cfs_rq struct rq 结构来说都内嵌了 struct sched_avg 结构。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051struct sched_avg { u64 last_update_time; // 上一次负载更新的时间 u64 load_sum; // running + runnable 的负载 u64 runnable_load_sum;// 姑且等于 load_sum u32 util_sum; // running 的负载 u32 period_contrib; unsigned long load_avg; // running + runnable 的平均负载 unsigned long runnable_load_avg; // 姑且等于 load_avg unsigned long util_avg; // running 的平均负载};struct sched_entity { struct load_weight load; unsigned long runnable_weight;#ifdef CONFIG_SMP /* * Per entity load average tracking. * * Put into separate cache line so it does not * collide with read-mostly values above. */ struct sched_avg avg;#endif};struct cfs_rq { struct load_weight load; unsigned long runnable_weight;#ifdef CONFIG_SMP /* * CFS load tracking */ struct sched_avg avg;#ifndef CONFIG_64BIT};struct rq { /* runqueue lock: */ raw_spinlock_t lock; /* * nr_running and cpu_load should be in the same cacheline because * remote CPUs use both these fields when doing load calculation. */ unsigned int nr_running; struct sched_avg avg_rt; struct sched_avg avg_dl;#ifdef CONFIG_HAVE_SCHED_AVG_IRQ struct sched_avg avg_irq;#endif}; 通过init可以更清楚知道他们含义，如果se是： task，那么 runnable_load_avg = load_avg 都和 se的权重相等 group se，runnable_load_avg = load_avg = 0，也反映了此时group se还没有任何任务。12345678910111213141516171819void init_entity_runnable_average(struct sched_entity *se){ struct sched_avg *sa = &amp;se-&gt;avg; memset(sa, 0, sizeof(*sa)); /* * Tasks are initialized with full load to be seen as heavy tasks until * they get a chance to stabilize to their real load level. * Group entities are initialized with zero load to reflect the fact that * nothing has been attached to the task group yet. */ if (entity_is_task(se)) sa-&gt;runnable_load_avg = sa-&gt;load_avg = scale_load_down(se-&gt;load.weight); se-&gt;runnable_weight = se-&gt;load.weight; /* when this task enqueue'ed, it will contribute to its cfs_rq's load_avg */} 那么如何计算当前se的负载？假设一个task从0时刻一直开始运行，在1022us时刻负载是多少？由于还没有之前运行周期可以凑满1024us 一个衰减周期，所以负载是1022，又运行了10us之后负载应该如何计算？ 123L = (10 - (1024 - 1022)) +(1024 - 1022 + 1022)y = (10 -2) + (1022 + 2) * y1 = 8 + 1024 * y1 假设上一时刻负载贡献是u，经历d时间后的负载贡献如何计算呢？根据上面的例子，我们可以把时间d分成3和部分：d1是离当前时间最远（不完整的）period 的剩余部分，d2 是完整period时间，而d3是（不完整的）当前 period 的剩余部分。假设时间d是经过p个周期（d=d1+d2+d3, p=1+d2/1024）。d1，d2，d3 的示意图如下： 1234567891011121314151617 d1 d2 d3 ^ ^ ^ | | | |&lt;-&gt;|&lt;-----------------&gt;|&lt;---&gt;||---x---|------| ... |------|-----x (now) | u时刻u' = (u + d1) y^p + 1024 * (y^1 + y^2 + ... + y^p-1) + d3 * y^0 p-1 = (u + d1) y^p + 1024 \\Sum y^n + d3 y^0 n=1 = u y^p + ---&gt; Step1 p-1 = d1 y^p + 1024 \\Sum y^n + d3 y^0 ---&gt; Step2 n=1 kernel中是用 accumulate_sum() 实现这个当前时刻负载计算的 period_contrib记录的是上次更新负载不足1024us周期的时间。delta是经过的时间，为了计算经过的周期个数需要加上period_contrib，然后整除1024。 计算周期个数 调用decay_load()函数计算公式中的step1部分 __accumulate_pelt_segments()负责计算公式step2部分 更新period_contrib为本次不足1024us部分123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081static u32 __accumulate_pelt_segments(u64 periods, u32 d1, u32 d3){ u32 c1, c2, c3 = d3; /* y^0 == 1 */ /* * c1 = d1 y^p */ c1 = decay_load((u64)d1, periods); /* * p-1 * c2 = 1024 \\Sum y^n * n=1 * * inf inf * = 1024 ( \\Sum y^n - \\Sum y^n - y^0 ) * n=0 n=p */ c2 = LOAD_AVG_MAX - decay_load(LOAD_AVG_MAX, periods) - 1024; return c1 + c2 + c3;}/* * Accumulate the three separate parts of the sum; d1 the remainder * of the last (incomplete) period, d2 the span of full periods and d3 * the remainder of the (incomplete) current period. * * d1 d2 d3 * ^ ^ ^ * | | | * |&lt;-&gt;|&lt;-----------------&gt;|&lt;---&gt;| * ... |---x---|------| ... |------|-----x (now) * * p-1 * u' = (u + d1) y^p + 1024 \\Sum y^n + d3 y^0 * n=1 * * = u y^p + (Step 1) * * p-1 * d1 y^p + 1024 \\Sum y^n + d3 y^0 (Step 2) * n=1 */static __always_inline u32accumulate_sum(u64 delta, struct sched_avg *sa, unsigned long load, unsigned long runnable, int running){ u32 contrib = (u32)delta; /* p == 0 -&gt; delta &lt; 1024 */ u64 periods; delta += sa-&gt;period_contrib; periods = delta / 1024; /* A period is 1024us (~1ms) */ /* * Step 1: decay old *_sum if we crossed period boundaries. */ if (periods) { sa-&gt;load_sum = decay_load(sa-&gt;load_sum, periods); sa-&gt;runnable_load_sum = decay_load(sa-&gt;runnable_load_sum, periods); sa-&gt;util_sum = decay_load((u64)(sa-&gt;util_sum), periods); /* * Step 2 */ delta %= 1024; contrib = __accumulate_pelt_segments(periods, 1024 - sa-&gt;period_contrib, delta); } sa-&gt;period_contrib = delta; if (load) sa-&gt;load_sum += load * contrib; if (runnable) sa-&gt;runnable_load_sum += runnable * contrib; if (running) sa-&gt;util_sum += contrib &lt;&lt; SCHED_CAPACITY_SHIFT; return periods;} 其中 ___update_load_sum() 函数 计算的都是sum负载总和，函数返回值是 0，如果和上次更新时间之间 没有 经历过一次完整的period 1，如果和上次更新时间之间 经历过一次完整的period 如果经历过一个 full period之后，就需要更新 load_avg。 1234567891011121314151617181920212223242526static __always_inline void___update_load_avg(struct sched_avg *sa, unsigned long load, unsigned long runnable){ u32 divider = LOAD_AVG_MAX - 1024 + sa-&gt;period_contrib; /* * Step 2: update *_avg. */ sa-&gt;load_avg = div_u64(load * sa-&gt;load_sum, divider); sa-&gt;runnable_load_avg = div_u64(runnable * sa-&gt;runnable_load_sum, divider); WRITE_ONCE(sa-&gt;util_avg, sa-&gt;util_sum / divider);}int __update_load_avg_se(u64 now, struct cfs_rq *cfs_rq, struct sched_entity *se){ if (___update_load_sum(now, &amp;se-&gt;avg, !!se-&gt;on_rq, !!se-&gt;on_rq, cfs_rq-&gt;curr == se)) { ___update_load_avg(&amp;se-&gt;avg, se_weight(se), se_runnable(se)); cfs_se_util_change(&amp;se-&gt;avg); trace_pelt_se_tp(se); return 1; } return 0;} |&lt;--1024--&gt;| |———————————————|—|——| |-|-| period_contrib divider = LOAD_AVG_MAX - (1024 - period_contrib) = LOAD_AVG_MAX - 1024 + period_contrib 可以大概理解： 12load_avg = load_weight * (load_sum / time_sum)util_avg = util_sum / time_sum 当一个task一直运行，负载足够高时，可以认为 1234load_sum == time_sumutil_sum == time_sumload_avg = load_weightutil_avg = 1 如何计算当前cfs_rq的负载我们跟踪se的负载，更多的想更精确的知道每时每刻 rq上有负载变化，从而对scheduler的行为作出指导。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677int __update_load_avg_cfs_rq(u64 now, struct cfs_rq *cfs_rq){ if (___update_load_sum(now, &amp;cfs_rq-&gt;avg, scale_load_down(cfs_rq-&gt;load.weight), scale_load_down(cfs_rq-&gt;runnable_weight), cfs_rq-&gt;curr != NULL)) { ___update_load_avg(&amp;cfs_rq-&gt;avg, 1, 1); trace_pelt_cfs_tp(cfs_rq); return 1; } return 0;}/** * update_cfs_rq_load_avg - update the cfs_rq's load/util averages * @now: current time, as per cfs_rq_clock_pelt() * @cfs_rq: cfs_rq to update * * The cfs_rq avg is the direct sum of all its entities (blocked and runnable) * avg. The immediate corollary is that all (fair) tasks must be attached, see * post_init_entity_util_avg(). * * cfs_rq-&gt;avg is used for task_h_load() and update_cfs_share() for example. * * Returns true if the load decayed or we removed load. * * Since both these conditions indicate a changed cfs_rq-&gt;avg.load we should * call update_tg_load_avg() when this function returns true. */ /* * cfs_rq avg 是 cfs_rq中所有 entities 的 sum。 * 返回值：1 如果 有线程被移出这个 cfs_rq 或者经过了一个衰减周期 * 0 */static inline intupdate_cfs_rq_load_avg(u64 now, struct cfs_rq *cfs_rq){ unsigned long removed_load = 0, removed_util = 0, removed_runnable_sum = 0; struct sched_avg *sa = &amp;cfs_rq-&gt;avg; int decayed = 0; if (cfs_rq-&gt;removed.nr) { unsigned long r; u32 divider = LOAD_AVG_MAX - 1024 + sa-&gt;period_contrib; raw_spin_lock(&amp;cfs_rq-&gt;removed.lock); swap(cfs_rq-&gt;removed.util_avg, removed_util); swap(cfs_rq-&gt;removed.load_avg, removed_load); swap(cfs_rq-&gt;removed.runnable_sum, removed_runnable_sum); cfs_rq-&gt;removed.nr = 0; raw_spin_unlock(&amp;cfs_rq-&gt;removed.lock); r = removed_load; sub_positive(&amp;sa-&gt;load_avg, r); sub_positive(&amp;sa-&gt;load_sum, r * divider); r = removed_util; sub_positive(&amp;sa-&gt;util_avg, r); sub_positive(&amp;sa-&gt;util_sum, r * divider); add_tg_cfs_propagate(cfs_rq, -(long)removed_runnable_sum); decayed = 1; } decayed |= __update_load_avg_cfs_rq(now, cfs_rq);#ifndef CONFIG_64BIT smp_wmb(); cfs_rq-&gt;load_last_update_time_copy = sa-&gt;last_update_time;#endif return decayed;}","link":"/2021/04/24/schedule/pelt/"},{"title":"sched_domain and root_domain","text":"root_domainroot_domain 相对于 sched_domain 来说简单很多， 这是描述一个cpu集合的 数据结构。每个 cpuset 都会拥有一个 root_domain，无论何时何地 cpuset被创建，同时会创建 并 attach 一个 root_domain结构。一个 root_domain 可能包含多个cpu，且这些cpu的能效不一定是一致的，所以内嵌了perf_domain 结构来描述这种差异。 perf_domain 的next 指针 就这样将这个root_domain 中的perf_domain 连接起来了。具有同一能效模型的俩cpu(在同一cluster上)，共享一个 em_perf_domain 能效模型结构，但是 perf_domain可能是分开的，因为这俩cpu可能是属于不同root_domain的。 参考 kernel 文档 1234567891011121314151617The lists are attached to the root domains in order to cope with exclusive cpuset configurations. Since the boundaries of exclusive cpusets do not necessarily match those of performance domains, the lists of different root domains can contain duplicate elements.Example 1.Let us consider a platform with 12 CPUs, split in 3 performance domains (pd0, pd4 and pd8), organized as follows:CPUs: 0 1 2 3 4 5 6 7 8 9 10 11PDs: |--pd0--|--pd4--|---pd8---|RDs: |----rd1----|-----rd2-----|Now, consider that userspace decided to split the system with two exclusive cpusets, hence creating two independent root domains, each containing 6 CPUs. The two root domains are denoted rd1 and rd2 in the above figure. Since pd4 intersects with both rd1 and rd2, it will be present in the linked list ‘-&gt;pd’ attached to each of them:rd1-&gt;pd: pd0 -&gt; pd4rd2-&gt;pd: pd4 -&gt; pd8Please note that the scheduler will create two duplicate list nodes for pd4 (one for each list). However, both just hold a pointer to the same shared data structure of the EM framework.Since the access to these lists can happen concurrently with hotplug and other things, they are protected by RCU, like the rest of topology structures manipulated by the scheduler.EAS also maintains a static key (sched_energy_present) which is enabled when at least one root domain meets all conditions for EAS to start. Those conditions are summarized in Section 6. 这是 root_domain 实现： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768struct perf_domain { struct em_perf_domain *em_pd; struct perf_domain *next; struct rcu_head rcu;};/* * We add the notion of a root-domain which will be used to define per-domain * variables. Each exclusive cpuset essentially defines an island domain by * fully partitioning the member CPUs from any other cpuset. Whenever a new * exclusive cpuset is created, we also create and attach a new root-domain * object. * */struct root_domain { atomic_t refcount; atomic_t rto_count; struct rcu_head rcu; cpumask_var_t span; cpumask_var_t online; /* * Indicate pullable load on at least one CPU, e.g: * - More than one runnable task * - Running task is misfit */ int overload; /* Indicate one or more cpus over-utilized (tipping point) */ int overutilized; /* * The bit corresponding to a CPU gets set here if such CPU has more * than one runnable -deadline task (as it is below for RT tasks). */ cpumask_var_t dlo_mask; atomic_t dlo_count; struct dl_bw dl_bw; struct cpudl cpudl;#ifdef HAVE_RT_PUSH_IPI /* * For IPI pull requests, loop across the rto_mask. */ struct irq_work rto_push_work; raw_spinlock_t rto_lock; /* These are only updated and read within rto_lock */ int rto_loop; int rto_cpu; /* These atomics are updated outside of a lock */ atomic_t rto_loop_next; atomic_t rto_loop_start;#endif /* * The &quot;RT overload&quot; flag: it gets set if a CPU has more than * one runnable RT task. */ cpumask_var_t rto_mask; struct cpupri cpupri; unsigned long max_cpu_capacity; /* * NULL-terminated list of performance domains intersecting with the * CPUs of the rd. Protected by RCU. */ struct perf_domain __rcu *pd;}; sched domain: WHAT?sched domain: 如何标识？include/linux/sched/sd_flags.h 123#define SDF_SHARED_CHILD 0x1#define SDF_SHARED_PARENT 0x2#define SDF_NEEDS_GROUPS 0x4 12345678910111213SD_FLAG(SD_BALANCE_NEWIDLE, SDF_SHARED_CHILD | SDF_NEEDS_GROUPS)SD_FLAG(SD_BALANCE_EXEC, SDF_SHARED_CHILD | SDF_NEEDS_GROUPS)SD_FLAG(SD_BALANCE_FORK, SDF_SHARED_CHILD | SDF_NEEDS_GROUPS)SD_FLAG(SD_BALANCE_WAKE, SDF_SHARED_CHILD | SDF_NEEDS_GROUPS)SD_FLAG(SD_WAKE_AFFINE, SDF_SHARED_CHILD)SD_FLAG(SD_ASYM_CPUCAPACITY, SDF_SHARED_PARENT | SDF_NEEDS_GROUPS)SD_FLAG(SD_SHARE_CPUCAPACITY, SDF_SHARED_CHILD | SDF_NEEDS_GROUPS)SD_FLAG(SD_SHARE_PKG_RESOURCES, SDF_SHARED_CHILD | SDF_NEEDS_GROUPS)SD_FLAG(SD_SERIALIZE, SDF_SHARED_PARENT | SDF_NEEDS_GROUPS)SD_FLAG(SD_ASYM_PACKING, SDF_SHARED_CHILD | SDF_NEEDS_GROUPS)SD_FLAG(SD_PREFER_SIBLING, SDF_NEEDS_GROUPS)SD_FLAG(SD_OVERLAP, SDF_SHARED_PARENT | SDF_NEEDS_GROUPS)SD_FLAG(SD_NUMA, SDF_SHARED_PARENT | SDF_NEEDS_GROUPS)","link":"/2021/04/12/schedule/sched_domain%20and%20root_domain/"},{"title":"util_est","text":"WHAT?struct util_est - Estimation utilization of FAIR tasks这是一个评估 fair task的 utilization的结构 123456It's worth noting that the estimated utilization is tracked only for objects of interests, specifically: - Tasks: to better support tasks placement decisions - root cfs_rqs: to better support both tasks placement decisions as well as frequencies selection 12345678910Moreover, the PELT utilization of a task can be updated every [ms], thus making it a continuously changing value for certain longer running tasks. This means that the instantaneous PELT utilization of a RUNNING task is not really meaningful to properly support scheduler decisions. For all these reasons, a more stable signal can do a better job of representing the expected/estimated utilization of a task/cfs_rq. Such a signal can be easily created on top of PELT by still using it as an estimator which produces values to be aggregated on meaningful events. 数据结构123456789101112131415161718192021222324252627/** * struct util_est - Estimation utilization of FAIR tasks * @enqueued: instantaneous estimated utilization of a task/cpu -- task的 瞬时的 util * @ewma: the Exponential Weighted Moving Average (EWMA) * utilization of a task -- task的 EWMA（移动加权平均） 的 util * * Support data structure to track an Exponential Weighted Moving Average * (EWMA) of a FAIR task's utilization. New samples are added to the moving * average each time a task completes an activation. Sample's weight is chosen * so that the EWMA will be relatively insensitive to transient changes to the * task's workload. * * The enqueued attribute has a slightly different meaning for tasks and cpus: * - task: the task's util_avg at last task dequeue time * - cfs_rq: the sum of util_est.enqueued for each RUNNABLE task on that CPU * Thus, the util_est.enqueued of a task represents the contribution on the * estimated utilization of the CPU where that task is currently enqueued. * * Only for tasks we track a moving average of the past instantaneous * estimated utilization. This allows to absorb sporadic drops in utilization * of an otherwise almost periodic task. */struct util_est { unsigned int enqueued; unsigned int ewma;#define UTIL_EST_WEIGHT_SHIFT 2} __attribute__((__aligned__(sizeof(u64)))); 当一个 task 完成一次激活（？？enqueue?）的时候，最新的采样会被 加权之后添加到 EWMA中；因为会对 sample 做一个加权，所以 EWMA 对 task workload的瞬时量不会态敏感。 .enqueue 这个含义对 task 与 cpu 有些不一样:task: 在上次 dequeue时刻，task的 util_avgcfs_rq: 在 这个cpu上 所有 RUNNABLE task的 util_est.enqueued之和 我们只对 tasks 跟踪 过去瞬时的 util，不会对 cpu跟踪这个。 APIutil_est 对外部只提供了四个API 1234567891011static inline void util_est_enqueue(struct cfs_rq *cfs_rq, struct task_struct *p);static inline void util_est_dequeue(struct cfs_rq *cfs_rq, struct task_struct *p);static inline void util_est_update(struct cfs_rq *cfs_rq, struct task_struct *p, bool task_sleep);static inline void cfs_se_util_change(struct sched_avg *avg); util_est_enqueue util_est_dequeue 都是只在 enqueue_task_fair dequeue_task_fair 中使用。 123456789101112131415161718192021222324252627282930313233static voidenqueue_task_fair(struct rq *rq, struct task_struct *p, int flags){ struct cfs_rq *cfs_rq; struct sched_entity *se = &amp;p-&gt;se; int idle_h_nr_running = task_has_idle_policy(p); int task_new = !(flags &amp; ENQUEUE_WAKEUP); /* * The code below (indirectly) updates schedutil which looks at * the cfs_rq utilization to select a frequency. * Let's add the task's estimated utilization to the cfs_rq's * estimated utilization, before we update schedutil. */ util_est_enqueue(&amp;rq-&gt;cfs, p); ......}static void dequeue_task_fair(struct rq *rq, struct task_struct *p, int flags){ struct cfs_rq *cfs_rq; struct sched_entity *se = &amp;p-&gt;se; int task_sleep = flags &amp; DEQUEUE_SLEEP; int idle_h_nr_running = task_has_idle_policy(p); bool was_sched_idle = sched_idle_rq(rq); util_est_dequeue(&amp;rq-&gt;cfs, p); ......dequeue_throttle: util_est_update(&amp;rq-&gt;cfs, p, task_sleep); hrtick_update(rq);} enqueue_task_fair 中调用 util_est_enqueue 时，此时 task-&gt;entity 还未 入队dequeue_task_fair 中调用 util_est_dequeue 时，此时 task-&gt;entity 还未 出队 在 enqueue_task_fair dequeue_task_fair 中，会对 entity 进行 enqueue_entity 和 dequeue_entity，两个过程都需要 update_load_avg， 1234567891011121314151617181920212223242526272829303132333435363738394041424344static inline void cfs_se_util_change(struct sched_avg *avg){ unsigned int enqueued; if (!sched_feat(UTIL_EST)) return; /* Avoid store if the flag has been already set */ enqueued = avg-&gt;util_est.enqueued; if (!(enqueued &amp; UTIL_AVG_UNCHANGED)) return; /* Reset flag to report util_avg has been updated */ enqueued &amp;= ~UTIL_AVG_UNCHANGED; WRITE_ONCE(avg-&gt;util_est.enqueued, enqueued);}int __update_load_avg_se(u64 now, struct cfs_rq *cfs_rq, struct sched_entity *se){ if (___update_load_sum(now, &amp;se-&gt;avg, !!se-&gt;on_rq, se_runnable(se), cfs_rq-&gt;curr == se)) { ___update_load_avg(&amp;se-&gt;avg, se_weight(se)); cfs_se_util_change(&amp;se-&gt;avg); trace_pelt_se_tp(se); return 1; } return 0;}static inline void update_load_avg(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags){ u64 now = cfs_rq_clock_pelt(cfs_rq); int decayed; /* * Track task load average for carrying it to new CPU after migrated, and * track group sched_entity load average for task_h_load calc in migration */ if (se-&gt;avg.last_update_time &amp;&amp; !(flags &amp; SKIP_AGE_LOAD)) __update_load_avg_se(now, cfs_rq, se); ......} 所以一个 task enqueue dequeue 两个过程 和 util_est相关的流程是 123456789101112131415161718192021222324252627enqueue_task_fair | util_est_enqueue | enqueue_entity | update_load_avg | __update_load_avg_se | cfs_se_util_change......dequeue_task_fair | util_est_dequeue | enqueue_entity | update_load_avg | __update_load_avg_se | cfs_se_util_change | util_est_update 都是先要经过 cfs_se_util_change 然后再经过 dequeue_task_fair 中的util_est_update ，在 util_est_update 中更新 EWMA的值。 内部实现使用了 UTIL_AVG_UNCHANGED 这个变量来同步这两个过程。 1234567/* * When a task is dequeued, its estimated utilization should not be update if * its util_avg has not been updated at least once. * This flag is used to synchronize util_avg updates with util_est updates. * We map this information into the LSB bit of the utilization saved at * dequeue time (i.e. util_est.dequeued). */ EWMA是啥？参考EWMA EWMA - Exponential Weighted Moving Average 指数加权移动平均。 ewma(t) = w * task_util(p) + (1-w) * ewma(t-1) = w * task_util(p) + ewma(t-1) - w * ewma(t-1) = w * (task_util(p) - ewma(t-1)) + ewma(t-1) = w * ( last_ewma_diff ) + ewma(t-1) = w * (last_ewma_diff + ewma(t-1) / w) 其中 w = 0.25 = 1 / 4 ewma(t) = (last_ewma_diff + ewma(t-1) &lt;&lt; 2) &gt;&gt; 2 = (last_ewma_diff + ewma(t-1) &lt;&lt; UTIL_EST_WEIGHT_SHIFT) &gt;&gt; UTIL_EST_WEIGHT_SHIFT 当前 采样周期只占用到了 (1 / 4) 权重 优化为了更快追踪负载的变化，util_est 作者引入了另一个 feature UTIL_EST_FASTUP, 用来加速 util上升 123456789101112131415161718192021222324sched/fair/util_est: Implement faster ramp-up EWMA on utilization increasesThe estimated utilization for a task: util_est = max(util_avg, est.enqueue, est.ewma)is defined based on: - util_avg: the PELT defined utilization - est.enqueued: the util_avg at the end of the last activation - est.ewma: a exponential moving average on the est.enqueued samplesAccording to this definition, when a task suddenly changes its bandwidthrequirements from small to big, the EWMA will need to collect multiplesamples before converging up to track the new big utilization.This slow convergence towards bigger utilization values is notaligned to the default scheduler behavior, which is to optimize forperformance. Moreover, the est.ewma component fails to compensate fortemporarely utilization drops which spans just few est.enqueued samples.To let util_est do a better job in the scenario depicted above, changeits definition by making util_est directly follow upward motion andonly decay the est.ewma on downward. 主要改动是这里 123456789101112131415161718--- a/kernel/sched/fair.c+++ b/kernel/sched/fair.c@@ -3768,11 +3768,22 @@ util_est_dequeue(struct cfs_rq *cfs_rq, struct task_struct *p, bool task_sleep) if (ue.enqueued &amp; UTIL_AVG_UNCHANGED) return;+ /*+ * Reset EWMA on utilization increases, the moving average is used only+ * to smooth utilization decreases.+ */+ ue.enqueued = (task_util(p) | UTIL_AVG_UNCHANGED);+ if (sched_feat(UTIL_EST_FASTUP)) {+ if (ue.ewma &lt; ue.enqueued) {+ ue.ewma = ue.enqueued;+ goto done;+ }+ }+ 开启 UTIL_EST_FASTUP 之后， 如果检测到 ewma(t-1) &lt; .enqueued 直接用当前的 负载util 替换掉历史负载 ewma. 这样task 在 small_to_big 的时候可以更快的跟踪负载，从而更好地 调频与 升核。","link":"/2021/04/30/schedule/util_est/"},{"title":"fair","text":"reweight_entitysched_init()-&gt;| |-&gt;set_load_weight(p, false)sched_fork()-&gt;| set_user_nice()———&gt;| |-&gt;set_load_weight(p, true)-&gt;reweight_task()-&gt;reweight_entity()__setscheduler_params()-&gt;| 只会在 cfs task改变 task nice值 或者 改变调度参数的时候会 调用到 reweight_entity() 12345678910111213141516171819202122232425262728293031static void reweight_entity(struct cfs_rq *cfs_rq, struct sched_entity *se, unsigned long weight, unsigned long runnable){ if (se-&gt;on_rq) { /* commit outstanding execution time */ if (cfs_rq-&gt;curr == se) update_curr(cfs_rq); account_entity_dequeue(cfs_rq, se); dequeue_runnable_load_avg(cfs_rq, se); } dequeue_load_avg(cfs_rq, se); se-&gt;runnable_weight = runnable; update_load_set(&amp;se-&gt;load, weight);#ifdef CONFIG_SMP do { u32 divider = LOAD_AVG_MAX - 1024 + se-&gt;avg.period_contrib; se-&gt;avg.load_avg = div_u64(se_weight(se) * se-&gt;avg.load_sum, divider); se-&gt;avg.runnable_load_avg = div_u64(se_runnable(se) * se-&gt;avg.runnable_load_sum, divider); } while (0);#endif enqueue_load_avg(cfs_rq, se); if (se-&gt;on_rq) { account_entity_enqueue(cfs_rq, se); enqueue_runnable_load_avg(cfs_rq, se); }} 由于重新计算 load_weight，所以需要 xx_dequeue() and xx_enqueue()。","link":"/2021/04/30/schedule/fair/"},{"title":"uclamp","text":"WHAT?uclamp(utilzation clamp) 是说 任务 cpu使用率夹钳，主要是 userspace 对 scheduler的一些hint。 比如某个 task 对用户体验有直接的影响，此 task至少需要 20%cpu，那么计算 cpu使用率，select_rq的时候，必须考虑此task的 minimum “requested”，然后选择 rq 和 freq。 比如某个低优先级的 task 对用户体验没有影响，此 task最多需要 60%cpu（60% &lt; 1024），那么计算 cpu使用率，select_rq的时候，必须考虑此task的 maximum “requested”，然后选择 rq 和freq。 12345678910111213sched/cpufreq, sched/uclamp: Add clamps for FAIR and RT tasks Each time a frequency update is required via schedutil, a frequency is selected to (possibly) satisfy the utilization reported by each scheduling class and irqs. However, when utilization clamping is in use, the frequency selection should consider userspace utilization clamping hints. This will allow, for example, to: - boost tasks which are directly affecting the user experience by running them at least at a minimum &quot;requested&quot; frequency - cap low priority tasks not directly affecting the user experience by running them only up to a maximum &quot;allowed&quot; frequency 在uapi 的头文件include/uapi/linux/sched/types.h中 12345678910111213 @sched_util_min represents the minimum utilization @sched_util_max represents the maximum utilizationUtilization is a value in the range [0..SCHED_CAPACITY_SCALE]. Itrepresents the percentage of CPU time used by a task when running at themaximum frequency on the highest capacity CPU of the system. For example, a20% utilization task is a task running for 2ms every 10ms at maximumfrequency.A task with a min utilization value bigger than 0 is more likely scheduledon a CPU with a capacity big enough to fit the specified value.A task with a max utilization value smaller than 1024 is more likelyscheduled on a CPU with no more capacity than the specified value. uclamp 的min..max 指的都是最大 freq下 的cpu使用率， 20%使用率 说的是在最大频率下，task 每10ms需要运行2ms。 data structureuclamp 分为per sched_entity 和 per rq 两种，分别使用 struct uclamp_se struct uclamp_rq来描述。 12345678910111213141516171819202122struct uclamp_se { unsigned int value : bits_per(SCHED_CAPACITY_SCALE); unsigned int bucket_id : bits_per(UCLAMP_BUCKETS); unsigned int active : 1; unsigned int user_defined : 1;};struct uclamp_rq { unsigned int value; struct uclamp_bucket bucket[UCLAMP_BUCKETS];};struct rq { /* runqueue lock: */ raw_spinlock_t lock;#ifdef CONFIG_UCLAMP_TASK /* Utilization clamp values based on CPU's RUNNABLE tasks */ struct uclamp_rq uclamp[UCLAMP_CNT] ____cacheline_aligned; unsigned int uclamp_flags;#define UCLAMP_FLAG_IDLE 0x01#endif} per sched entity 又分为两种，一种是 per task，一种是 per task group。 12345678910111213141516171819202122struct task_struct { ......#ifdef CONFIG_UCLAMP_TASK /* Clamp values requested for a scheduling entity */ struct uclamp_se uclamp_req[UCLAMP_CNT]; /* Effective clamp values used for a scheduling entity */ struct uclamp_se uclamp[UCLAMP_CNT];#endif ......}struct task_group { struct cgroup_subsys_state css;#ifdef CONFIG_UCLAMP_TASK_GROUP /* The two decimal precision [%] value requested from user-space */ unsigned int uclamp_pct[UCLAMP_CNT]; /* Clamp values requested for a task group */ struct uclamp_se uclamp_req[UCLAMP_CNT]; /* Effective clamp values used for a task group */ struct uclamp_se uclamp[UCLAMP_CNT];#endif}; apiinit12345678910111213141516171819202122static void __init init_uclamp(void){ struct uclamp_se uc_max = {}; enum uclamp_id clamp_id; int cpu; mutex_init(&amp;uclamp_mutex); for_each_possible_cpu(cpu) init_uclamp_rq(cpu_rq(cpu)); for_each_clamp_id(clamp_id) { uclamp_se_set(&amp;init_task.uclamp_req[clamp_id], uclamp_none(clamp_id), false); } /* System defaults allow max clamp values for both indexes */ uclamp_se_set(&amp;uc_max, uclamp_none(UCLAMP_MAX), false); for_each_clamp_id(clamp_id) { uclamp_default[clamp_id] = uc_max; }} kernel api12345678910111213141516171819202122232425262728293031static inline void uclamp_rq_inc(struct rq *rq, struct task_struct *p) { }static inline void uclamp_rq_dec(struct rq *rq, struct task_struct *p) { }static inline void enqueue_task(struct rq *rq, struct task_struct *p, int flags){ if (!(flags &amp; ENQUEUE_NOCLOCK)) update_rq_clock(rq); if (!(flags &amp; ENQUEUE_RESTORE)) { sched_info_queued(rq, p); psi_enqueue(p, flags &amp; ENQUEUE_WAKEUP); } uclamp_rq_inc(rq, p); p-&gt;sched_class-&gt;enqueue_task(rq, p, flags);}static inline void dequeue_task(struct rq *rq, struct task_struct *p, int flags){ if (!(flags &amp; DEQUEUE_NOCLOCK)) update_rq_clock(rq); if (!(flags &amp; DEQUEUE_SAVE)) { sched_info_dequeued(rq, p); psi_dequeue(p, flags &amp; DEQUEUE_SLEEP); } uclamp_rq_dec(rq, p); p-&gt;sched_class-&gt;dequeue_task(rq, p, flags);} 在 enqueue_task dequeue_task 中会 根据 此task是否 开启 uclamp来更改 rq的统计的uclamp_rq 数据。 user apiuclamp 在 userspace 提供了三类接口 全局uclamp接口 /proc/sys/kernel/sched_util_clamp_min /proc/sys/kernel/sched_util_clamp_max cgroup based API cpu.uclamp.max cpu.uclamp.min per-task API 一般全局uclamp接口都是设置为 min-1024 max-1024。cgroup 接口一般只会限制最大使用率per-task的情况较为复杂，以pixel6为例： 在pixel6搭载的Android 12中，google去除了schedtue sched boost等机制，pixel6 且采用了双X1架构的芯片，没有使用walt算法。为了尽量提升用户流畅性体验，所以对uclamp较为依赖。 kernel space对外提供的接口是 sched_setattr, android 框架层对他做了封装setUclamp 在 pixel6 中使用 uclamp的场景大概分为以下几类: 应用启动 滑动boost top-app 首先是应用启动，应用启动往往需要创建多个线程，读取很多资源。所以在应用冷启动时刻，除了将所有cpu的频率固定在最高频之外，还将应用的 UI_thread Render_thread hwui_task0 hwui_task1 uclamp直接设置为 512，这样会使得这几个重载线程在启动时刻就跑在超大核心上，减少应用冷启动时间。 滑动场景，由于滑动场景有较多复杂的动画，UI_thread Render_thread hwui_task0 hwui_task1的负载较重，如果不及时上超大核，有丢帧的风险，所以滑动场景也是将这几个task的uclamp_min 设置为 512，来保证可以上超大核。 根据场景设置在 pixel6 中，从代码中看 有 powerhint.json 这个文件。 1234567891011121314{ &quot;Name&quot;: &quot;TAUClampBoost&quot;, &quot;Path&quot;: &quot;/sys/kernel/vendor_sched/ta_uclamp_min&quot;, &quot;Values&quot;: [ &quot;553&quot;, &quot;1&quot;, &quot;246&quot;, &quot;185&quot;, &quot;123&quot;, &quot;62&quot; ], &quot;DefaultIndex&quot;: 1, &quot;ResetOnInit&quot;: true}, 这个文件将 ta_uclamp_min 重命名为 TAUClampBoost，然后根据 不同场景来设置不同 的val。 12345678910111213141516171819202122232425262728293031323334353637{ &quot;PowerHint&quot;: &quot;LAUNCH&quot;, &quot;Node&quot;: &quot;TAUClampBoost&quot;, &quot;Duration&quot;: 5000, &quot;Value&quot;: &quot;553&quot;},{ &quot;PowerHint&quot;: &quot;REFRESH_120FPS&quot;, &quot;Node&quot;: &quot;TAUClampBoost&quot;, &quot;Duration&quot;: 0, &quot;Value&quot;: &quot;185&quot;},{ &quot;PowerHint&quot;: &quot;REFRESH_90FPS&quot;, &quot;Node&quot;: &quot;TAUClampBoost&quot;, &quot;Duration&quot;: 0, &quot;Value&quot;: &quot;123&quot;},{ &quot;PowerHint&quot;: &quot;REFRESH_60FPS&quot;, &quot;Node&quot;: &quot;TAUClampBoost&quot;, &quot;Duration&quot;: 0, &quot;Value&quot;: &quot;62&quot;},{ &quot;PowerHint&quot;: &quot;ADPF_DISABLE_TA_BOOST&quot;, &quot;Node&quot;: &quot;TAUClampBoost&quot;, &quot;Duration&quot;: 0, &quot;Value&quot;: &quot;1&quot;},{ &quot;PowerHint&quot;: &quot;DISABLE_TA_BOOST&quot;, &quot;Node&quot;: &quot;TAUClampBoost&quot;, &quot;Duration&quot;: 0, &quot;Value&quot;: &quot;1&quot;}, boost 场景与 ta_uclamp_min 关系 12345boost 场景 boost valLAUCH 553120 FPS 18590 FPS 12360FPS 62 cpu0-7 capacity 1234cpu capacitycpu0-3 124cpu4-5 427cpu6-7 1024 游戏场景，由于游戏等前台应用对响应时间较为敏感，pixel6 上android的框架层直接将 ta_uclamp_min 设置为 185或者更多，这样游戏进程主线程或者其他前台进程主线程就无法跑在小核心上，至少是大核起步 123raven:/sys/kernel/vendor_sched # cat ta_uclamp_min185raven:/sys/kernel/vendor_sched #","link":"/2021/05/01/schedule/uclamp/"},{"title":"boost","text":"WHAT?Scheduler boost 是一个机制：将task放到capacity比自己需求大很多 cpu上的机制，开启 boost 的entity也需要为他结束负责。 boost机制 主要是给上层 or framework层写文件节点的，改变boost机制，是整个系统生效的，并不是针对于某单个thread。 1234Scheduler boost is a mechanism to temporarily place tasks on CPUswith higher capacity than those where a task would have normallyended up with their load characteristics. Any entity enablingboost is responsible for disabling it as well. WHY?boost 基本概念boost type 分为以下几种 12345678#define NO_BOOST 0#define FULL_THROTTLE_BOOST 1#define CONSERVATIVE_BOOST 2#define RESTRAINED_BOOST 3#define FULL_THROTTLE_BOOST_DISABLE -1#define CONSERVATIVE_BOOST_DISABLE -2#define RESTRAINED_BOOST_DISABLE -3#define MAX_NUM_BOOST_TYPE (RESTRAINED_BOOST+1) 可以通过sched_boost()获得 12345extern unsigned int sched_boost_type;static inline int sched_boost(void){ return sched_boost_type;} boost policy 分为以下几种 12345enum sched_boost_policy { SCHED_BOOST_NONE, SCHED_BOOST_ON_BIG, SCHED_BOOST_ON_ALL,}; 可以通过sched_boost_policy()获得 12345extern enum sched_boost_policy boost_policy;static inline enum sched_boost_policy sched_boost_policy(void){ return boost_policy;} 每个boost type有相关的enter，exit方法， 12345678910111213141516171819202122232425262728struct sched_boost_data { int refcount; void (*enter)(void); void (*exit)(void);};static struct sched_boost_data sched_boosts[] = { [NO_BOOST] = { .refcount = 0, .enter = sched_no_boost_nop, .exit = sched_no_boost_nop, }, [FULL_THROTTLE_BOOST] = { .refcount = 0, .enter = sched_full_throttle_boost_enter, .exit = sched_full_throttle_boost_exit, }, [CONSERVATIVE_BOOST] = { .refcount = 0, .enter = sched_conservative_boost_enter, .exit = sched_conservative_boost_exit, }, [RESTRAINED_BOOST] = { .refcount = 0, .enter = sched_restrained_boost_enter, .exit = sched_restrained_boost_exit, },}; api写/proc 文件节点 最后调用到sched_boost_handler()–&gt;_sched_set_boost()。 1234567891011121314151617181920212223static void _sched_set_boost(int type){ if (type == 0) sched_boost_disable_all(); else if (type &gt; 0) sched_boost_enable(type); else sched_boost_disable(-type); sched_boost_type = sched_effective_boost(); sysctl_sched_boost = sched_boost_type; set_boost_policy(sysctl_sched_boost);}int sched_boost_handler(struct ctl_table *table, int write, void __user *buffer, size_t *lenp, loff_t *ppos){ mutex_lock(&amp;boost_mutex); _sched_set_boost(*data);done: mutex_unlock(&amp;boost_mutex);} _sched_set_boost(int type) type可能是以下一种： 1234567#define NO_BOOST 0#define FULL_THROTTLE_BOOST 1#define CONSERVATIVE_BOOST 2#define RESTRAINED_BOOST 3#define FULL_THROTTLE_BOOST_DISABLE -1#define CONSERVATIVE_BOOST_DISABLE -2#define RESTRAINED_BOOST_DISABLE -3 type == NO_BOOST ==&gt; disable alltype &gt; NO_BOOST ==&gt; enable one boosttype &lt; NO_BOOST ==&gt; disable one boost sched_boosts 使用了refcount机制，已经开启的boost_type的 refcount &gt; 0，如果使能多个boost type，则哪个值大，生效哪个，可以重复开启某一个type的 boost，由于有refcount，所以也需要多次disable。 sched_boost_disable_all()，直接调用所有refcount &gt; 0的 boost type的 .exit()方法，然后 refcount = 0。 1234567891011static void sched_boost_disable_all(void){ int i; for (i = SCHED_BOOST_START; i &lt; SCHED_BOOST_END; i++) { if (sched_boosts[i].refcount &gt; 0) { sched_boosts[i].exit(); sched_boosts[i].refcount = 0; } }} sched_boost_enable() 首先refcount++，最终调用到 .enter()方法然后实现enable。 12345678910111213141516static void sched_boost_enable(int type){ struct sched_boost_data *sb = &amp;sched_boosts[type]; int next_boost, prev_boost = sched_boost_type; sb-&gt;refcount++; if (sb-&gt;refcount != 1) // 如果已经处于这种模式了，那就直接返回 return; next_boost = sched_effective_boost(); if (next_boost == prev_boost) return; sched_boosts[prev_boost].exit(); sched_boosts[next_boost].enter();} boost type实现针对于每种boost type，Q实现了其 struct sched_boost_data 12345struct sched_boost_data { int refcount; void (*enter)(void); void (*exit)(void);}; NO_BOOST1234567static struct sched_boost_data sched_boosts[] = { [NO_BOOST] = { .refcount = 0, .enter = sched_no_boost_nop, .exit = sched_no_boost_nop, },}; sched_no_boost_nop() 就是 nop空函数 FULL_THROTTLE_BOOST1234567static struct sched_boost_data sched_boosts[] = { [FULL_THROTTLE_BOOST] = { .refcount = 0, .enter = sched_full_throttle_boost_enter, .exit = sched_full_throttle_boost_exit, },}; .enter() .exit()方法 1234567891011static void sched_full_throttle_boost_enter(void){ core_ctl_set_boost(true); // 与 core_ctl强相关，后面再看 walt_enable_frequency_aggregation(true);}static void sched_full_throttle_boost_exit(void){ core_ctl_set_boost(false); walt_enable_frequency_aggregation(false);} CONSERVATIVE_BOOST1234567static struct sched_boost_data sched_boosts[] = { [CONSERVATIVE_BOOST] = { .refcount = 0, .enter = sched_conservative_boost_enter, .exit = sched_conservative_boost_exit, },}; .enter() .exit()方法 123456789static void sched_conservative_boost_enter(void){ update_cgroup_boost_settings();}static void sched_conservative_boost_exit(void){ restore_cgroup_boost_settings();} RESTRAINED_BOOST1234567static struct sched_boost_data sched_boosts[] = { [RESTRAINED_BOOST] = { .refcount = 0, .enter = sched_restrained_boost_enter, .exit = sched_restrained_boost_exit, },}; .enter() .exit()方法 123456789static void sched_restrained_boost_enter(void){ walt_enable_frequency_aggregation(true);}static void sched_restrained_boost_exit(void){ walt_enable_frequency_aggregation(false);} 结论Full throttle： FULL_THROTTLE_BOOST 12341、通过core control，将所有cpu都进行unisolation2、通过freq聚合，将load计算放大。从而触发提升freq，或者迁移等3、通过设置boost policy= SCHED_BOOST_ON_BIG，迁移挑选target cpu时，只会选择大核最终效果应该尽可能把任务都放在大核运行（除了cpuset中有限制） Conservative： CONSERVATIVE_BOOST 123451、通过更新group boost配置，仅让top-app和foreground组进行task placement boost2、提高min_task_util的门限，让进行up migrate的条件更苛刻。只有load较大（&gt;1ms）的task，会进行up migrate。2、同上，更改min_task_util门限后，会提醒系统task与cpu是misfit，需要进行迁移。3、通过设置boost policy= SCHED_BOOST_ON_BIG，迁移挑选target cpu时，只会选择大核最终效果：top-app和foreground的一些task会迁移到大核运行 Restrained： RESTRAINED_BOOST 121、通过freq聚合，将load计算放大。从而触发提升freq，或者迁移等load放大后，仍遵循基本EAS。提升freq或者迁移，视情况而定。","link":"/2021/04/28/schedule/Qcom%20kernel/boost/"},{"title":"core_ctl","text":"","link":"/2021/04/30/schedule/Qcom%20kernel/core_ctl/"},{"title":"walt","text":"WHY?WHAT?walt (Windows Assisted load tracking–窗口协助负载跟踪算法)，主要是跟踪过去一段时间内 entity，rq的负载，给++调频、预测负载、负载均衡++ 等一些调度行为提供一些参考。 walt 数据结构struct walt_task_struct 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374struct walt_task_struct { /* * 'mark_start' marks the beginning of an event (task waking up, task * starting to execute, task being preempted) within a window * * 'sum' represents how runnable a task has been within current * window. It incorporates both running time and wait time and is * frequency scaled. * * 'sum_history' keeps track of history of 'sum' seen over previous * RAVG_HIST_SIZE windows. Windows where task was entirely sleeping are * ignored. * * 'demand' represents maximum sum seen over previous * sysctl_sched_ravg_hist_size windows. 'demand' could drive frequency * demand for tasks. * * 'curr_window_cpu' represents task's contribution to cpu busy time on * various CPUs in the current window * * 'prev_window_cpu' represents task's contribution to cpu busy time on * various CPUs in the previous window * * 'curr_window' represents the sum of all entries in curr_window_cpu * * 'prev_window' represents the sum of all entries in prev_window_cpu * * 'pred_demand' represents task's current predicted cpu busy time * * 'busy_buckets' groups historical busy time into different buckets * used for prediction * * 'demand_scaled' represents task's demand scaled to 1024 */ u64 mark_start; // beginning of events(task start executing or waking up) u32 sum; // runnable time within a window u32 demand; // max sum in past serval(sysctl_sched_ravg_hist_size == 5) windows u32 coloc_demand; u32 sum_history[RAVG_HIST_SIZE_MAX]; u32 *curr_window_cpu, *prev_window_cpu; u32 curr_window, prev_window; u32 pred_demand; u8 busy_buckets[NUM_BUSY_BUCKETS]; u16 demand_scaled; u16 pred_demand_scaled; u64 active_time; int boost; bool wake_up_idle; bool misfit; bool rtg_high_prio; u8 low_latency; u64 boost_period; u64 boost_expires; u64 last_sleep_ts; u32 init_load_pct; u32 unfilter; u64 last_wake_ts; u64 last_enqueued_ts; struct walt_related_thread_group __rcu *grp; struct list_head grp_list; u64 cpu_cycles; cpumask_t cpus_requested; bool iowaited;};struct task_struct { ...... struct sched_entity se; struct sched_rt_entity rt;#ifdef CONFIG_SCHED_WALT struct walt_task_struct wts;#endif ......}; struct walt_rq 123456789101112131415161718192021222324252627282930313233343536373839404142struct walt_rq { struct task_struct *push_task; struct walt_sched_cluster *cluster; struct cpumask freq_domain_cpumask; struct walt_sched_stats walt_stats; u64 window_start; u32 prev_window_size; unsigned long walt_flags; u64 avg_irqload; u64 last_irq_window; u64 prev_irq_time; struct task_struct *ed_task; u64 task_exec_scale; u64 old_busy_time; u64 old_estimated_time; u64 curr_runnable_sum; u64 prev_runnable_sum; u64 nt_curr_runnable_sum; u64 nt_prev_runnable_sum; u64 cum_window_demand_scaled; struct group_cpu_time grp_time; struct load_subtractions load_subs[NUM_TRACKED_WINDOWS]; DECLARE_BITMAP_ARRAY(top_tasks_bitmap, NUM_TRACKED_WINDOWS, NUM_LOAD_INDICES); u8 *top_tasks[NUM_TRACKED_WINDOWS]; u8 curr_table; int prev_top; int curr_top; bool notif_pending; bool high_irqload; u64 last_cc_update; u64 cycles;};struct rq {#ifdef CONFIG_SCHED_WALT struct walt_rq wrq;#endif /* CONFIG_SCHED_WALT */ ......} walt api使用 init 12345678910111213141516171819202122232425void walt_sched_init_rq(struct rq *rq){ int j; if (cpu_of(rq) == 0) walt_init_once(); cpumask_set_cpu(cpu_of(rq), &amp;rq-&gt;wrq.freq_domain_cpumask); rq-&gt;wrq.walt_stats.cumulative_runnable_avg_scaled = 0; rq-&gt;wrq.prev_window_size = sched_ravg_window; rq-&gt;wrq.window_start = 0; rq-&gt;wrq.walt_stats.nr_big_tasks = 0; ......}void __init sched_init(void){ ...... for_each_possible_cpu(i) { struct rq *rq; walt_sched_init_rq(rq); } ......} set_start() 1234567891011void set_window_start(struct rq *rq){ static int sync_cpu_available; if (likely(rq-&gt;wrq.window_start)) return; if (!sync_cpu_available) { rq-&gt;wrq.window_start = 1; ...... }} ‘ a WALT TIME由于存在多个cluster，不同 cluster的频率 和架构都不一样，所以同一 task在不同 cluster上运行，需要的时间是不一样的。为了更好衡量一个 task的负载或者需求，我们必须考虑 cpu freq 和 ipc的差异，所以需要将不同cpu 上运行的task时间 归一化 到同一个度量尺子上： 123456789101112131415161718192021// delta 是 task实际 运行时间// task_exec_scale 是task 执行时间的 刻度，每次在 update_task_rq_cpu_cycles 中更新static inline u64 scale_exec_time(u64 delta, struct rq *rq){ return (delta * rq-&gt;wrq.task_exec_scale) &gt;&gt; 10;}unsigned long topology_get_cpu_scale(int cpu){ return per_cpu(cpu_scale, cpu);}#define arch_scale_cpu_capacity topology_get_cpu_scalestatic voidupdate_task_rq_cpu_cycles(struct task_struct *p, struct rq *rq, int event, u64 wallclock, u64 irqtime){ rq-&gt;wrq.task_exec_scale = DIV64_U64_ROUNDUP(cpu_cur_freq(cpu) * arch_scale_cpu_capacity(cpu), rq-&gt;wrq.cluster-&gt;max_possible_freq);} cur_freq task_exec_scale = ———— * scale_capacity max_freq cur_freq scale_exec_time = delta * ———— * scale_capacity max_freq scale_capacity 是driver 初始化的时候设置的。 WALT updatewalt_update_task_ravg() 是 walt的最主要输入，在各种event 下更新负载 1234567891011121314151617181920212223242526272829303132333435363738394041424344enum task_event { PUT_PREV_TASK = 0, PICK_NEXT_TASK = 1, TASK_WAKE = 2, TASK_MIGRATE = 3, TASK_UPDATE = 4, IRQ_UPDATE = 5,};/* Reflect task activity on its demand and cpu's busy time statistics */void walt_update_task_ravg(struct task_struct *p, struct rq *rq, int event, u64 wallclock, u64 irqtime){ u64 old_window_start; // rq没有初始化 or task的 mark_start 与 wallclock相等==&gt;刚刚才更新过 if (!rq-&gt;wrq.window_start || p-&gt;wts.mark_start == wallclock) return; old_window_start = update_window_start(rq, wallclock, event); // 更新 rq-&gt;wrq.window_start，方便后面计算 if (!p-&gt;wts.mark_start) { update_task_cpu_cycles(p, cpu_of(rq), wallclock); goto done; } update_task_rq_cpu_cycles(p, rq, event, wallclock, irqtime); // 更新 task_exec_scale // 更新 task的 demand 和 perd_demand 期望下一个window 的运行时间 update_task_demand(p, rq, event, wallclock); update_cpu_busy_time(p, rq, event, wallclock, irqtime); // update_task_pred_demand(rq, p, event); if (event == PUT_PREV_TASK &amp;&amp; p-&gt;state) p-&gt;wts.iowaited = p-&gt;in_iowait; trace_sched_update_task_ravg(p, rq, event, wallclock, irqtime, &amp;rq-&gt;wrq.grp_time); trace_sched_update_task_ravg_mini(p, rq, event, wallclock, irqtime, &amp;rq-&gt;wrq.grp_time);done: p-&gt;wts.mark_start = wallclock;// void walt_irq_work(struct irq_work *irq_work) run_walt_irq_work(old_window_start, rq);} 下面关注update_task_demand() 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970static u64 update_task_demand(struct task_struct *p, struct rq *rq, int event, u64 wallclock){ u64 mark_start = p-&gt;wts.mark_start; u64 delta, window_start = rq-&gt;wrq.window_start; int new_window, nr_full_windows; u32 window_size = sched_ravg_window; u64 runtime; new_window = mark_start &lt; window_start; // window_start = rq-&gt;wrq.window_start 在之前俩函数已经更新过了， // 正常如果经历了新的window，new_window =1 if (!account_busy_for_task_demand(rq, p, event)) { if (new_window) /* * If the time accounted isn't being accounted as * busy time, and a new window started, only the * previous window need be closed out with the * pre-existing demand. Multiple windows may have * elapsed, but since empty windows are dropped, * it is not necessary to account those. */ update_history(rq, p, p-&gt;wts.sum, 1, event); return 0; } // case: 没有经历新的window，p-&gt;wts.sum += delta， sum是保存所有不满一个 window的时间 if (!new_window) { /* * The simple case - busy time contained within the existing * window. */ return add_to_task_demand(rq, p, wallclock - mark_start); } /* * Busy time spans at least two windows. Temporarily rewind * window_start to first window boundary after mark_start. */ delta = window_start - mark_start; nr_full_windows = div64_u64(delta, window_size); window_start -= (u64)nr_full_windows * (u64)window_size; /* Process (window_start - mark_start) first */ runtime = add_to_task_demand(rq, p, window_start - mark_start); /* Push new sample(s) into task's demand history */ update_history(rq, p, p-&gt;wts.sum, 1, event); if (nr_full_windows) { u64 scaled_window = scale_exec_time(window_size, rq); // 为啥这里没有 add_to_task_demand()? ,因为 更新demand也只是更新 p-&gt;wts.sum， 这里 // 因为涉及多个 window，所以 sum肯定会超过一个window的时间。 // 关注 update_history() 中的 p-&gt;wts.sum = 0; update_history(rq, p, scaled_window, nr_full_windows, event); runtime += nr_full_windows * scaled_window; } /* * Roll window_start back to current to process any remainder * in current window. */ window_start += (u64)nr_full_windows * (u64)window_size; /* Process (wallclock - window_start) next */ mark_start = window_start; runtime += add_to_task_demand(rq, p, wallclock - mark_start); return runtime;} add_to_task_demand() 只是在 scale-invariant的尺度上将当前运行时间累计到 task-&gt;wts.sum上，update_history() 是 更新 p-&gt;wts.sum_history[5] 这个历史窗口的数据，类似不停地滚动，update_history()参数runtime也是 scale之后的，demand是根据不同的WINDOW_STATS_XXX决定的；然后根据 predict_and_update_buckets() 来预测 perd_demand需求，这里用到了buckets算法（不是重点）。 freq_policy_load() – __cpu_util_freq_walt() – cpu_util_freq_walt() – schedutil 使用此接口去获得 cpu使用率 walt 输出api1task_util() 1234567以can_migrate_task()函数为例：通过task_util()获取该task的demand，即task级负载cpu_util_cum()获取cpu rq的累计demand，即cpu级负载如果 dst_cpu累计demand + task_demand &gt; src_cpu累计demand + task_demand，那么说明不满足迁移条件。","link":"/2021/04/28/schedule/Qcom%20kernel/walt/"},{"title":"cgroup v1 v2","text":"cgroup v1 与 v2 可以同时并存吗？ cgroup v1 v2 在编译时，是同时被编译进 kernel Image的 123ubuntu@zeku_server:~/workspace/linux $ cat kernel/cgroup/Makefile# SPDX-License-Identifier: GPL-2.0obj-y := cgroup.o rstat.o namespace.o cgroup-v1.o freezer.o ubuntu 如何区分当前正在使用的是 cgroup v1 还是 v2cgroup v1:123456789101112131415tencent_clould@ubuntu: ~/workspace# mount | grep cgrouptmpfs on /sys/fs/cgroup type tmpfs (ro,nosuid,nodev,noexec,mode=755)cgroup2 on /sys/fs/cgroup/unified type cgroup2 (rw,nosuid,nodev,noexec,relatime)cgroup on /sys/fs/cgroup/systemd type cgroup (rw,nosuid,nodev,noexec,relatime,xattr,name=systemd)cgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,cpu,cpuacct)cgroup on /sys/fs/cgroup/devices type cgroup (rw,nosuid,nodev,noexec,relatime,devices)cgroup on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,cpuset,clone_children)cgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio)cgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory)cgroup on /sys/fs/cgroup/perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,perf_event)cgroup on /sys/fs/cgroup/hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,hugetlb)cgroup on /sys/fs/cgroup/net_cls,net_prio type cgroup (rw,nosuid,nodev,noexec,relatime,net_cls,net_prio)cgroup on /sys/fs/cgroup/freezer type cgroup (rw,nosuid,nodev,noexec,relatime,freezer)cgroup on /sys/fs/cgroup/rdma type cgroup (rw,nosuid,nodev,noexec,relatime,rdma)cgroup on /sys/fs/cgroup/pids type cgroup (rw,nosuid,nodev,noexec,relatime,pids) cgroup v2: 123ubuntu@zeku_server:~/workspace/linux $ mount | grep cgroupcgroup2 on /sys/fs/cgroup type cgroup2 (rw,nosuid,nodev,noexec,relatime,nsdelegate)ubuntu@zeku_server:~/workspace/linux $ 如何是的默认开启 cgroup v2a. 修改 command line123ubuntu@zeku_server:~/workspace/linux $ cat /etc/default/grub | grep LINEGRUB_CMDLINE_LINUX_DEFAULT=&quot;quiet splash&quot;GRUB_CMDLINE_LINUX=&quot;systemd.unified_cgroup_hierarchy=1&quot; b. 修改 command line, 来disable 所有 cgroup v1 的 feature 1cgroup_no_v1=all 对应 kernel/cgroup/cgroup-v1.c 中 cgroup_no_v1 1static int __init cgroup_no_v1(char *str); c. 可以在 其他挂载点，重新挂载 cgroup2 1mount -t cgroup2 none /sys/fs/cgroup cgroup v1、v2 中task 是如何加入 cgroup的？cgroup v1:123456789101112131415tencent_clould@ubuntu: ~/workspace# cat /proc/$$/cgroup12:pids:/user.slice/user-1000.slice/session-99270.scope11:rdma:/10:freezer:/user/ubuntu/09:net_cls,net_prio:/8:hugetlb:/7:perf_event:/6:memory:/user.slice/user-1000.slice/session-99270.scope5:blkio:/user.slice4:cpuset:/3:devices:/user.slice2:cpu,cpuacct:/user.slice1:name=systemd:/user.slice/user-1000.slice/session-99270.scope0::/user.slice/user-1000.slice/session-99270.scopetencent_clould@ubuntu: ~/workspace# cgroup v1: 123ubuntu@zeku_server:~/workspace/linux $ cat /proc/$$/cgroup0::/user.slice/user-1000.slice/session-166.scopeubuntu@zeku_server:~/workspace/linux $ 可以看到 cgroup v1中，一个 task会存在多个 子系统的目录下，即使这个子系统没有起作用（在子系统根目录）；在 cgroup v2 中，一个 task 只会存在一个 目录下，简洁（一个目录根据 cgoups.controller 来使能相关的 子系统） a aa","link":"/2021/07/06/Docker%E6%9C%BA%E5%88%B6%E5%88%86%E6%9E%90/cgroup%20v1%20&&%20v2/"},{"title":"namespace之uts","text":"demo1shell1: 123456stable_kernel@1kernel: /var/crash# hostnamerlk-Standard-PC-i440FX-PIIX-1996stable_kernel@kernel: /var/crash# sudo unshare -p -f /bin/bashxhost: unable to open display &quot;&quot;root@rlk-Standard-PC-i440FX-PIIX-1996:/var/crash# hostnamerlk-Standard-PC-i440FX-PIIX-1996 shell2: 12345678stable_kernel@kernel: /var/crash# sudo unshare -p -f /bin/bashxhost: unable to open display &quot;&quot;root@rlk-Standard-PC-i440FX-PIIX-1996:/var/crash# hostnamerlk-Standard-PC-i440FX-PIIX-1996root@rlk-Standard-PC-i440FX-PIIX-1996:/var/crash# hostname hhhroot@rlk-Standard-PC-i440FX-PIIX-1996:/var/crash# hostnamehhhroot@rlk-Standard-PC-i440FX-PIIX-1996:/var/crash# shell1: 123456789stable_kernel@1kernel: /var/crash# hostnamerlk-Standard-PC-i440FX-PIIX-1996stable_kernel@kernel: /var/crash# sudo unshare -p -f /bin/bashxhost: unable to open display &quot;&quot;root@rlk-Standard-PC-i440FX-PIIX-1996:/var/crash# hostnamerlk-Standard-PC-i440FX-PIIX-1996root@rlk-Standard-PC-i440FX-PIIX-1996:/var/crash# hostnamehhhroot@rlk-Standard-PC-i440FX-PIIX-1996:/var/crash# 可以看出在没有新建uts namespace的时候， 他们都是同一个 uts name；只要修改任何一个 uts 中的 hostname 的话，所有的都会被修改 demo2shell1: 12345678stable_kernel@kernel: /var/crash# sudo unshare -p -f -u /bin/bashxhost: unable to open display &quot;&quot;root@rlk-Standard-PC-i440FX-PIIX-1996:/var/crash# hostnamerlk-Standard-PC-i440FX-PIIX-1996root@rlk-Standard-PC-i440FX-PIIX-1996:/var/crash# hostname shell1root@rlk-Standard-PC-i440FX-PIIX-1996:/var/crash# hostnameshell1root@rlk-Standard-PC-i440FX-PIIX-1996:/var/crash# shell2: 12345678stable_kernel@kernel: /var/crash# sudo unshare -p -f -u /bin/bashxhost: unable to open display &quot;&quot;root@rlk-Standard-PC-i440FX-PIIX-1996:/var/crash# hostnamerlk-Standard-PC-i440FX-PIIX-1996root@rlk-Standard-PC-i440FX-PIIX-1996:/var/crash# hostname shell2root@rlk-Standard-PC-i440FX-PIIX-1996:/var/crash# hostnameshell2root@rlk-Standard-PC-i440FX-PIIX-1996:/var/crash# shell3: 123stable_kernel@kernel: /var/crash# hostnamerlk-Standard-PC-i440FX-PIIX-1996stable_kernel@kernel: /var/crash# 可以看出，只要新建了 uts namespace 之后，在 new uts namespace中做任何对 hostname 的修改，都不会影响到 其他 old uts namespace 的 hostname. demo3shell1: 12345root@rlk-Standard-PC-i440FX-PIIX-1996:/var/crash# hostnameshell1root@rlk-Standard-PC-i440FX-PIIX-1996:/var/crash# sleep 10000 &amp;[1] 18root@rlk-Standard-PC-i440FX-PIIX-1996:/var/crash# shell2: 12345678stable_kernel@kernel: /var/crash# ps -aux | grep sleeproot 3860 0.0 0.0 16716 528 pts/0 S 17:02 0:00 sleep 10000rlk 3862 0.0 0.0 17676 728 pts/1 S+ 17:02 0:00 grep --color=auto --exclude-dir=.bzr --exclude-dir=CVS --exclude-dir=.git --exclude-dir=.hg --exclude-dir=.svn --exclude-dir=.idea --exclude-dir=.tox sleepstable_kernel@1kernel: /var/crash# sudo nsenter -u -t3860xhost: unable to open display &quot;&quot;root@shell1:/var/crash# hostnameshell1root@shell1:/var/crash# uts_namespace 实现12345678910111213141516171819202122232425262728293031323334copy_processes-&gt;copy_namespaces-&gt;create_new_namespaces-&gt;copy_utsname +----------------------+ | | | | | copy_processes | | | +----------+-----------+ | |+-----------v------------+| || || copy_namespaces || |+----------+-------------+ | | |+----------v--------------+| || || create_new_namespaces || |+----------+--------------+ | | |+----------v------------+| || || copy_utsname || |+----------+------------+ clone_uts_ns 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647static struct uts_namespace *clone_uts_ns(struct user_namespace *user_ns, struct uts_namespace *old_ns){ struct uts_namespace *ns; struct ucounts *ucounts; int err; err = -ENOSPC; ucounts = inc_uts_namespaces(user_ns); if (!ucounts) goto fail; err = -ENOMEM; ns = create_uts_ns(); if (!ns) goto fail_dec; err = ns_alloc_inum(&amp;ns-&gt;ns); if (err) goto fail_free; ns-&gt;ucounts = ucounts; ns-&gt;ns.ops = &amp;utsns_operations; down_read(&amp;uts_sem); memcpy(&amp;ns-&gt;name, &amp;old_ns-&gt;name, sizeof(ns-&gt;name)); // name 是 struct new_utsname 类型，不是 string ns-&gt;user_ns = get_user_ns(user_ns); up_read(&amp;uts_sem); return ns;}struct uts_namespace *copy_utsname(unsigned long flags, struct user_namespace *user_ns, struct uts_namespace *old_ns){ struct uts_namespace *new_ns; BUG_ON(!old_ns); get_uts_ns(old_ns); if (!(flags &amp; CLONE_NEWUTS)) return old_ns; new_ns = clone_uts_ns(user_ns, old_ns); // 完全 clone parent 的 uts_namespace put_uts_ns(old_ns); return new_ns;} set看一下 strace: 123root@rlk-Standard-PC-i440FX-PIIX-1996:/var/crash# strace hostname shell0execve(&quot;/usr/bin/hostname&quot;, [&quot;hostname&quot;, &quot;shell0&quot;], 0x7ffd8feacde8 /* 27 vars */) = 0sethostname(&quot;shell0&quot;, 6) = 0 linux 是通过 sethostname syscall 来修改 hostname(保存在 nodename 中)的。sethostname 123456789101112131415161718192021222324252627282930313233343536static struct ctl_table uts_kern_table[] = { { .procname = &quot;hostname&quot;, .data = init_uts_ns.name.nodename, .maxlen = sizeof(init_uts_ns.name.nodename), .mode = 0644, .proc_handler = proc_do_uts_string, .poll = &amp;hostname_poll, },......};SYSCALL_DEFINE2(sethostname, char __user *, name, int, len){ int errno; char tmp[__NEW_UTS_LEN]; if (!ns_capable(current-&gt;nsproxy-&gt;uts_ns-&gt;user_ns, CAP_SYS_ADMIN)) return -EPERM; if (len &lt; 0 || len &gt; __NEW_UTS_LEN) return -EINVAL; errno = -EFAULT; if (!copy_from_user(tmp, name, len)) { struct new_utsname *u; down_write(&amp;uts_sem); u = utsname(); memcpy(u-&gt;nodename, tmp, len); memset(u-&gt;nodename + len, 0, sizeof(u-&gt;nodename) - len); errno = 0; uts_proc_notify(UTS_PROC_HOSTNAME); up_write(&amp;uts_sem); } return errno;} get看一下 strace: 123root@rlk-Standard-PC-i440FX-PIIX-1996:/var/crash# strace hostnameexecve(&quot;/usr/bin/hostname&quot;, [&quot;hostname&quot;], 0x7ffe740a5980 /* 27 vars */) = 0uname({sysname=&quot;Linux&quot;, nodename=&quot;shell2&quot;, ...}) = 0 linux 是通过 uname syscall 来获取 uts 相关信息的。 12345678910111213141516171819SYSCALL_DEFINE1(uname, struct old_utsname __user *, name){ struct old_utsname tmp; if (!name) return -EFAULT; down_read(&amp;uts_sem); memcpy(&amp;tmp, utsname(), sizeof(tmp)); // 获取 task相关utsname 信息 up_read(&amp;uts_sem); if (copy_to_user(name, &amp;tmp, sizeof(tmp))) // copy 到 user space return -EFAULT; if (override_release(name-&gt;release, sizeof(name-&gt;release))) return -EFAULT; if (override_architecture(name)) return -EFAULT; return 0;} 参考：Linux Namespace : UTS","link":"/2021/07/06/Docker%E6%9C%BA%E5%88%B6%E5%88%86%E6%9E%90/namespace%E4%B9%8Buts/"},{"title":"cgroup v2 使用","text":"如何加入 cgroupcgroup.procs 1234567root@rlk-Standard-PC-i440FX-PIIX-1996:/sys/fs/cgroup/tmp# sleep 10000 &amp;[2] 9087root@rlk-Standard-PC-i440FX-PIIX-1996:/sys/fs/cgroup/tmp# echo 9087 &gt; cgroup.procsroot@rlk-Standard-PC-i440FX-PIIX-1996:/sys/fs/cgroup/tmp# cat cgroup.procs74129087root@rlk-Standard-PC-i440FX-PIIX-1996:/sys/fs/cgroup/tmp# code新建一个 cgroup123root@rlk-Standard-PC-i440FX-PIIX-1996:/sys/fs/cgroup/tmp# mkdir 123root@rlk-Standard-PC-i440FX-PIIX-1996:/sys/fs/cgroup/tmp# ls123 cgroup.type io.stat memory.stat codepid cgroupfreeze cgroupdemo1234root@rlk-Standard-PC-i440FX-PIIX-1996:/sys/fs/cgroup/user.slice/user-1000.slice/session-1.scope# cat cgroup.freeze0root@rlk-Standard-PC-i440FX-PIIX-1996:/sys/fs/cgroup/user.slice/user-1000.slice/session-1.scope# echo 1 &gt; cgroup.freezeroot@rlk-Standard-PC-i440FX-PIIX-1996:/sys/fs/cgroup/user.slice/user-1000.slice/session-1.scope# 在这个session 中的所有 task都不执行了，相对于被冻住了。 123456789root@rlk-Standard-PC-i440FX-PIIX-1996:/sys/fs/cgroup/user.slice/user-1000.slice/session-1.scope# ps -auxUSER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMANDroot 1 0.0 0.3 167756 11436 ? Ss 10:32 0:01 /sbin/initroot 2 0.0 0.0 0 0 ? S 10:32 0:00 [kthreadd]root 3 0.0 0.0 0 0 ? I&lt; 10:32 0:00 [rcu_gp]root 3342 0.0 0.0 3864 1044 pts/2 S+ 10:33 0:00 stress -c 3root 3343 77.4 0.0 3864 100 pts/2 S+ 10:33 26:01 stress -c 3root 3344 77.2 0.0 3864 100 pts/2 S+ 10:33 25:56 stress -c 3root 3345 77.2 0.0 3864 100 pts/2 S+ 10:33 25:56 stress -c 3 ps -aux 看到 stress thread 都是 处于 sleep状态。 12root@rlk-Standard-PC-i440FX-PIIX-1996:/sys/fs/cgroup/user.slice/user-1000.slice/session-1.scope# echo 0 &gt; cgroup.freezeroot@rlk-Standard-PC-i440FX-PIIX-1996:/sys/fs/cgroup/user.slice/user-1000.slice/session-1.scope# 解除 freeze之后，再通过 ps -aux 查看 12345678root@rlk-Standard-PC-i440FX-PIIX-1996:/sys/fs/cgroup/user.slice/user-1000.slice/session-1.scope# ps -auxUSER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMANDroot 1 0.0 0.3 167756 11436 ? Ss 10:32 0:01 /sbin/initroot 2 0.0 0.0 0 0 ? S 10:32 0:00 [kthreadd]root 3 0.0 0.0 0 0 ? I&lt; 10:32 0:00 [rcu_gp]root 3343 72.7 0.0 3864 100 pts/2 R+ 10:33 26:11 stress -c 3root 3344 72.4 0.0 3864 100 pts/2 R+ 10:33 26:05 stress -c 3root 3345 72.5 0.0 3864 100 pts/2 R+ 10:33 26:06 stress -c 3 stress 基本一直处于 R+ 状态。 cpu cgroup通过 各个层级的cgroup.controllers 和 cat cgroup.subtree_control 设置，将 user.slice/user-1000.slice/session-1.scope 开启 cpu cgroup. 1234567root@rlk-Standard-PC-i440FX-PIIX-1996:/sys/fs/cgroup/user.slice/user-1000.slice/session-1.scope# ls -a | grep cpucpu.maxcpu.max.burstcpu.pressurecpu.statcpu.weightcpu.weight.nice 控制cpu配额默认值 max 100000，标识不显示 cpu使用率 12root@rlk-Standard-PC-i440FX-PIIX-1996:/sys/fs/cgroup/user.slice/user-1000.slice/session-1.scope# cat cpu.maxmax 100000 设置限制 单核 50% 1234root@rlk-Standard-PC-i440FX-PIIX-1996:/sys/fs/cgroup/user.slice/user-1000.slice/session-1.scope# echo 50000 100000 &gt; cpu.maxroot@rlk-Standard-PC-i440FX-PIIX-1996:/sys/fs/cgroup/user.slice/user-1000.slice/session-1.scope# cat cpu.max50000 100000root@rlk-Standard-PC-i440FX-PIIX-1996:/sys/fs/cgroup/user.slice/user-1000.slice/session-1.scope# 四核心 机器上 top 看到 1234567891011top - 11:01:26 up 28 min, 3 users, load average: 0.27, 1.84, 2.08 [1/11]Tasks: 191 total, 4 running, 187 sleeping, 0 stopped, 0 zombie%Cpu(s): 12.9 us, 0.2 sy, 0.0 ni, 86.9 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 stMiB Mem : 3671.7 total, 2367.6 free, 644.6 used, 659.5 buff/cacheMiB Swap: 1497.1 total, 1497.1 free, 0.0 used. 2972.4 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 3343 root 20 0 3864 100 0 R 17.4 0.0 25:21.85 stress 3345 root 20 0 3864 100 0 R 16.5 0.0 25:18.51 stress 3344 root 20 0 3864 100 0 R 15.8 0.0 25:19.38 stress 163 root 20 0 0 0 0 I 0.3 0.0 0:00.34 kworker/2:3-events 控制cpu权重后续补充。。","link":"/2021/07/06/Docker%E6%9C%BA%E5%88%B6%E5%88%86%E6%9E%90/cgroup%20v2%20%E4%BD%BF%E7%94%A8/"},{"title":"namespace之pid","text":"demo1shell 1: 1234567891011121314151617181920212223242526272829303132stable_kernel@kernel: /var/crash# echo $$4371stable_kernel@kernel: /var/crash# readlink /proc/$$/ns/pidpid:[4026531836]stable_kernel@kernel: /var/crash#stable_kernel@kernel: /var/crash# sudo unshare --pid --mount-proc --fork /bin/bashxhost: unable to open display &quot;&quot;root@rlk-Standard-PC-i440FX-PIIX-1996:/var/crash# readlink /proc/$$/ns/pidpid:[4026532210]root@rlk-Standard-PC-i440FX-PIIX-1996:/var/crash# ps PID TTY TIME CMD 1 pts/3 00:00:00 bash 10 pts/3 00:00:00 psroot@rlk-Standard-PC-i440FX-PIIX-1996:/var/crash# stress -c 10stress: info: [11] dispatching hogs: 10 cpu, 0 io, 0 vm, 0 hdd^Z[1]+ Stopped stress -c 10root@rlk-Standard-PC-i440FX-PIIX-1996:/var/crash# ps -auxUSER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMANDroot 1 0.0 0.1 18476 4136 pts/3 S 12:10 0:00 /bin/bashroot 11 0.0 0.0 3864 984 pts/3 T 12:12 0:00 stress -c 10root 12 36.8 0.0 3864 100 pts/3 T 12:12 1:21 stress -c 10root 13 39.4 0.0 3864 100 pts/3 T 12:12 1:27 stress -c 10root 14 40.0 0.0 3864 100 pts/3 T 12:12 1:28 stress -c 10root 15 38.8 0.0 3864 100 pts/3 T 12:12 1:25 stress -c 10root 16 39.3 0.0 3864 100 pts/3 T 12:12 1:26 stress -c 10root 17 38.8 0.0 3864 100 pts/3 T 12:12 1:25 stress -c 10root 18 40.3 0.0 3864 100 pts/3 T 12:12 1:29 stress -c 10root 19 39.4 0.0 3864 100 pts/3 T 12:12 1:27 stress -c 10root 20 40.4 0.0 3864 100 pts/3 T 12:12 1:29 stress -c 10root 21 38.3 0.0 3864 100 pts/3 T 12:12 1:24 stress -c 10root 22 0.0 0.0 20312 3608 pts/3 R+ 12:16 0:00 ps -aux shell 2 123456789101112root 4501 0.0 0.0 3864 984 pts/3 S+ 12:12 0:00 stress -c 10root 4502 37.2 0.0 3864 100 pts/3 R+ 12:12 1:18 stress -c 10root 4503 40.1 0.0 3864 100 pts/3 R+ 12:12 1:24 stress -c 10root 4504 40.9 0.0 3864 100 pts/3 R+ 12:12 1:26 stress -c 10root 4505 39.4 0.0 3864 100 pts/3 R+ 12:12 1:22 stress -c 10root 4506 39.9 0.0 3864 100 pts/3 R+ 12:12 1:23 stress -c 10root 4507 39.4 0.0 3864 100 pts/3 R+ 12:12 1:22 stress -c 10root 4508 41.0 0.0 3864 100 pts/3 R+ 12:12 1:26 stress -c 10root 4509 40.2 0.0 3864 100 pts/3 R+ 12:12 1:24 stress -c 10 &quot;rlk-Standard-PC-i440F&quot; 12:18 07-7月-21root 4510 41.0 0.0 3864 100 pts/3 R+ 12:12 1:26 stress -c 10root 4511 39.1 0.0 3864 100 pts/3 R+ 12:12 1:22 stress -c 10rlk 4522 0.0 0.0 20312 3744 pts/2 R+ 12:15 0:00 ps -aux debug log 1234567891011121314151617181920212223[ 2276.675896] [robert] alloc_pid level = 1, pid-&gt;numbers[i].nr = 11[ 2276.679264] [robert] alloc_pid level = 1, pid-&gt;numbers[i].nr = 4501[ 2276.694094] [robert] alloc_pid level = 1, pid-&gt;numbers[i].nr = 12[ 2276.697715] [robert] alloc_pid level = 1, pid-&gt;numbers[i].nr = 4502[ 2276.700705] [robert] alloc_pid level = 1, pid-&gt;numbers[i].nr = 13[ 2276.703495] [robert] alloc_pid level = 1, pid-&gt;numbers[i].nr = 4503[ 2276.706523] [robert] alloc_pid level = 1, pid-&gt;numbers[i].nr = 14[ 2276.709158] [robert] alloc_pid level = 1, pid-&gt;numbers[i].nr = 4504[ 2276.712157] [robert] alloc_pid level = 1, pid-&gt;numbers[i].nr = 15[ 2276.714937] [robert] alloc_pid level = 1, pid-&gt;numbers[i].nr = 4505[ 2276.717940] [robert] alloc_pid level = 1, pid-&gt;numbers[i].nr = 16[ 2276.720724] [robert] alloc_pid level = 1, pid-&gt;numbers[i].nr = 4506[ 2276.723808] [robert] alloc_pid level = 1, pid-&gt;numbers[i].nr = 17[ 2276.726108] [robert] alloc_pid level = 1, pid-&gt;numbers[i].nr = 4507[ 2276.728367] [robert] alloc_pid level = 1, pid-&gt;numbers[i].nr = 18[ 2276.730191] [robert] alloc_pid level = 1, pid-&gt;numbers[i].nr = 4508[ 2276.732382] [robert] alloc_pid level = 1, pid-&gt;numbers[i].nr = 19[ 2276.734118] [robert] alloc_pid level = 1, pid-&gt;numbers[i].nr = 4509[ 2276.736167] [robert] alloc_pid level = 1, pid-&gt;numbers[i].nr = 20[ 2276.737602] [robert] alloc_pid level = 1, pid-&gt;numbers[i].nr = 4510[ 2276.739537] [robert] alloc_pid level = 1, pid-&gt;numbers[i].nr = 21[ 2276.740830] [robert] alloc_pid level = 1, pid-&gt;numbers[i].nr = 4511[ 2279.849411] [robert] alloc_pid level = 0, pid-&gt;numbers[i].nr = 4512 可以看到， new pid_namespace 中的 21 对应 old pid_namespace 中的 4511… 添加 log的 patch是： 123456789101112131415161718192021222324252627282930313233343536ubuntu@zeku_server:~/workspace/linux $ git diffdiff --git a/kernel/pid.c b/kernel/pid.cindex ebdf9c60cd0b..eb80bac14e13 100644--- a/kernel/pid.c+++ b/kernel/pid.c@@ -186,7 +186,6 @@ struct pid *alloc_pid(struct pid_namespace *ns, pid_t *set_tid, for (i = ns-&gt;level; i &gt;= 0; i--) { int tid = 0;- if (set_tid_size) { tid = set_tid[ns-&gt;level - i];@@ -243,6 +242,9 @@ struct pid *alloc_pid(struct pid_namespace *ns, pid_t *set_tid, pid-&gt;numbers[i].nr = nr; pid-&gt;numbers[i].ns = tmp;+ pr_err(&quot;[robert] alloc_pid level = %d, pid-&gt;numbers[i].nr = %d\\n&quot;,+ pid-&gt;level, nr);+ tmp = tmp-&gt;parent; }diff --git a/kernel/pid_namespace.c b/kernel/pid_namespace.cindex ca43239a255a..72ad5a3ddf29 100644--- a/kernel/pid_namespace.c+++ b/kernel/pid_namespace.c@@ -46,6 +46,7 @@ static struct kmem_cache *create_pid_cachep(unsigned int level) if (kc) return kc;+ pr_err(&quot;[robert] create_pid_cachep level = %d\\n&quot;, level); snprintf(name, sizeof(name), &quot;pid_%u&quot;, level + 1); len = sizeof(struct pid) + level * sizeof(struct upid); mutex_lock(&amp;pid_caches_mutex);ubuntu@zeku_server:~/workspace/linux $ demo2shell1: 123456789101112131415161718rlk@rlk-Standard-PC-i440FX-PIIX-1996:/var/crash$ echo $$4724rlk@rlk-Standard-PC-i440FX-PIIX-1996:/var/crash$ sudo unshare --pid --mount-proc --fork /bin/bashxhost: unable to open display &quot;&quot;root@rlk-Standard-PC-i440FX-PIIX-1996:/var/crash# echo $$1root@rlk-Standard-PC-i440FX-PIIX-1996:/var/crash# sudo unshare --pid --mount-proc --fork /bin/bashxhost: unable to open display &quot;&quot;root@rlk-Standard-PC-i440FX-PIIX-1996:/var/crash# echo $$1root@rlk-Standard-PC-i440FX-PIIX-1996:/var/crash# sudo unshare --pid --mount-proc --fork /bin/bashxhost: unable to open display &quot;&quot;root@rlk-Standard-PC-i440FX-PIIX-1996:/var/crash# echo $$1root@rlk-Standard-PC-i440FX-PIIX-1996:/var/crash# sudo unshare --pid --mount-proc --fork /bin/bashxhost: unable to open display &quot;&quot;root@rlk-Standard-PC-i440FX-PIIX-1996:/var/crash# stress -c 10stress: info: [9] dispatching hogs: 10 cpu, 0 io, 0 vm, 0 hdd shell2: 12345stable_kernel@kernel: /var/crash# pstree -p 4724bash(4724)───sudo(4734)───unshare(4735)───bash(4736)───sudo(4744)───unshare(4746)───bash(4747)───sudo(4759)───unshare(4760)───bash(4761)───sudo(4770)──+++stable_kernel@kernel: /var/crash# grep pid /proc/4761/statusNSpid: 4761 21 11 1stable_kernel@kernel: /var/crash# 第四个 bash 在 四个 pid namespace 中 分配 到的 pid 分别是4761 21 11 1 pid_namespace 实现create123456789101112131415161718192021222324252627282930313233343536373839404142copy_processes-&gt;copy_namespaces-&gt;create_new_namespaces-&gt;copy_pid_ns-&gt;create_pid_namespace +----------------------+ | | | | | copy_processes | | | +----------+-----------+ | |+-----------v------------+| || || copy_namespaces || |+----------+-------------+ | | |+----------v--------------+| || || create_new_namespaces || |+----------+--------------+ | | |+----------v------------+| || || copy_pid_ns || |+----------+------------+ | | +--------v------------+ | | | | | create_pid_namespace| | | +---------------------+ create_pid_namespace 1234567891011121314151617181920212223242526272829303132333435363738394041static struct pid_namespace *create_pid_namespace(struct user_namespace *user_ns, struct pid_namespace *parent_pid_ns){ struct pid_namespace *ns; unsigned int level = parent_pid_ns-&gt;level + 1; // level 在 parent的基础上 +1 struct ucounts *ucounts; int err; err = -ENOMEM; ns = kmem_cache_zalloc(pid_ns_cachep, GFP_KERNEL); if (ns == NULL) goto out_dec; idr_init(&amp;ns-&gt;idr); ns-&gt;pid_cachep = create_pid_cachep(level); // 创建 cachep, 是根据 level 创建的。最大level 是 32 if (ns-&gt;pid_cachep == NULL) goto out_free_idr; err = ns_alloc_inum(&amp;ns-&gt;ns); if (err) goto out_free_idr; ns-&gt;ns.ops = &amp;pidns_operations; refcount_set(&amp;ns-&gt;ns.count, 1); ns-&gt;level = level; // 设置 level ns-&gt;parent = get_pid_ns(parent_pid_ns); // 设置 parent ns-&gt;user_ns = get_user_ns(user_ns); ns-&gt;ucounts = ucounts; ns-&gt;pid_allocated = PIDNS_ADDING; return ns;out_free_idr: idr_destroy(&amp;ns-&gt;idr); kmem_cache_free(pid_ns_cachep, ns);out_dec: dec_pid_namespaces(ucounts);out: return ERR_PTR(err);} allocalloc_pid 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566struct pid *alloc_pid(struct pid_namespace *ns, pid_t *set_tid, size_t set_tid_size){ struct pid *pid; enum pid_type type; int i, nr; struct pid_namespace *tmp; struct upid *upid; int retval = -ENOMEM; pid = kmem_cache_alloc(ns-&gt;pid_cachep, GFP_KERNEL); if (!pid) return ERR_PTR(retval); tmp = ns; pid-&gt;level = ns-&gt;level; // 设置 level for (i = ns-&gt;level; i &gt;= 0; i--) { // 循环遍历 各个level，从大到小，pid的值从小到大（debug log中看到） int tid = 0; idr_preload(GFP_KERNEL); spin_lock_irq(&amp;pidmap_lock); if (tid) { nr = idr_alloc(&amp;tmp-&gt;idr, NULL, tid, tid + 1, GFP_ATOMIC); // 从 tmp-&gt;idr 中分配 /* * If ENOSPC is returned it means that the PID is * alreay in use. Return EEXIST in that case. */ if (nr == -ENOSPC) nr = -EEXIST; } else { int pid_min = 1; /* * init really needs pid 1, but after reaching the * maximum wrap back to RESERVED_PIDS */ if (idr_get_cursor(&amp;tmp-&gt;idr) &gt; RESERVED_PIDS) pid_min = RESERVED_PIDS; /* * Store a null pointer so find_pid_ns does not find * a partially initialized PID (see below). */ nr = idr_alloc_cyclic(&amp;tmp-&gt;idr, NULL, pid_min, pid_max, GFP_ATOMIC); } spin_unlock_irq(&amp;pidmap_lock); idr_preload_end(); if (nr &lt; 0) { retval = (nr == -ENOSPC) ? -EAGAIN : nr; goto out_free; } pid-&gt;numbers[i].nr = nr; // pid 结构保存 各个level 的 pid数值，获取pid 的时候需要带上当前在哪一个 level上 pid-&gt;numbers[i].ns = tmp; pr_err(&quot;[robert] alloc_pid level = %d, pid-&gt;numbers[i].nr = %d\\n&quot;, pid-&gt;level, nr); tmp = tmp-&gt;parent; // 获取上层的 pid_namespace }......} unshare 、 nsenter 用法这些都是 shell 提供的和namespace 相关的命令 unshareunshare 解除 namespaces share, 创建新的进程，是Linux容器工作的基础之一。通过 strace 工具看一下 1234stable_kernel@kernel: /var/crash# sudo strace unshare -p -u -i /bin/bashexecve(&quot;/usr/bin/unshare&quot;, [&quot;unshare&quot;, &quot;-p&quot;, &quot;-u&quot;, &quot;-i&quot;, &quot;/bin/bash&quot;], 0x7ffeeb2594b0 /* 22 vars */) = 0unshare(CLONE_NEWUTS|CLONE_NEWIPC|CLONE_NEWPID) = 0execve(&quot;/bin/bash&quot;, [&quot;/bin/bash&quot;], 0x7ffc50847728 /* 22 vars */) = 0 可以看到 unshare(1) 是通过 unshare(2) 实现的mount mount-proc 区别： mount: 创建 mount namespace mount-proc: 自动挂载 /proc 文件系统，无需我们手动执行 mount -t proc proc /proc 命令。 -p -f 一般是一起使用。 nsenter一个最典型的用途就是进入容器的网络命令空间。相当多的容器为了轻量级，是不包含较为基础的命令的，比如说ip address，ping，telnet，ss，tcpdump等等命令，这就给调试容器网络带来相当大的困扰：只能通过docker inspect ContainerID命令获取到容器IP，以及无法测试和其他网络的连通性。这时就可以使用nsenter命令仅进入该容器的网络命名空间，使用宿主机的命令调试容器网络。 此外，nsenter也可以进入mnt, uts, ipc, pid, user命令空间，以及指定根目录和工作目录。shell1: 12345678910root@rlk-Standard-PC-i440FX-PIIX-1996:/var/crash# sleep 100000 &amp;[2] 23root@rlk-Standard-PC-i440FX-PIIX-1996:/var/crash# dmesg |tail -n 30[ 1515.281875] [robert] alloc_pid level = 5, pid-&gt;numbers[i].nr = 23[ 1515.285805] [robert] alloc_pid level = 5, pid-&gt;numbers[i].nr = 33[ 1515.287239] [robert] alloc_pid level = 5, pid-&gt;numbers[i].nr = 54[ 1515.288000] [robert] alloc_pid level = 5, pid-&gt;numbers[i].nr = 64[ 1515.288656] [robert] alloc_pid level = 5, pid-&gt;numbers[i].nr = 74[ 1515.289393] [robert] alloc_pid level = 5, pid-&gt;numbers[i].nr = 3791root@rlk-Standard-PC-i440FX-PIIX-1996:/var/crash# 这是在 一个 独立 namespace 中的 shell，在 global namespace中 pid是 3791 shell2: 1234stable_kernel@kernel: /var/crash# sudo nsenter -p -t3791xhost: unable to open display &quot;&quot;root@rlk-Standard-PC-i440FX-PIIX-1996:/var/crash# ls &amp;[1] 36 通过 strace 工具看一下 123stable_kernel@1kernel: /var/crash# sudo strace nsenter -p -t4123 ls -alexecve(&quot;/usr/bin/nsenter&quot;, [&quot;nsenter&quot;, &quot;-p&quot;, &quot;-t4123&quot;, &quot;ls&quot;, &quot;-al&quot;], 0x7fff5f859620 /* 22 vars */) = 0setns(3, CLONE_NEWPID) = 0 nsenter相当于在setns的示例程序之上做了一层封装，使我们无需指定命名空间的文件描述符，而是指定进程号即可。 unshare 、 setns 、 clone 用法后续有空 用 C实现一下 example。。。 参考：容器实现-namespace","link":"/2021/07/06/Docker%E6%9C%BA%E5%88%B6%E5%88%86%E6%9E%90/namespace%E4%B9%8Bpid/"},{"title":"","text":"sb_bread__bread_gfp__getblk_gfp__getblk_slowgrow_buffersgrow_dev_page—-|-alloc_page_buffers","link":"/2021/07/14/filesystem/buffer_head/"},{"title":"file attr","text":"file attributes 概念除了控制用户和组读取，写和执行权限的文件模式位之外，几个文件系统支持文件属性，可以进一步自定义允许的文件操作。 1Apart from the file mode bits that control user and group read, write and execute permissions, several file systems support file attributes that enable further customization of allowable file operations. This section describes some of these attributes and how to work with them. cp rsync 等操作不会保留 file attributes。只有部分 filesystem 支持这样的属性，常见的 ext2 ext3 ext4 都是支持的。 file attributes FLAGinclude/uapi/linux/fs.h 中 12345678910111213141516171819202122232425262728/* * Inode flags (FS_IOC_GETFLAGS / FS_IOC_SETFLAGS) * * Note: for historical reasons, these flags were originally used and * defined for use by ext2/ext3, and then other file systems started * using these flags so they wouldn't need to write their own version * of chattr/lsattr (which was shipped as part of e2fsprogs). You * should think twice before trying to use these flags in new * contexts, or trying to assign these flags, since they are used both * as the UAPI and the on-disk encoding for ext2/3/4. Also, we are * almost out of 32-bit flags. :-) * * We have recently hoisted FS_IOC_FSGETXATTR / FS_IOC_FSSETXATTR from * XFS to the generic FS level interface. This uses a structure that * has padding and hence has more room to grow, so it may be more * appropriate for many new use cases. * * Please do not change these flags or interfaces before checking with * linux-fsdevel@vger.kernel.org and linux-api@vger.kernel.org. */#define FS_SECRM_FL 0x00000001 /* Secure deletion */#define FS_UNRM_FL 0x00000002 /* Undelete */#define FS_COMPR_FL 0x00000004 /* Compress file */#define FS_SYNC_FL 0x00000008 /* Synchronous updates */#define FS_IMMUTABLE_FL 0x00000010 /* Immutable file */#define FS_APPEND_FL 0x00000020 /* writes to file may only append */#define FS_NODUMP_FL 0x00000040 /* do not dump file */#define FS_NOATIME_FL 0x00000080 /* do not update atime */ 这些 FLAG get set 都是通过 ioctl(FS_IOC_FSGETXATTR / FS_IOC_FSSETXATTR) 命令来实现的。 可以通过chattr的 如下命令修改 123456789101112131415a -- append onlyA -- no atime updatesc -- compressedC -- no copy on writed -- no dumpD -- no synchronous directory updatese -- extent formati -- immutablej -- data journallingP -- project hierarchys -- secure deletionS -- synchronous updatest -- no tail-mergingT -- top of directory hierarchyu -- undeletable demoi 属性就是 immutable，意思是不可变的。 immutable 不可变属性123456789101112131415161718192021222324stable_kernel@kernel: ~/workspace/fs/ext2_dir/123# touch 1stable_kernel@kernel: ~/workspace/fs/ext2_dir/123# touch 2stable_kernel@kernel: ~/workspace/fs/ext2_dir/123# lsattr-------------------- ./1-------------------- ./2stable_kernel@kernel: ~/workspace/fs/ext2_dir/123# chattr +i 1chattr: Operation not permitted while setting flags on 1stable_kernel@1kernel: ~/workspace/fs/ext2_dir/123# sudo chattr +i 1stable_kernel@kernel: ~/workspace/fs/ext2_dir/123# lsattr----i--------------- ./1-------------------- ./2stable_kernel@kernel: ~/workspace/fs/ext2_dir/123# rm 1rm: cannot remove '1': Operation not permittedstable_kernel@1kernel: ~/workspace/fs/ext2_dir/123# rm 2stable_kernel@kernel: ~/workspace/fs/ext2_dir/123# ls1stable_kernel@kernel: ~/workspace/fs/ext2_dir/123# sudo rm 1rm: cannot remove '1': Operation not permittedstable_kernel@1kernel: ~/workspace/fs/ext2_dir/123# chattr -i 1chattr: Operation not permitted while setting flags on 1stable_kernel@1kernel: ~/workspace/fs/ext2_dir/123# sudo chattr -i 1stable_kernel@kernel: ~/workspace/fs/ext2_dir/123# rm 1stable_kernel@kernel: ~/workspace/fs/ext2_dir/123# lsstable_kernel@kernel: ~/workspace/fs/ext2_dir/123# file attributes cp 时保留吗？123456789101112131415stable_kernel@kernel: ~/workspace/fs/ext2_dir/123# touch 1stable_kernel@kernel: ~/workspace/fs/ext2_dir/123# lsattr-------------------- ./1stable_kernel@kernel: ~/workspace/fs/ext2_dir/123# sudo chattr +i 1stable_kernel@kernel: ~/workspace/fs/ext2_dir/123# lsattr----i--------------- ./1stable_kernel@kernel: ~/workspace/fs/ext2_dir/123# cp 1 2stable_kernel@kernel: ~/workspace/fs/ext2_dir/123# lsattr----i--------------- ./1-------------------- ./2stable_kernel@kernel: ~/workspace/fs/ext2_dir/123# mv 1 11mv: cannot move '1' to '11': Operation not permittedstable_kernel@1kernel: ~/workspace/fs/ext2_dir/123# sudo mv 1 11mv: cannot move '1' to '11': Operation not permittedstable_kernel@1kernel: ~/workspace/fs/ext2_dir/123# 可以看到在 执行 cp 命令时， file attributes 是不保留的。 tracechattr 1234567891011stable_kernel@kernel: ~/workspace/fs/ext2_dir/123# sudo strace chattr +i 1execve(&quot;/usr/bin/chattr&quot;, [&quot;chattr&quot;, &quot;+i&quot;, &quot;1&quot;], 0x7fffa9701fd0 /* 22 vars */) = 0......openat(AT_FDCWD, &quot;1&quot;, O_RDONLY|O_NONBLOCK) = 3ioctl(3, FS_IOC_GETFLAGS, 0x7ffe9578ee3c) = 0close(3) = 0lstat(&quot;1&quot;, {st_mode=S_IFREG|0664, st_size=0, ...}) = 0openat(AT_FDCWD, &quot;1&quot;, O_RDONLY|O_NONBLOCK) = 3ioctl(3, FS_IOC_SETFLAGS, 0x7ffe9578ee3c) = 0close(3) = 0...... lsattr 123456789stable_kernel@kernel: ~/workspace/fs/ext2_dir/123# strace lsattrexecve(&quot;/usr/bin/lsattr&quot;, [&quot;lsattr&quot;], 0x7ffca62b1850 /* 37 vars */) = 0openat(AT_FDCWD, &quot;./2&quot;, O_RDONLY|O_NONBLOCK) = 4ioctl(4, FS_IOC_GETFLAGS, 0x7ffc5889dbac) = 0close(4) = 0write(1, &quot;-------------------- ./2\\n&quot;, 25-------------------- ./2) = 25getdents64(3, /* 0 entries */, 32768) = 0close(3) = 0","link":"/2021/07/12/filesystem/file%20attr/"},{"title":"drop_caches  相关","text":"proc文件系统drop_caches 文件位于系统 /proc/sys/vm 123Inspiron-5548@ubuntu: /proc/sys/vm# ls -al | grep drop--w------- 1 root root 0 2月 7 14:14 drop_cachesInspiron-5548@ubuntu: /proc/sys/vm# 可以写入1:drop page cache 2: drop slab 3: drop page cache and slab 等值 代码分析相关代码主要在 fs/drop_caches.c 目录下 1234567891011121314int drop_caches_sysctl_handler(struct ctl_table *table, int write, void *buffer, size_t *length, loff_t *ppos){ ...... if (sysctl_drop_caches &amp; 1) { iterate_supers(drop_pagecache_sb, NULL); count_vm_event(DROP_PAGECACHE); } if (sysctl_drop_caches &amp; 2) { drop_slab(); count_vm_event(DROP_SLAB); } ......} 可以看到：如果写入的值 bit1 是1，就会回收page cache，如果写入的值 bit2 是1，就会回收slab。 pagecache 回收简化一些定义 lock操作，如下： 123456789101112131415161718192021222324static void drop_pagecache_sb(struct super_block *sb, void *unused){ ...... list_for_each_entry(inode, &amp;sb-&gt;s_inodes, i_sb_list) { __iget(inode); invalidate_mapping_pages(inode-&gt;i_mapping, 0, -1); cond_resched(); } iput(toput_inode); ......}void iterate_supers(void (*f)(struct super_block *, void *), void *arg){ ...... list_for_each_entry(sb, &amp;super_blocks, s_list) { sb-&gt;s_count++; down_read(&amp;sb-&gt;s_umount); if (sb-&gt;s_root &amp;&amp; (sb-&gt;s_flags &amp; SB_BORN)) f(sb, arg); up_read(&amp;sb-&gt;s_umount); } ......} 首先遍历super_blocks，然后 遍历sb-&gt;s_inodes，实际的回收 pagecache是针对于 inode-&gt;i_mappings的，对应代码就是 invalidate_mapping_pages(inode-&gt;i_mapping, 0, -1);。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596int remove_mapping(struct address_space *mapping, struct page *page){ if (__remove_mapping(mapping, page, false, NULL)) { /* * Unfreezing the refcount with 1 rather than 2 effectively * drops the pagecache ref for us without requiring another * atomic operation. */ page_ref_unfreeze(page, 1); return 1; } return 0;}static intinvalidate_complete_page(struct address_space *mapping, struct page *page){ int ret; if (page-&gt;mapping != mapping) return 0; if (page_has_private(page) &amp;&amp; !try_to_release_page(page, 0)) return 0; ret = remove_mapping(mapping, page); return ret;}int invalidate_inode_page(struct page *page){ struct address_space *mapping = page_mapping(page); if (!mapping) return 0; if (PageDirty(page) || PageWriteback(page)) return 0; if (page_mapped(page)) return 0; return invalidate_complete_page(mapping, page);}static unsigned long __invalidate_mapping_pages(struct address_space *mapping, pgoff_t start, pgoff_t end, unsigned long *nr_pagevec){ while (index &lt;= end &amp;&amp; pagevec_lookup_entries(&amp;pvec, mapping, index, min(end - index, (pgoff_t)PAGEVEC_SIZE - 1) + 1, indices)) { for (i = 0; i &lt; pagevec_count(&amp;pvec); i++) { struct page *page = pvec.pages[i]; /* We rely upon deletion not changing page-&gt;index */ index = indices[i]; ret = invalidate_inode_page(page); unlock_page(page); /* * Invalidation is a hint that the page is no longer * of interest and try to speed up its reclaim. */ if (!ret) { deactivate_file_page(page); /* It is likely on the pagevec of a remote CPU */ if (nr_pagevec) (*nr_pagevec)++; } count += ret; } pagevec_remove_exceptionals(&amp;pvec); pagevec_release(&amp;pvec); index++; } return count;}/** * invalidate_mapping_pages - Invalidate all the unlocked pages of one inode * @mapping: the address_space which holds the pages to invalidate * @start: the offset 'from' which to invalidate * @end: the offset 'to' which to invalidate (inclusive) * * This function only removes the unlocked pages, if you want to * remove all the pages of one inode, you must call truncate_inode_pages. * * invalidate_mapping_pages() will not block on IO activity. It will not * invalidate pages which are dirty, locked, under writeback or mapped into * pagetables. * * Return: the number of the pages that were invalidated */unsigned long invalidate_mapping_pages(struct address_space *mapping, pgoff_t start, pgoff_t end){ return __invalidate_mapping_pages(mapping, start, end, NULL);}EXPORT_SYMBOL(invalidate_mapping_pages); invalidate_mapping_pages 会尝试 将这个inode对应的pagecacheinvaild掉（就像注释里面写的，除去dirty locked writeback的 page）。 所以 drop cache 不会将脏页给drop掉，要想 drop掉脏页，需要先sync()一下。 slab 回收1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859static unsigned long shrink_slab(gfp_t gfp_mask, int nid, struct mem_cgroup *memcg, int priority){ unsigned long ret, freed = 0; struct shrinker *shrinker; if (!down_read_trylock(&amp;shrinker_rwsem)) goto out; list_for_each_entry(shrinker, &amp;shrinker_list, list) { struct shrink_control sc = { .gfp_mask = gfp_mask, .nid = nid, .memcg = memcg, }; ret = do_shrink_slab(&amp;sc, shrinker, priority); if (ret == SHRINK_EMPTY) ret = 0; freed += ret; /* * Bail out if someone want to register a new shrinker to * prevent the registration from being stalled for long periods * by parallel ongoing shrinking. */ if (rwsem_is_contended(&amp;shrinker_rwsem)) { freed = freed ? : 1; break; } } up_read(&amp;shrinker_rwsem);out: cond_resched(); return freed;}void drop_slab_node(int nid){ unsigned long freed; do { struct mem_cgroup *memcg = NULL; freed = 0; memcg = mem_cgroup_iter(NULL, NULL, NULL); do { freed += shrink_slab(GFP_KERNEL, nid, memcg, 0); } while ((memcg = mem_cgroup_iter(NULL, memcg, NULL)) != NULL); } while (freed &gt; 10);}void drop_slab(void){ int nid; for_each_online_node(nid) drop_slab_node(nid);} drop_slab 过程首先会遍历 online_node，对该 node进行 drop_slab_node。 在drop_slab_node 中，会遍历 memcg进行shrink_slab，退出条件是，这一次回收的 slab objects 小于10. 在 shrink_slab 中，会遍历注册的 shrinker，使用do_shrink_slab 对每个shrinker 进行slab内存回收。 12345678910111213141516171819202122232425262728static unsigned long do_shrink_slab(struct shrink_control *shrinkctl, struct shrinker *shrinker, int priority){ trace_mm_shrink_slab_start(shrinker, shrinkctl, nr, freeable, delta, total_scan, priority); while (total_scan &gt;= batch_size || total_scan &gt;= freeable) { unsigned long ret; unsigned long nr_to_scan = min(batch_size, total_scan); shrinkctl-&gt;nr_to_scan = nr_to_scan; shrinkctl-&gt;nr_scanned = nr_to_scan; ret = shrinker-&gt;scan_objects(shrinker, shrinkctl); if (ret == SHRINK_STOP) break; freed += ret; count_vm_events(SLABS_SCANNED, shrinkctl-&gt;nr_scanned); total_scan -= shrinkctl-&gt;nr_scanned; scanned += shrinkctl-&gt;nr_scanned; cond_resched(); } trace_mm_shrink_slab_end(shrinker, nid, freed, nr, new_nr, total_scan); return freed;} 在 do_shrink_slab 中，有两个 tracepoint点，分别是 start end，可以用来追踪相关事件，实际回收的函数是 shrinker-&gt;scan_objects(shrinker, shrinkctl); 不同的slab，会注册不同的 shrinker.example: 1234567891011121314151617int ext4_es_register_shrinker(struct ext4_sb_info *sbi){ sbi-&gt;s_es_shrinker.scan_objects = ext4_es_scan; sbi-&gt;s_es_shrinker.count_objects = ext4_es_count; sbi-&gt;s_es_shrinker.seeks = DEFAULT_SEEKS; err = register_shrinker(&amp;sbi-&gt;s_es_shrinker);}static int __init hugepage_init(void){ err = register_shrinker(&amp;huge_zero_page_shrinker); if (err) goto err_hzp_shrinker; err = register_shrinker(&amp;deferred_split_shrinker); if (err) goto err_split_shrinker;} 实际上 shrink_slab 不仅仅在 drop_slab_node 中使用，在 内存回收的时候，也会通过shrink_node_memcgs 调用到 12345678910111213static void shrink_node_memcgs(pg_data_t *pgdat, struct scan_control *sc){ ...... shrink_lruvec(lruvec, sc); shrink_slab(sc-&gt;gfp_mask, pgdat-&gt;node_id, memcg, sc-&gt;priority); ......}static void shrink_node(pg_data_t *pgdat, struct scan_control *sc){ shrink_node_memcgs(pgdat, sc);} 来trace一下看看 12345tencent_clould@ubuntu: ~/workspace/tmp# sudo bpftrace -e 'tracepoint:vmscan:mm_shrink_slab_start {printf(&quot;[pid-%d:%s]: trace_mm_shrink_slab_start start\\n&quot;, pid,comm)}'Attaching 1 probe...[pid-1747572:zsh]: trace_mm_shrink_slab_start start[pid-1747572:zsh]: trace_mm_shrink_slab_start start 1234567tencent_clould@ubuntu: ~/workspace/tmp# sudo bpftrace -e 'tracepoint:vmscan:mm_shrink_slab_end {printf(&quot;[pid-%d:%s]: trace_mm_shrink_slab_end total_scan = %d\\n&quot;, pid,comm, args-&gt;total_scan)}'Attaching 1 probe...[pid-1747572:zsh]: trace_mm_shrink_slab_end total_scan = 0[pid-1747572:zsh]: trace_mm_shrink_slab_end total_scan = 0[pid-1747572:zsh]: trace_mm_shrink_slab_end total_scan = 57[pid-1747572:zsh]: trace_mm_shrink_slab_end total_scan = 0[pid-1747572:zsh]: trace_mm_shrink_slab_end total_scan = 1 代码定义大多来自于Linux 5.10","link":"/2021/02/01/filesystem/drop_caches%20%20%E7%9B%B8%E5%85%B3/"},{"title":"benchmark","text":"参考LWN文章 benchmarkhackbenchhackbench 包含在 rt-tests 中，主要是 rt性能 的测试。可以下载源码编译安装或者rt-tests安装包安装 123ubuntu@zeku_server:/tmp/test $ sudo apt-cache search hackbenchrt-tests - Test programs for rt kernelsubuntu@zeku_server:/tmp/test $ 12345ubuntu@zeku_server:/tmp/test $ hackbenchRunning in process mode with 10 groups using 40 file descriptors each (== 400 tasks)Each sender will pass 100 messages of 100 bytesTime: 0.093ubuntu@zeku_server:/tmp/test $ schbenchschbench 需要自己手动编译安装，地址在 这里下载 1git clone git://git.kernel.org/pub/scm/linux/kernel/git/mason/schbench.git 编译 123456789ubuntu@zeku_server:/tmp/schbench $ lsMakefile schbench.c schedstat.pyubuntu@zeku_server:/tmp/schbench $ubuntu@zeku_server:/tmp/schbench $ makegcc -o schbench.o -c -Wall -O2 -g -W -D_GNU_SOURCE -D_LARGEFILE_SOURCE -D_FILE_OFFSET_BITS=64 schbench.cgcc -Wall -O2 -g -W -D_GNU_SOURCE -D_LARGEFILE_SOURCE -D_FILE_OFFSET_BITS=64 -o schbench schbench.o -lpthreadubuntu@zeku_server:/tmp/schbench $ lsMakefile schbench schbench.c schbench.o schedstat.pyubuntu@zeku_server:/tmp/schbench $ 安装 123456ubuntu@zeku_server:/tmp/schbench $ lsMakefile schbench schbench.c schbench.o schedstat.pyubuntu@zeku_server:/tmp/schbench $ sudo mv schbench /usr/bin/ubuntu@zeku_server:/tmp/schbench $ lsMakefile schbench.c schbench.o schedstat.pyubuntu@zeku_server:/tmp/schbench $ 测试结果 12345678910111213141516171819202122232425262728293031ubuntu@zeku_server:/tmp/schbench $ schbenchwarmup done, zeroing statsLatency percentiles (usec) runtime 10 (s) (1649 total samples) 50.0th: 80 (833 samples) 75.0th: 8040 (404 samples) 90.0th: 14352 (248 samples) 95.0th: 16016 (95 samples) *99.0th: 24992 (54 samples) 99.5th: 28960 (7 samples) 99.9th: 31968 (8 samples) min=4, max=31999Latency percentiles (usec) runtime 20 (s) (4929 total samples) 50.0th: 77 (2473 samples) 75.0th: 8056 (1234 samples) 90.0th: 14160 (735 samples) 95.0th: 15824 (247 samples) *99.0th: 24736 (193 samples) 99.5th: 28000 (24 samples) 99.9th: 32032 (22 samples) min=3, max=39652Latency percentiles (usec) runtime 30 (s) (8274 total samples) 50.0th: 75 (4163 samples) 75.0th: 8008 (2059 samples) 90.0th: 14096 (1226 samples) 95.0th: 15760 (415 samples) *99.0th: 24096 (329 samples) 99.5th: 26976 (41 samples) 99.9th: 31968 (34 samples) min=3, max=39652ubuntu@zeku_server:/tmp/schbench $ adrestiaadrestia 需要自己手动编译安装，地址在 这里 下载 1git clone https://github.com/mfleming/adrestia.git 编译 1234ubuntu@zeku_server:/tmp/schbench/adrestia $ makecc -Wall -lpthread -I /tmp/schbench/adrestia/include -g -c -o wake.o wake.ccc -Wall -lpthread -I /tmp/schbench/adrestia/include -g -c -o stats.o stats.ccc -Wall -lpthread -I /tmp/schbench/adrestia/include -g adrestia.c wake.o stats.o -o adrestia -lpthread 测试结果 1234ubuntu@zeku_server:/tmp/schbench/adrestia $ ./adrestiawakeup cost (single): 11uswakeup cost (periodic, 10us): 12usubuntu@zeku_server:/tmp/schbench/adrestia $ rt-apprt-app 需要自己手动编译安装，地址在 这里 下载 编译 安装 1git clone https://github.com/scheduler-tools/rt-app 也可以直接安装包安装 123ubuntu@zeku_server:~/workspace/rt-tests $ sudo apt-cache search rt-apppixelmed-webstart-apps - DICOM implementation containing Image Viewer and a ECG Viewer - jnlprt-app - Test application which simulates a real-time periodic load guide 在 rt-app/doc/tutorial.txt 目录下，有几个比较重要的 配置： 12&quot;cpus&quot; : [2,3], /* set cpu affinity*/....... 运行直接 1rt-app tmp.json 利用rt-app 可以做很多场景的模拟，doc/example目录中有模拟 mp3 codec的json文件，后续再研究。 cyclictest也是 rt-tests 中一个组件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475ubuntu@zeku_server:~/workspace/rt-tests $ ./cyclictest --helpcyclictest V 2.30Usage:cyclictest &lt;options&gt;-a [CPUSET] --affinity Run thread #N on processor #N, if possible, or if CPUSET given, pin threads to that set of processors in round- robin order. E.g. -a 2 pins all threads to CPU 2, but -a 3-5,0 -t 5 will run the first and fifth threads on CPU (0),thread #2 on CPU 3, thread #3 on CPU 4, and thread #5 on CPU 5.-A USEC --aligned=USEC align thread wakeups to a specific offset-b USEC --breaktrace=USEC send break trace command when latency &gt; USEC-c CLOCK --clock=CLOCK select clock 0 = CLOCK_MONOTONIC (default) 1 = CLOCK_REALTIME --default-system Don't attempt to tune the system from cyclictest. Power management is not suppressed. This might give poorer results, but will allow you to discover if you need to tune the system-d DIST --distance=DIST distance of thread intervals in us, default=500-D --duration=TIME specify a length for the test run. Append 'm', 'h', or 'd' to specify minutes, hours or days.-F --fifo=&lt;path&gt; create a named pipe at path and write stats to it-h --histogram=US dump a latency histogram to stdout after the run US is the max latency time to be tracked in microseconds This option runs all threads at the same priority.-H --histofall=US same as -h except with an additional summary column --histfile=&lt;path&gt; dump the latency histogram to &lt;path&gt; instead of stdout-i INTV --interval=INTV base interval of thread in us default=1000 --json=FILENAME write final results into FILENAME, JSON formatted --laptop Save battery when running cyclictest This will give you poorer realtime results but will not drain your battery so quickly --latency=PM_QOS power management latency target value This value is written to /dev/cpu_dma_latency and affects c-states. The default is 0-l LOOPS --loops=LOOPS number of loops: default=0(endless) --mainaffinity=CPUSET Run the main thread on CPU #N. This only affects the main thread and not the measurement threads-m --mlockall lock current and future memory allocations-M --refresh_on_max delay updating the screen until a new max latency is hit. Useful for low bandwidth.-N --nsecs print results in ns instead of us (default us)-o RED --oscope=RED oscilloscope mode, reduce verbose output by RED-p PRIO --priority=PRIO priority of highest prio thread --policy=NAME policy of measurement thread, where NAME may be one of: other, normal, batch, idle, fifo or rr. --priospread spread priority levels starting at specified value-q --quiet print a summary only on exit-r --relative use relative timer instead of absolute-R --resolution check clock resolution, calling clock_gettime() many times. List of clock_gettime() values will be reported with -X --secaligned [USEC] align thread wakeups to the next full second and apply the optional offset-s --system use sys_nanosleep and sys_setitimer-S --smp Standard SMP testing: options -a -t and same priority of all threads --spike=&lt;trigger&gt; record all spikes &gt; trigger --spike-nodes=[num of nodes] These are the maximum number of spikes we can record. The default is 1024 if not specified --smi Enable SMI counting-t --threads one thread per available processor-t [NUM] --threads=NUM number of threads: without NUM, threads = max_cpus without -t default = 1 --tracemark write a trace mark when -b latency is exceeded-u --unbuffered force unbuffered output for live processing-v --verbose output values on stdout for statistics format: n:c:v n=tasknum c=count v=value in us --dbg_cyclictest print info useful for debugging cyclictest-x --posix_timers use POSIX timers instead of clock_nanosleep. 参考Realtime Testing Best Practices","link":"/2021/06/01/schedule/benchmark/"},{"title":"idle 进程的由来","text":"idle 线程linux 中给 per cpu都安排了一个结构runqueue，只要此 runqueue中有 runnable的thread,scheduler 就不会进入 idle状态；相反，只要没有runnable的task，scheduler就会执行 idle task，使得cpu进入 idle模式。 idle 线程如何产生的？单cpu core架构中123456789101112131415161718192021222324void cpu_startup_entry(enum cpuhp_state state){ arch_cpu_idle_prepare(); cpuhp_online_idle(state); while (1) do_idle();}noinline void __ref rest_init(void){ ...... cpu_startup_entry(CPUHP_ONLINE);}void __init __weak arch_call_rest_init(void){ rest_init();}asmlinkage __visible void __init __no_sanitize_address start_kernel(void){ ...... arch_call_rest_init();} smp架构中smp 架构在boot的时候也是单cpu 先boot,然后结果smp_init()，将系统中其他cpu boot起来。所以在 boot cpu中，idle线程还是通过 rest_init() 产生的。 非 boot cpu 的情况下，idle 线程是通过idle_threads_init() 产生的： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152static inline void idle_init(unsigned int cpu){ struct task_struct *tsk = per_cpu(idle_threads, cpu); if (!tsk) { tsk = fork_idle(cpu); if (IS_ERR(tsk)) pr_err(&quot;SMP: fork_idle() failed for CPU %u\\n&quot;, cpu); else per_cpu(idle_threads, cpu) = tsk; }}/** * idle_threads_init - Initialize idle threads for all cpus */void __init idle_threads_init(void){ unsigned int cpu, boot_cpu; boot_cpu = smp_processor_id(); for_each_possible_cpu(cpu) { if (cpu != boot_cpu) idle_init(cpu); }}/* Called by boot processor to activate the rest. */void __init smp_init(void){ ...... idle_threads_init();}static noinline void __init kernel_init_freeable(void){ ...... smp_init();}static int __ref kernel_init(void *unused){ ...... kernel_init_freeable();}noinline void __ref rest_init(void){ ...... pid = kernel_thread(kernel_init, NULL, CLONE_FS);} rest_init – kernel_init – kernel_init_freeable – smp_init – idle_threads_init – idle_init idle 线程干了啥？后续熟悉 arm cpu指令再补充..","link":"/2021/06/15/schedule/idle%20%E8%BF%9B%E7%A8%8B%E7%9A%84%E7%94%B1%E6%9D%A5/"},{"title":"sched_features","text":"WHAT?sched features 是 linux scheduler 的一些 feature实现开关，可以在系统运行中动态开关，相关 feature 在 kernel/sched/features.h 有定义。 feature 拆解每个 feature 对调度行为都有一定影响或者优化，但是需要根据实际情况来选择是否开启 这个 feature. GENTLE_FAIR_SLEEPERS123456/* * Only give sleepers 50% of their service deficit. This allows * them to run sooner, but does not allow tons of sleepers to * rip the spread apart. */SCHED_FEAT(GENTLE_FAIR_SLEEPERS, true) GENTLE_FAIR_SLEEPERS 主要是 减少对 sleep task的 vruntime 补偿。 12345678/* * Halve their sleep time's effect, to allow * for a gentler effect of sleepers: */ if (sched_feat(GENTLE_FAIR_SLEEPERS)) thresh &gt;&gt;= 1; vruntime -= thresh; 实际 使用时，仅仅是将 补偿时间减少一半。 START_DEBIT12345/* * Place new tasks ahead so that they do not starve already running * tasks */SCHED_FEAT(START_DEBIT, true) START_DEBIT 主要是 对 initial task 的一种惩罚，对 vruntime 增加，可以让运行时间减少 和 不会立刻运行。 12345678910111213static voidplace_entity(struct cfs_rq *cfs_rq, struct sched_entity *se, int initial){ u64 vruntime = cfs_rq-&gt;min_vruntime; /* * The 'current' period is already promised to the current tasks, * however the extra weight of the new task will slow them down a * little, place the new task so that it fits in the slot that * stays open at the end. */ if (initial &amp;&amp; sched_feat(START_DEBIT)) vruntime += sched_vslice(cfs_rq, se); NEXT_BUDDY123456/* * Prefer to schedule the task we woke last (assuming it failed * wakeup-preemption), since its likely going to consume data we * touched, increases cache locality. */SCHED_FEAT(NEXT_BUDDY, false) NEXT_BUDDY 主要是 希望可以 尽快调度到 抢占 curr的 task. 1234567static void check_preempt_wakeup(struct rq *rq, struct task_struct *p, int wake_flags){ ...... if (sched_feat(NEXT_BUDDY) &amp;&amp; scale &amp;&amp; !(wake_flags &amp; WF_FORK)) { set_next_buddy(pse); next_buddy_marked = 1; } LAST_BUDDY123456/* * Prefer to schedule the task that ran last (when we did * wake-preempt) as that likely will touch the same data, increases * cache locality. */SCHED_FEAT(LAST_BUDDY, true) LAST_BUDDY 主要是 希望可以尽快调度到上次被 wakeup task preempt 的task。 123456static void check_preempt_wakeup(struct rq *rq, struct task_struct *p, int wake_flags){ ...... if (sched_feat(LAST_BUDDY) &amp;&amp; scale &amp;&amp; entity_is_task(se)) set_last_buddy(se);} CACHE_HOT_BUDDY12345/* * Consider buddies to be cache hot, decreases the likeliness of a * cache buddy being migrated away, increases cache locality. */SCHED_FEAT(CACHE_HOT_BUDDY, true) 认为 [last|next]buddies 是 cache hot的，所以不能让他们 migrate away，增加 cache 命中率。 1234567891011121314/* * Is this task likely cache-hot: */static int task_hot(struct task_struct *p, struct lb_env *env){ ...... /* * Buddy candidates are cache hot: */ if (sched_feat(CACHE_HOT_BUDDY) &amp;&amp; env-&gt;dst_rq-&gt;nr_running &amp;&amp; (&amp;p-&gt;se == cfs_rq_of(&amp;p-&gt;se)-&gt;next || &amp;p-&gt;se == cfs_rq_of(&amp;p-&gt;se)-&gt;last)) return 1;} WAKEUP_PREEMPTION1234/* * Allow wakeup-time preemption of the current task: */SCHED_FEAT(WAKEUP_PREEMPTION, true) 是否可以唤醒强占， 后续补充 HRTICK HRTICK_DL DOUBLE_TICK123SCHED_FEAT(HRTICK, false)SCHED_FEAT(HRTICK_DL, false)SCHED_FEAT(DOUBLE_TICK, false) HRTICK HRTICK_DL DOUBLE_TICK 这三个 feature 主要是利用 rq-&gt;hrtick_timer 优化调度行为的。 12345static void __sched notrace __schedule(bool preempt){ ...... if (sched_feat(HRTICK) || sched_feat(HRTICK_DL)) hrtick_clear(rq); 主要是 dl task 会使用 这个 hrtimer 在dl-&gt;runtime到期的时候 发生一个 tick，使得任务运行时间更加精准 123456789101112static void start_hrtick_dl(struct rq *rq, struct task_struct *p){ hrtick_start(rq, p-&gt;dl.runtime);}static void set_next_task_dl(struct rq *rq, struct task_struct *p, bool first){ p-&gt;se.exec_start = rq_clock_task(rq); if (hrtick_enabled_dl(rq)) start_hrtick_dl(rq, p);} cfs task 会使用 这个 hrtimer 在delta 期望运行时间 到期的时候 发生一个 tick_fair，使得运行时间更加精准。 12345678910111213141516171819202122232425262728static void hrtick_start_fair(struct rq *rq, struct task_struct *p){ struct sched_entity *se = &amp;p-&gt;se; struct cfs_rq *cfs_rq = cfs_rq_of(se); SCHED_WARN_ON(task_rq(p) != rq); if (rq-&gt;cfs.h_nr_running &gt; 1) { u64 slice = sched_slice(cfs_rq, se); u64 ran = se-&gt;sum_exec_runtime - se-&gt;prev_sum_exec_runtime; s64 delta = slice - ran; if (delta &lt; 0) { if (task_current(rq, p)) resched_curr(rq); return; } hrtick_start(rq, delta); }}struct task_struct *pick_next_task_fair(struct rq *rq, struct task_struct *prev, struct rq_flags *rf){ ...... if (hrtick_enabled_fair(rq)) hrtick_start_fair(rq, p);} NONTASK_CAPACITYTTWU_QUEUE12345/* * Queue remote wakeups on the target CPU and process them * using the scheduler IPI. Reduces rq-&gt;lock contention/bounces. */SCHED_FEAT(TTWU_QUEUE, true) TTWU_QUEUE 主要是 使用 IPI 去唤醒 remote queue 上的task，而不是操作rq-&gt;lock，然后操作队列上的 entity，这会减少 rq-&gt;lock的争用。 12345678910111213static bool ttwu_queue_wakelist(struct task_struct *p, int cpu, int wake_flags){ if (sched_feat(TTWU_QUEUE) &amp;&amp; ttwu_queue_cond(cpu, wake_flags)) { if (WARN_ON_ONCE(cpu == smp_processor_id())) return false; sched_clock_cpu(cpu); /* Sync clocks across CPUs */ __ttwu_queue_wakelist(p, cpu, wake_flags); return true; } return false;} SIS_PROPWARN_DOUBLE_CLOCKRT_PUSH_IPIRT_RUNTIME_SHARELB_MINATTACH_AGE_LOADWA_IDLEWA_WEIGHTWA_BIAS","link":"/2021/05/25/schedule/sched_features/"},{"title":"stop_task","text":"stop_taskstop_task 实现在 kernel/sched/stop_task.c 中，在所有调度类中stop_sched_class的优先级是最高的。 123456789#define SCHED_DATA \\ STRUCT_ALIGN(); \\ __begin_sched_classes = .; \\ *(__idle_sched_class) \\ *(__fair_sched_class) \\ *(__rt_sched_class) \\ *(__dl_sched_class) \\ *(__stop_sched_class) \\ __end_sched_classes = .; 其实在 linux中一个 per cpu的 rq中，每个rq只会有一个 stop task 123456789struct rq { /* runqueue lock: */ raw_spinlock_t lock; struct task_struct __rcu *curr; struct task_struct *idle; struct task_struct *stop; ......} 且 stop task never migrates，不会迁移，只会固定在某个 cpu上跑，所以 select_task_rq_stop 实现: 12345static intselect_task_rq_stop(struct task_struct *p, int cpu, int flags){ return task_cpu(p); /* stop tasks as never migrate */} 且 stop task can not be preempted，不会被抢占，check_preempt_curr_stop 实现： 12345static voidcheck_preempt_curr_stop(struct rq *rq, struct task_struct *p, int flags){ /* we're never preempted */} 因为 stop_task， rq上只有一个stop_sched_class的 task，所以 pick_next_task_stop 实现很简单： 12345678static struct task_struct *pick_next_task_stop(struct rq *rq){ if (!sched_stop_runnable(rq)) return NULL; set_next_task_stop(rq, rq-&gt;stop, true); return rq-&gt;stop;} 外部模块使用在 soft lockup 检测模块中，就利用了stop_class 这个调度类。watchdog.c 中使用 1234567891011121314151617181920212223/* watchdog kicker functions */static enum hrtimer_restart watchdog_timer_fn(struct hrtimer *hrtimer){ unsigned long touch_ts, period_ts, now; struct pt_regs *regs = get_irq_regs(); int duration; int softlockup_all_cpu_backtrace = sysctl_softlockup_all_cpu_backtrace; if (!watchdog_enabled) return HRTIMER_NORESTART; /* kick the hardlockup detector */ watchdog_interrupt_count(); /* kick the softlockup detector */ if (completion_done(this_cpu_ptr(&amp;softlockup_completion))) { reinit_completion(this_cpu_ptr(&amp;softlockup_completion)); stop_one_cpu_nowait(smp_processor_id(), softlockup_fn, NULL, this_cpu_ptr(&amp;softlockup_stop_work)); } ......} 其中 stop_one_cpu_nowait 并不直接调用 softlockup_fn函数，只是将他 queue work 而已 1234567891011121314151617181920212223242526272829/* queue @work to @stopper. if offline, @work is completed immediately */static bool cpu_stop_queue_work(unsigned int cpu, struct cpu_stop_work *work){ struct cpu_stopper *stopper = &amp;per_cpu(cpu_stopper, cpu); DEFINE_WAKE_Q(wakeq); unsigned long flags; bool enabled; preempt_disable(); raw_spin_lock_irqsave(&amp;stopper-&gt;lock, flags); enabled = stopper-&gt;enabled; if (enabled) __cpu_stop_queue_work(stopper, work, &amp;wakeq); else if (work-&gt;done) cpu_stop_signal_done(work-&gt;done); raw_spin_unlock_irqrestore(&amp;stopper-&gt;lock, flags); wake_up_q(&amp;wakeq); preempt_enable(); return enabled;}bool stop_one_cpu_nowait(unsigned int cpu, cpu_stop_fn_t fn, void *arg, struct cpu_stop_work *work_buf){ *work_buf = (struct cpu_stop_work){ .fn = fn, .arg = arg, .caller = _RET_IP_, }; return cpu_stop_queue_work(cpu, work_buf);} 可以看到 stopper 是 &amp;per_cpu(cpu_stopper, cpu), 定义是 12345678910111213struct cpu_stopper { struct task_struct *thread; raw_spinlock_t lock; bool enabled; /* is this stopper enabled? */ struct list_head works; /* list of pending works */ struct cpu_stop_work stop_work; /* for stop_cpus */ unsigned long caller; cpu_stop_fn_t fn;};static DEFINE_PER_CPU(struct cpu_stopper, cpu_stopper); 在 每个 CPU 上的 stop_class 进程是 migration/x 123456789static struct smp_hotplug_thread cpu_stop_threads = { .store = &amp;cpu_stopper.thread, .thread_should_run = cpu_stop_should_run, .thread_fn = cpu_stopper_thread, .thread_comm = &quot;migration/%u&quot;, .create = cpu_stop_create, .park = cpu_stop_park, .selfparking = true,}; shell 可以看到 123456789101112131415ubuntu@zeku_server:~/workspace/linux $ ps -aux | grep migrationroot 14 0.0 0.0 0 0 ? S 9月03 0:05 [migration/0]root 19 0.0 0.0 0 0 ? S 9月03 0:05 [migration/1]root 25 0.0 0.0 0 0 ? S 9月03 0:05 [migration/2]root 31 0.0 0.0 0 0 ? S 9月03 0:04 [migration/3]root 37 0.0 0.0 0 0 ? S 9月03 0:04 [migration/4]root 43 0.0 0.0 0 0 ? S 9月03 0:05 [migration/5]root 49 0.0 0.0 0 0 ? S 9月03 0:05 [migration/6]root 55 0.0 0.0 0 0 ? S 9月03 0:05 [migration/7]root 61 0.0 0.0 0 0 ? S 9月03 0:05 [migration/8]root 67 0.0 0.0 0 0 ? S 9月03 0:05 [migration/9]root 73 0.0 0.0 0 0 ? S 9月03 0:05 [migration/10]root 79 0.0 0.0 0 0 ? S 9月03 0:05 [migration/11]ubuntu 35458 0.0 0.0 12224 836 pts/0 S+ 15:26 0:00 grep --color=auto migrationubuntu@zeku_server:~/workspace/linux $","link":"/2021/05/19/schedule/stop_task/"},{"title":"fsck","text":"demo1ext2 for example 以 ext2 filesystem 为例。在qemu环境中 1234567891011121314151617181920212223stable_kernel@kernel: ~/workspace/fs# mkdir ext2_dirstable_kernel@kernel: ~/workspace/fs# dd if=/dev/zero of=ext2.img bs=4k count=10241024+0 records in1024+0 records out4194304 bytes (4.2 MB, 4.0 MiB) copied, 0.00530677 s, 790 MB/sstable_kernel@kernel: ~/workspace/fs# mkfs.ext2 ext2.imgmke2fs 1.45.5 (07-Jan-2020)Discarding device blocks: doneCreating filesystem with 1024 4k blocks and 1024 inodesAllocating group tables: doneWriting inode tables: doneWriting superblocks and filesystem accounting information: donestable_kernel@kernel: ~/workspace/fs# sudo mount ext2.img ext2_dirstable_kernel@kernel: ~/workspace/fs#stable_kernel@kernel: ~/workspace/fs/ext2_dir# echo -n &quot;FFFFFFFF&quot; &gt; filestable_kernel@130kernel: ~/workspace/fs/ext2_dir# ls -alihtotal 28K 2 drwxrwxrwx 3 root root 4.0K 7月 15 11:30 .791692 drwxrwxr-x 3 rlk rlk 4.0K 7月 15 14:07 .. 12 -rw-rw-r-- 1 rlk rlk 8 7月 15 14:09 file 11 drwx------ 2 root root 16K 7月 15 11:28 lost+foundstable_kernel@kernel: ~/workspace/fs/ext2_dir# dumpe2fs 信息 1234567891011stable_kernel@kernel: ~/workspace/fs/ext2_dir# sudo dumpe2fs /dev/loop0 | tail -n 8dumpe2fs 1.45.5 (07-Jan-2020)Group 0: (Blocks 0-1023) Primary superblock at 0, Group descriptors at 1-1 Block bitmap at 2 (+2) Inode bitmap at 3 (+3) Inode table at 4-35 (+4) 981 free blocks, 1012 free inodes, 2 directories Free blocks: 43-1023 Free inodes: 13-1024stable_kernel@kernel: ~/workspace/fs/ext2_dir# Block bitmap 数据 12345678910111213stable_kernel@kernel: ~/workspace/fs/ext2_dir# sudo dd if=/dev/loop0 bs=4096 skip=2 count=1 | xxd -u -l 1281+0 records in1+0 records out4096 bytes (4.1 kB, 4.0 KiB) copied, 5.4055e-05 s, 75.8 MB/s00000000: FFFF FFFF FF07 0000 0000 0000 0000 0000 ................00000010: 0000 0000 0000 0000 0000 0000 0000 0000 ................00000020: 0000 0000 0000 0000 0000 0000 0000 0000 ................00000030: 0000 0000 0000 0000 0000 0000 0000 0000 ................00000040: 0000 0000 0000 0000 0000 0000 0000 0000 ................00000050: 0000 0000 0000 0000 0000 0000 0000 0000 ................00000060: 0000 0000 0000 0000 0000 0000 0000 0000 ................00000070: 0000 0000 0000 0000 0000 0000 0000 0000 ................stable_kernel@kernel: ~/workspace/fs/ext2_dir# Inode bitmap 数据 12345678910111213stable_kernel@kernel: ~/workspace/fs/ext2_dir# sudo dd if=/dev/loop0 bs=4096 skip=3 count=1 | xxd -u -l 1281+0 records in1+0 records out4096 bytes (4.1 kB, 4.0 KiB) copied, 6.0532e-05 s, 67.7 MB/s00000000: FF0F 0000 0000 0000 0000 0000 0000 0000 ................00000010: 0000 0000 0000 0000 0000 0000 0000 0000 ................00000020: 0000 0000 0000 0000 0000 0000 0000 0000 ................00000030: 0000 0000 0000 0000 0000 0000 0000 0000 ................00000040: 0000 0000 0000 0000 0000 0000 0000 0000 ................00000050: 0000 0000 0000 0000 0000 0000 0000 0000 ................00000060: 0000 0000 0000 0000 0000 0000 0000 0000 ................00000070: 0000 0000 0000 0000 0000 0000 0000 0000 ................stable_kernel@kernel: ~/workspace/fs/ext2_dir# Block bitmap 数据被破坏Block bitmap 在 block-2 上，block-size是 4096，我们破坏掉 block-2 上32--63 区域的数据，将其改写为1；需要跨过 4096*2+32 = 257 * 32 12345678stable_kernel@kernel: ~/workspace/fs/ext2_dir# cat fileFFFFFFFF%stable_kernel@kernel: ~/workspace/fs/ext2_dir#stable_kernel@kernel: ~/workspace/fs/ext2_dir# sudo dd if=file of=/dev/loop0 bs=32 seek=257 count=10+1 records in0+1 records out8 bytes copied, 0.00134476 s, 5.9 kB/sstable_kernel@kernel: ~/workspace/fs/ext2_dir# skip 是 跳过 inputseek 是 跳过 output 12seek=N skip N obs-sized blocks at start of outputskip=N skip N ibs-sized blocks at start of input 将 Block bitmap 的 32-63 byte 数据 dd 出来看 12345678910111213stable_kernel@kernel: ~/workspace/fs# sudo dd if=/dev/loop0 bs=4096 skip=2 count=1 | xxd -u -l 1281+0 records in1+0 records out4096 bytes (4.1 kB, 4.0 KiB) copied, 4.4243e-05 s, 92.6 MB/s00000000: FFFF FFFF FF07 0000 0000 0000 0000 0000 ................00000010: 0000 0000 0000 0000 0000 0000 0000 0000 ................00000020: 4646 4646 4646 4646 0000 0000 0000 0000 FFFFFFFF........00000030: 0000 0000 0000 0000 0000 0000 0000 0000 ................00000040: 0000 0000 0000 0000 0000 0000 0000 0000 ................00000050: 0000 0000 0000 0000 0000 0000 0000 0000 ................00000060: 0000 0000 0000 0000 0000 0000 0000 0000 ................00000070: 0000 0000 0000 0000 0000 0000 0000 0000 ................stable_kernel@kernel: ~/workspace/fs# 可以看到与 Block bitmap 没被破坏之前的数据对比。 如何修复？ 123456789101112131415161718FSCK(8) System Administration FSCK(8)NAME fsck - check and repair a Linux filesystemSYNOPSIS fsck [-lsAVRTMNP] [-r [fd]] [-C [fd]] [-t fstype] [filesystem...] [--] [fs-specific-options]DESCRIPTION fsck is used to check and optionally repair one or more Linux filesystems. filesys can be a device name (e.g. /dev/hdc1, /dev/sdb2), a mount point (e.g. /, /usr, /home), or an filesystem label or UUID specifier (e.g. UUID=8868abf6-88c5-4a83-98b8-bfc24057f7bd or LABEL=root). Normally, the fsck program will try to handle filesystems on different physical disk drives in parallel to reduce the total amount of time needed to check all of them. If no filesystems are specified on the command line, and the -A option is not specified, fsck will default to checking filesystems in /etc/fstab serially. This is equivalent to the -As options. The exit code returned by fsck is the sum of the following conditions: 直接检查 ext2.img 1234567891011121314stable_kernel@kernel: ~/workspace/fs# fsck.ext2 ext2.imge2fsck 1.45.5 (07-Jan-2020)ext2.img was not cleanly unmounted, check forced.Pass 1: Checking inodes, blocks, and sizesPass 2: Checking directory structurePass 3: Checking directory connectivityPass 4: Checking reference countsPass 5: Checking group summary informationBlock bitmap differences: -(257--258) -262 -(265--266) -270 -(273--274) -278 -(281--282) -286 -(289--290) -294 -(297--298) -302 -(305--306) -310 -(313--314) -318Fix&lt;y&gt;? yesext2.img: ***** FILE SYSTEM WAS MODIFIED *****ext2.img: 12/1024 files (0.0% non-contiguous), 43/1024 blocksstable_kernel@1kernel: ~/workspace/fs# 可以看到 fsck.ext2 检查出了 Block bitmap 的 differences，选择 y 之后就直接修复了。 然后 dd Block bitmap 数据看 12345678910111213stable_kernel@kernel: ~/workspace/fs# sudo dd if=/dev/loop0 bs=4096 skip=2 count=1 | xxd -u -l 1281+0 records in1+0 records out4096 bytes (4.1 kB, 4.0 KiB) copied, 5.6547e-05 s, 72.4 MB/s00000000: FFFF FFFF FF07 0000 0000 0000 0000 0000 ................00000010: 0000 0000 0000 0000 0000 0000 0000 0000 ................00000020: 0000 0000 0000 0000 0000 0000 0000 0000 ................00000030: 0000 0000 0000 0000 0000 0000 0000 0000 ................00000040: 0000 0000 0000 0000 0000 0000 0000 0000 ................00000050: 0000 0000 0000 0000 0000 0000 0000 0000 ................00000060: 0000 0000 0000 0000 0000 0000 0000 0000 ................00000070: 0000 0000 0000 0000 0000 0000 0000 0000 ................stable_kernel@kernel: ~/workspace/fs# Block bitmap已经恢复到了 被破坏之前的数据 Inode bitmap 数据被破坏Inode bitmap 在 block-3 上，block-size是 4096，我们破坏掉 block-3 上32--63 区域的数据，将其改写为1；需要跨过 4096*3+32 = 385 * 32 12345678stable_kernel@kernel: ~/workspace/fs/ext2_dir# cat fileFFFFFFFF%stable_kernel@kernel: ~/workspace/fs/ext2_dir#stable_kernel@kernel: ~/workspace/fs/ext2_dir# sudo dd if=file of=/dev/loop0 bs=32 seek=385 count=10+1 records in0+1 records out8 bytes copied, 0.00134476 s, 5.9 kB/sstable_kernel@kernel: ~/workspace/fs/ext2_dir# dd可以看到与 Inode bitmap 没被破坏之前的数据对比。 12345678910111213stable_kernel@kernel: ~/workspace/fs# sudo dd if=/dev/loop0 bs=4096 skip=3 count=1 | xxd -u -l 1281+0 records in1+0 records out4096 bytes (4.1 kB, 4.0 KiB) copied, 4.842e-05 s, 84.6 MB/s00000000: FF0F 0000 0000 0000 0000 0000 0000 0000 ................00000010: 0000 0000 0000 0000 0000 0000 0000 0000 ................00000020: 4646 4646 4646 4646 0000 0000 0000 0000 FFFFFFFF........00000030: 0000 0000 0000 0000 0000 0000 0000 0000 ................00000040: 0000 0000 0000 0000 0000 0000 0000 0000 ................00000050: 0000 0000 0000 0000 0000 0000 0000 0000 ................00000060: 0000 0000 0000 0000 0000 0000 0000 0000 ................00000070: 0000 0000 0000 0000 0000 0000 0000 0000 ................stable_kernel@kernel: ~/workspace/fs# 直接可以使用 fsck.ext2 进行修复 1234567891011121314stable_kernel@kernel: ~/workspace/fs# fsck.ext2 ext2.imge2fsck 1.45.5 (07-Jan-2020)ext2.img was not cleanly unmounted, check forced.Pass 1: Checking inodes, blocks, and sizesPass 2: Checking directory structurePass 3: Checking directory connectivityPass 4: Checking reference countsPass 5: Checking group summary informationInode bitmap differences: -(258--259) -263 -(266--267) -271 -(274--275) -279 -(282--283) -287 -(290--291) -295 -(298--299) -303 -(306--307) -311 -(314--315) -319Fix&lt;y&gt;? yesext2.img: ***** FILE SYSTEM WAS MODIFIED *****ext2.img: 12/1024 files (0.0% non-contiguous), 43/1024 blocksstable_kernel@1kernel: ~/workspace/fs# 可以看到 fsck.ext2 检查出了 Inode bitmap 的 differences，选择 y 之后就直接修复了。 demo2ext4 for example","link":"/2021/07/15/filesystem/fsck/"},{"title":"kfence 使用","text":"Kfence 配置看下.config的改变 123456789ubuntu@zeku_server:~/workspace/linux $ diff ./out/.config /tmp/.config4713c4713,4717&lt; # CONFIG_KFENCE is not set---&gt; CONFIG_KFENCE=y&gt; CONFIG_KFENCE_STATIC_KEYS=y&gt; CONFIG_KFENCE_SAMPLE_INTERVAL=100&gt; CONFIG_KFENCE_NUM_OBJECTS=255&gt; CONFIG_KFENCE_STRESS_TEST_FAULTS=0 Kfence 使用写了一个test case，参考代码 oob检测1234567891011121314151617static int kfence_debug_oob(void *data){ char *p[100] = {NULL, }; int i = 0; data = data; msleep(1000 * 5); for (i = 0; i &lt; 10; i++) { p[i] = (char *)kmalloc(32, GFP_KERNEL); p[i][32] = 'a'; } while(!kthread_should_stop()) { msleep_interruptible(1000); } return 1;} 主要就是越界写了 1byte的 kmalloc数据。 insmod 之后立刻报错 1234567891011121314151617[ 779.341929] kfence_debug: loading out-of-tree module taints kernel.[ 784.638024] ==================================================================[ 784.641414] BUG: KFENCE: out-of-bounds write in kfence_debug_oob+0x26/0x60 [kfence_debug][ 784.643513] Out-of-bounds write at 0x00000000514f5e22 (32B right of kfence-#176):[ 784.644142] kfence_debug_oob+0x26/0x60 [kfence_debug][ 784.644144] kthread+0xf9/0x130[ 784.644146] ret_from_fork+0x22/0x30[ 784.644283] kfence-#176 [0x000000001f204f03-0x00000000533650da, size=32, cache=kmalloc-32] allocated by task 3757:[ 784.644288] kfence_debug_oob+0x26/0x60 [kfence_debug][ 784.644289] kthread+0xf9/0x130[ 784.644290] ret_from_fork+0x22/0x30[ 784.644423] CPU: 2 PID: 3757 Comm: kfence_debug Kdump: loaded Tainted: G O 5.13.0-rc5+ #4[ 784.645207] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS 1.13.0-1ubuntu1.1 04/01/2014[ 784.646006] ================================================================== oob 的 代码堆栈直接也打印出来了，十分清晰 use after free 检测1234567891011121314151617181920212223static int kfence_debug_use_after_free(void *data){ char *p[100] = {NULL, }; int i = 0; data = data; msleep(1000 * 5); for (i = 0; i &lt; 10; i++) { p[i] = (char *)kmalloc(32, GFP_KERNEL); p[i][30] = 'a'; } for (i = 0; i &lt; 10; i++) { kfree(p[i]); msleep_interruptible(100); p[i][30] = 'a'; } while(!kthread_should_stop()) { msleep_interruptible(1000); } return 1;} user after free: kmalloc 32 bytes mem =&gt; p assign val to p[30] free p assign val to p[30] insmod 之后立刻报错 1234567891011121314151617181920[ 1779.536493] ==================================================================[ 1779.539850] BUG: KFENCE: use-after-free write in kfence_debug_use_after_free+0x7e/0xd0 [kfence_debug][ 1779.542427] Use-after-free write at 0x0000000013fef528 (in kfence-#218):[ 1779.542985] kfence_debug_use_after_free+0x7e/0xd0 [kfence_debug][ 1779.542987] kthread+0xf9/0x130[ 1779.542990] ret_from_fork+0x22/0x30[ 1779.543123] kfence-#218 [0x000000007dc7fe8d-0x0000000000dd0a85, size=32, cache=kmalloc-32] allocated by task 3868:[ 1779.543127] kfence_debug_use_after_free+0x58/0xd0 [kfence_debug][ 1779.543129] kthread+0xf9/0x130[ 1779.543130] ret_from_fork+0x22/0x30[ 1779.543131] freed by task 3868:[ 1779.543133] kthread+0xf9/0x130[ 1779.543134] ret_from_fork+0x22/0x30[ 1779.543266] CPU: 1 PID: 3868 Comm: kfence_debug Kdump: loaded Tainted: G B O 5.13.0-rc5+ #4[ 1779.544051] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS 1.13.0-1ubuntu1.1 04/01/2014[ 1779.544818] ================================================================== 可以看出这是一个 kmalloc-32 的 mem，且是 allocated by task 3868，freed by task 3868。也可以看到详细堆栈。 double free 检测12345678910111213141516171819202122232425262728static int kfence_debug_double_free(void *data){ char *p[100] = {NULL, }; int i = 0; data = data; msleep(1000 * 5); for (i = 0; i &lt; 10; i++) { p[i] = (char *)kmalloc(32, GFP_KERNEL); p[i][30] = 'a'; } for (i = 0; i &lt; 10; i++) { kfree(p[i]); msleep_interruptible(100); } for (i = 0; i &lt; 10; i++) { kfree(p[i]); msleep_interruptible(100); } while(!kthread_should_stop()) { msleep_interruptible(1000); } return 1;} double free: kmalloc 32 bytes mem =&gt; p free p free p insmod 之后立刻报错 12345678910111213141516171819[ 2489.576810] ==================================================================[ 2489.577668] BUG: KFENCE: invalid free in kthread+0xf9/0x130[ 2489.578464] Invalid free of 0x00000000ed008e01 (in kfence-#160):[ 2489.579128] kthread+0xf9/0x130[ 2489.579131] ret_from_fork+0x22/0x30[ 2489.579305] kfence-#160 [0x00000000ed008e01-0x0000000086ffed42, size=32, cache=kmalloc-32] allocated by task 3914:[ 2489.579310] kfence_debug_double_free+0x58/0xd0 [kfence_debug][ 2489.579312] kthread+0xf9/0x130[ 2489.579313] ret_from_fork+0x22/0x30[ 2489.579315] freed by task 3914:[ 2489.579318] kthread+0xf9/0x130[ 2489.579319] ret_from_fork+0x22/0x30[ 2489.579492] CPU: 3 PID: 3914 Comm: kfence_debug Kdump: loaded Tainted: G B O 5.13.0-rc5+ #4[ 2489.580590] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS 1.13.0-1ubuntu1.1 04/01/2014[ 2489.581698] ================================================================== 可以看出这是一个 kmalloc-32 的 mem，且是 allocated by task 3914, freed by task 3914。也可以看到详细堆栈。 Kfence 原理","link":"/2021/06/11/%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86/kfence%20%E4%BD%BF%E7%94%A8/"},{"title":"file hole","text":"","link":"/2021/07/16/filesystem/file%20hole/"},{"title":"sync  相关","text":"syscallslinux 关于 sync的系统调用很多，都在fs/sync.c 目录下： sync： sync整个系统中所有 sb syncfs：sync 单个文件系统单个 sb fsync: sync 单个文件，包括 mdata fdatasync: sync单个文件，不包括 mdata sync_file_range：sync文件某段内容，是否脏页，应用程序最清楚 sync_file_range2：同 sync_file_range 代码分析syncsync整个系统 1234567891011121314151617181920212223242526272829/* * Sync everything. We start by waking flusher threads so that most of * writeback runs on all devices in parallel. Then we sync all inodes reliably * which effectively also waits for all flusher threads to finish doing * writeback. At this point all data is on disk so metadata should be stable * and we tell filesystems to sync their metadata via -&gt;sync_fs() calls. * Finally, we writeout all block devices because some filesystems (e.g. ext2) * just write metadata (such as inodes or bitmaps) to block device page cache * and do not sync it on their own in -&gt;sync_fs(). */void ksys_sync(void){ int nowait = 0, wait = 1; wakeup_flusher_threads(WB_REASON_SYNC); // 唤醒 flusher 线程， 所有设备上的 大多数 writeback 可以并行发生 iterate_supers(sync_inodes_one_sb, NULL); // sync all inodes iterate_supers(sync_fs_one_sb, &amp;nowait); // sync filesystem metdata iterate_supers(sync_fs_one_sb, &amp;wait); iterate_bdevs(fdatawrite_one_bdev, NULL); // writeout all block devices iterate_bdevs(fdatawait_one_bdev, NULL); if (unlikely(laptop_mode)) laptop_sync_completion();}SYSCALL_DEFINE0(sync){ ksys_sync(); return 0;} fsync 、 fdatasync12345678910111213141516171819202122232425262728293031323334/** * vfs_fsync - perform a fsync or fdatasync on a file * @file: file to sync * @datasync: only perform a fdatasync operation * * Write back data and metadata for @file to disk. If @datasync is * set only metadata needed to access modified file data is written. */int vfs_fsync(struct file *file, int datasync){ return vfs_fsync_range(file, 0, LLONG_MAX, datasync);}static int do_fsync(unsigned int fd, int datasync){ struct fd f = fdget(fd); int ret = -EBADF; if (f.file) { ret = vfs_fsync(f.file, datasync); fdput(f); } return ret;}SYSCALL_DEFINE1(fsync, unsigned int, fd){ return do_fsync(fd, 0);}SYSCALL_DEFINE1(fdatasync, unsigned int, fd){ return do_fsync(fd, 1);} 来trace一下看看 12345tencent_clould@ubuntu: ~/workspace/tmp# sudo bpftrace -e 'tracepoint:vmscan:mm_shrink_slab_start {printf(&quot;[pid-%d:%s]: trace_mm_shrink_slab_start start\\n&quot;, pid,comm)}'Attaching 1 probe...[pid-1747572:zsh]: trace_mm_shrink_slab_start start[pid-1747572:zsh]: trace_mm_shrink_slab_start start 1234567tencent_clould@ubuntu: ~/workspace/tmp# sudo bpftrace -e 'tracepoint:vmscan:mm_shrink_slab_end {printf(&quot;[pid-%d:%s]: trace_mm_shrink_slab_end total_scan = %d\\n&quot;, pid,comm, args-&gt;total_scan)}'Attaching 1 probe...[pid-1747572:zsh]: trace_mm_shrink_slab_end total_scan = 0[pid-1747572:zsh]: trace_mm_shrink_slab_end total_scan = 0[pid-1747572:zsh]: trace_mm_shrink_slab_end total_scan = 57[pid-1747572:zsh]: trace_mm_shrink_slab_end total_scan = 0[pid-1747572:zsh]: trace_mm_shrink_slab_end total_scan = 1","link":"/2021/02/01/filesystem/sync%20%E7%9B%B8%E5%85%B3/"},{"title":"hugepage example","text":"熟悉一项技术的最简单方式就是去使用他，而不是蒙头去看源代码。 概念这只是我当前的理解，TIME: 2021-06-09 123huge page: 对大页的统称，相对于 4k 的小页来说hugetlb : hugetlb 是 huge page 的一种使用机制，静态分配预留机制hugetlbfs: 是一个文件系统，为用户提供使用 预留的 hugetlb 的文件系统 环境准备需要安装 libhugetlbfs，源码在 github 12345ubuntu@zeku_server:/tmp/test $ sudo apt-cache search libhugetlbfslibhugetlbfs-bin - Tools to ease use of hugetlbfslibhugetlbfs-dev - Development files to build programs using libhugetlbfslibhugetlbfs0 - Preload library to back program memory with hugepagesubuntu@zeku_server:/tmp/test $ mount 检查 hugetlbfs 是否挂载 123ubuntu@zeku_server:~ $ mount | grep hugecgroup on /sys/fs/cgroup/hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,hugetlb)hugetlbfs on /dev/hugepages type hugetlbfs (rw,relatime,pagesize=2M) 设置 huge page 的数量设置 huge page 数量前 1234567891011ubuntu@zeku_server:~ $ cat /proc/meminfo | grep HugeAnonHugePages: 0 kBShmemHugePages: 0 kBFileHugePages: 0 kBHugePages_Total: 0HugePages_Free: 0HugePages_Rsvd: 0HugePages_Surp: 0Hugepagesize: 2048 kBHugetlb: 0 kBubuntu@zeku_server:~ $ 设置 huge page 数量 123456789ubuntu@zeku_server:~ $ cat /proc/sys/vm/nr_hugepages0ubuntu@zeku_server:~ $ sudo suroot@ubuntu-HP-ProDesk-680-G4-MT:/home/ubuntu# echo 128 &gt; /proc/sys/vm/nr_hugepagesroot@ubuntu-HP-ProDesk-680-G4-MT:/home/ubuntu# exitexitubuntu@zeku_server:~ $ cat /proc/sys/vm/nr_hugepages128ubuntu@zeku_server:~ $ 设置 huge page 数量后 1234567891011ubuntu@zeku_server:~ $ cat /proc/meminfo | grep HugeAnonHugePages: 0 kBShmemHugePages: 0 kBFileHugePages: 0 kBHugePages_Total: 128HugePages_Free: 128HugePages_Rsvd: 0HugePages_Surp: 0Hugepagesize: 2048 kBHugetlb: 262144 kBubuntu@zeku_server:~ $ test code准备代码 1234567891011121314#include &lt;stdio.h&gt;int main(){ int i, len; int *mem; len = 98 * 1024 * 1024; mem = (int*)malloc(sizeof(int) * len); for(i = 0; i &lt; len; i++) mem[i] = i; getchar(); free(mem); return 0;} 虽然会一开始直接申请 13M 大小的 memory，不做特殊处理情况下，这个程序也不会使用到 huge page。 直接运行直接编译运行之后，查看 /proc/meminfo: 1234567891011ubuntu@zeku_server:/dev/hugepages $ cat /proc/meminfo | grep HugeAnonHugePages: 0 kBShmemHugePages: 0 kBFileHugePages: 0 kBHugePages_Total: 256HugePages_Free: 256HugePages_Rsvd: 0HugePages_Surp: 0Hugepagesize: 2048 kBHugetlb: 524288 kBubuntu@zeku_server:/dev/hugepages $ 为了与 hugetlb 对比性能差距，需要查看 cache miss数据 1234567891011ubuntu@zeku_server:~/workspace/share/test_modules/memory/hugepage/hugetlb $ sudo perf stat -e dtlb_load_misses.miss_causes_a_walk ./a.out Performance counter stats for './a.out': 109,129 dtlb_load_misses.miss_causes_a_walk 1.194155646 seconds time elapsed 0.149356000 seconds user 0.076465000 seconds sys 使用 huge page这里涉及到 LD_PRELOAD：LD_PRELOAD，是个环境变量，用于动态库的加载，动态库加载的优先级最高，一般情况下，其加载顺序为LD_PRELOAD&gt;LD_LIBRARY_PATH&gt;/etc/ld.so.cache&gt;/lib&gt;/usr/lib。程序中我们经常要调用一些外部库的函数，以malloc为例，如果我们有个自定义的malloc函数，把它编译成动态库后，通过LD_PRELOAD加载，当程序中调用malloc函数时，调用的其实是我们自定义的函数，下面以一个例子说明。参考LD_PRELOAD用法 使用 libhugetlbfs.so 重载 libc中的 一些函数。如 malloc free 等;HUGETLB_MORECORE 是讲 1LD_PRELOAD=libhugetlbfs.so HUGETLB_MORECORE=yes ./a.out 运行之后,查看 /proc/meminfo: 1234567891011ubuntu@zeku_server:/dev/hugepages $ cat /proc/meminfo | grep HugeAnonHugePages: 0 kBShmemHugePages: 0 kBFileHugePages: 0 kBHugePages_Total: 256HugePages_Free: 58HugePages_Rsvd: 0HugePages_Surp: 0Hugepagesize: 2048 kBHugetlb: 524288 kBubuntu@zeku_server:/dev/hugepages $ perf 查看 cache miss 等数据 12345678910111213141516ubuntu@zeku_server:~/workspace/share/test_modules/memory/hugepage/hugetlb $ export LD_PRELOAD=libhugetlbfs.soubuntu@zeku_server:~/workspace/share/test_modules/memory/hugepage/hugetlb $ export HUGETLB_MORECORE=yesroot@ubuntu-HP-ProDesk-680-G4-MT:/home/ubuntu/workspace/share/test_modules/memory/hugepage/hugetlb# perf stat -e dtlb_load_misses.miss_causes_a_walk ./a.out Performance counter stats for './a.out': 2,495 dtlb_load_misses.miss_causes_a_walk 1.644226237 seconds time elapsed 0.105609000 seconds user 0.040619000 seconds sysroot@ubuntu-HP-ProDesk-680-G4-MT:/home/ubuntu/workspace/share/test_modules/memory/hugepage/hugetlb# 对比 normal memory hugetlb 两个场景时间: user time| normal memory | hugetlb || —- | —- || 0.149356000 | 0.105609000 | sys time| normal memory | hugetlb || —- | —- || 0.076465000 | 0.040619000 | 不管 sys user 时间，hugetlb 性能都是大大领先的。","link":"/2021/02/01/memory/hugepage/hugepage%20example/"},{"title":"vdso","text":"系统调用linux 系统调用需要用户态，内核态中切换，代价比较大。假设 gettimeofday 这种常用的系统调用，在无人机这种场景中几乎每个service都需要频繁去调用，这样势必会导致较大的开销(无论采用早期的int 0x80/iret中断，还是sysenter/sysexit指令，再到syscall/sysexit指令)。 linux针对这种场景推出了 vsyscall 和 vdso 两种解决方案。 vsyscallvirtual system call，vsyscall 的工作原则其实十分简单。Linux 内核在用户空间映射一个包含一些内核变量及一些系统调用的实现的内存页。 有些系统调用不会向 kernel 传递什么参数，只是读取一些参数比如时间等信息；这种参数都是kernel定期去更新；这类系统调用就很适合放到 vsyscall中, ubuntu 20.04 开启了下面这些 和 vsyscall 相关的宏 1234567ubuntu@zeku_server:~ $ cat /boot/config-5.4.0-72-generic | grep VSYSCALLCONFIG_GENERIC_TIME_VSYSCALL=yCONFIG_X86_VSYSCALL_EMULATION=y# CONFIG_LEGACY_VSYSCALL_EMULATE is not setCONFIG_LEGACY_VSYSCALL_XONLY=y# CONFIG_LEGACY_VSYSCALL_NONE is not setubuntu@zeku_server:~ $ 在 /proc/$self/maps 也可以看到 vsyscall 的 section段 123456789101112131415161718192021222324252627ubuntu@zeku_server:~ $ cat /proc/self/maps558ea272a000-558ea272c000 r--p 00000000 103:02 9830525 /bin/cat558ea272c000-558ea2731000 r-xp 00002000 103:02 9830525 /bin/cat558ea2731000-558ea2734000 r--p 00007000 103:02 9830525 /bin/cat558ea2734000-558ea2735000 r--p 00009000 103:02 9830525 /bin/cat558ea2735000-558ea2736000 rw-p 0000a000 103:02 9830525 /bin/cat558ea34ce000-558ea34ef000 rw-p 00000000 00:00 0 [heap]7f2093d4a000-7f2093d6c000 rw-p 00000000 00:00 07f2093d6c000-7f20945dc000 r--p 00000000 103:02 11280639 /usr/lib/locale/locale-archive7f20945dc000-7f2094601000 r--p 00000000 103:02 7079520 /lib/x86_64-linux-gnu/libc-2.31.so7f2094601000-7f2094779000 r-xp 00025000 103:02 7079520 /lib/x86_64-linux-gnu/libc-2.31.so7f2094779000-7f20947c3000 r--p 0019d000 103:02 7079520 /lib/x86_64-linux-gnu/libc-2.31.so7f20947c3000-7f20947c4000 ---p 001e7000 103:02 7079520 /lib/x86_64-linux-gnu/libc-2.31.so7f20947c4000-7f20947c7000 r--p 001e7000 103:02 7079520 /lib/x86_64-linux-gnu/libc-2.31.so7f20947c7000-7f20947ca000 rw-p 001ea000 103:02 7079520 /lib/x86_64-linux-gnu/libc-2.31.so7f20947ca000-7f20947d0000 rw-p 00000000 00:00 07f20947e4000-7f20947e5000 r--p 00000000 103:02 7077894 /lib/x86_64-linux-gnu/ld-2.31.so7f20947e5000-7f2094808000 r-xp 00001000 103:02 7077894 /lib/x86_64-linux-gnu/ld-2.31.so7f2094808000-7f2094810000 r--p 00024000 103:02 7077894 /lib/x86_64-linux-gnu/ld-2.31.so7f2094811000-7f2094812000 r--p 0002c000 103:02 7077894 /lib/x86_64-linux-gnu/ld-2.31.so7f2094812000-7f2094813000 rw-p 0002d000 103:02 7077894 /lib/x86_64-linux-gnu/ld-2.31.so7f2094813000-7f2094814000 rw-p 00000000 00:00 07ffc7c696000-7ffc7c6b7000 rw-p 00000000 00:00 0 [stack]7ffc7c754000-7ffc7c757000 r--p 00000000 00:00 0 [vvar]7ffc7c757000-7ffc7c758000 r-xp 00000000 00:00 0 [vdso]ffffffffff600000-ffffffffff601000 --xp 00000000 00:00 0 [vsyscall]ubuntu@zeku_server:~ $ vdsovdso 全称 virtual dynamic share object。由于 vsyscall 会将某些系统调用或者数据 固定映射到某个物理地址，这样存在一定的安全隐患，所以后面就出现了 vdso 这样的替代方案。 vdso的随机映射在一定程度上缓解了安全威胁。虽然有了vdso，但从历史兼容性上来讲，vsyscall不能就此完全抛弃，否则将导致一些陈旧的（特别是静态连接的）应用程序无法执行，因此现在在5.4内核上，还可以同时看到vdso和vsyscall。 ubuntu 20.04 开启了下面这些 和 vdso 相关的宏 1234ubuntu@zeku_server:~ $ cat /boot/config-5.4.0-72-generic | grep VDSO# CONFIG_COMPAT_VDSO is not setCONFIG_HAVE_GENERIC_VDSO=yubuntu@zeku_server:~ $ 在 /proc/$self/maps 也可以看到 vdso 的 section 段 123ubuntu@zeku_server:~ $ cat /proc/self/maps | grep vdso7fffd4086000-7fffd4087000 r-xp 00000000 00:00 0 [vdso]ubuntu@zeku_server:~ $ 可以将vdso看成一个shared objdect file（这个文件实际上不存在）,内核将其映射到某个地址空间，被所有程序所共享。（我觉得这里用到了一个技术：多个虚拟页面映射到同一个物理页面。即内核把vdso映射到某个物理页面上，然后所有程序都会有一个页表项指向它，以此来共享，这样每个程序的vdso地址就可以不相同了） 几乎所有程序都会连接 vdso 这个库 123456789ubuntu@zeku_server:~ $ ldd /bin/cat linux-vdso.so.1 (0x00007ffd75ff5000) libc.so.6 =&gt; /lib/x86_64-linux-gnu/libc.so.6 (0x00007f4775d15000) /lib64/ld-linux-x86-64.so.2 (0x00007f4775f29000)ubuntu@zeku_server:~ $ ldd /bin/uname linux-vdso.so.1 (0x00007ffc839be000) libc.so.6 =&gt; /lib/x86_64-linux-gnu/libc.so.6 (0x00007f7a445f0000) /lib64/ld-linux-x86-64.so.2 (0x00007f7a44803000)ubuntu@zeku_server:~ $ testslinux kernel 针对 vdso 有一些测试项目位于 tools/testing/selftests/vDSO， 123456789101112131415161718192021ubuntu@zeku_server:~/workspace/linux/tools/testing/selftests/vDSO $ lsMakefile parse_vdso.h vdso_standalone_test_x86.c vdso_test_clock_getres.c vdso_test_getcpu.cparse_vdso.c vdso_config.h vdso_test_abi.c vdso_test_correctness.c vdso_test_gettimeofday.cubuntu@zeku_server:~/workspace/linux/tools/testing/selftests/vDSO $ makegcc -std=gnu99 vdso_test_gettimeofday.c parse_vdso.c -o /home/ubuntu/workspace/linux/tools/testing/selftests/vDSO/vdso_test_gettimeofdaygcc -std=gnu99 vdso_test_getcpu.c parse_vdso.c -o /home/ubuntu/workspace/linux/tools/testing/selftests/vDSO/vdso_test_getcpugcc -std=gnu99 vdso_test_abi.c parse_vdso.c -o /home/ubuntu/workspace/linux/tools/testing/selftests/vDSO/vdso_test_abigcc -std=gnu99 vdso_test_clock_getres.c -o /home/ubuntu/workspace/linux/tools/testing/selftests/vDSO/vdso_test_clock_getresgcc -std=gnu99 -nostdlib -fno-asynchronous-unwind-tables -fno-stack-protector \\ vdso_standalone_test_x86.c parse_vdso.c \\ -o /home/ubuntu/workspace/linux/tools/testing/selftests/vDSO/vdso_standalone_test_x86gcc -std=gnu99 \\ vdso_test_correctness.c \\ -o /home/ubuntu/workspace/linux/tools/testing/selftests/vDSO/vdso_test_correctness \\ -ldlubuntu@zeku_server:~/workspace/linux/tools/testing/selftests/vDSO $ lsMakefile vdso_standalone_test_x86 vdso_test_clock_getres vdso_test_getcpuparse_vdso.c vdso_standalone_test_x86.c vdso_test_clock_getres.c vdso_test_getcpu.cparse_vdso.h vdso_test_abi vdso_test_correctness vdso_test_gettimeofdayvdso_config.h vdso_test_abi.c vdso_test_correctness.c vdso_test_gettimeofday.cubuntu@zeku_server:~/workspace/linux/tools/testing/selftests/vDSO $","link":"/2020/09/12/%E7%94%A8%E6%88%B7%E7%A9%BA%E9%97%B4/vdso/"},{"title":"perf example","text":"perf_event_open demoperformance counters 是在现代cpu上的一种特殊硬件寄存器，可以对某些特定hw events进行计数，比如指令执行数目，cache miss数量等。perf 子系统给应用层单独开放的接口 是 perf_event_open，会返回 一个fd，后续的操作都是围绕这个 fd来的，可以用 ioctl,fcntl 来控制这个fd。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;stdint.h&gt;#include &lt;unistd.h&gt;#include &lt;sys/syscall.h&gt;#include &lt;linux/perf_event.h&gt;#include &lt;sys/ioctl.h&gt;//目前perf_event_open在glibc中没有封装，需要手工封装一下int perf_event_open(struct perf_event_attr *attr, pid_t pid, int cpu, int group_fd, unsigned long flags){ return syscall(__NR_perf_event_open, attr, pid, cpu, group_fd, flags);}/** * read instructions*/int main(void){ struct perf_event_attr attr; memset(&amp;attr, 0, sizeof(struct perf_event_attr)); attr.size = sizeof(struct perf_event_attr); //监测硬件 attr.type = PERF_TYPE_HARDWARE; //监测指令数 attr.config = PERF_COUNT_HW_INSTRUCTIONS; //初始状态为禁用 attr.disabled = 1; //创建perf文件描述符，其中pid=0,cpu=-1表示监测当前进程，不论运行在那个cpu上 int fd = perf_event_open(&amp;attr, 0, -1, -1, 0); if (fd &lt; 0) { perror(&quot;Cannot open perf fd!&quot;); return 1; } //启用（开始计数） ioctl(fd, PERF_EVENT_IOC_ENABLE, 0); while (1) { uint64_t instructions; //读取最新的计数值 read(fd, &amp;instructions, sizeof(instructions)); printf(&quot;instructions = %ld\\n&quot;, instructions); sleep(1); }} 编译运行 12345ubuntu@zeku_server:~/workspace/share/test_modules/performance/perf_event_open $ gcc perf_count.cubuntu@zeku_server:~/workspace/share/test_modules/performance/perf_event_open $ sudo ./a.outinstructions = 7516instructions = 82866^C 接口分析通过 perf_event_open、ioctl 等实现了 perf 部分功能。调用perf_event_open 之前需要先构造attr，attr属性主要有type，主要有六类 12345678910111213/* * attr.type */enum perf_type_id { PERF_TYPE_HARDWARE = 0, PERF_TYPE_SOFTWARE = 1, PERF_TYPE_TRACEPOINT = 2, PERF_TYPE_HW_CACHE = 3, PERF_TYPE_RAW = 4, PERF_TYPE_BREAKPOINT = 5, PERF_TYPE_MAX, /* non-ABI */}; 每个type还有具体细分如 perf_hw_cache_id perf_hw_cache_op_id perf_hw_cache_op_result_id perf_sw_ids 等： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556/* * Generalized hardware cache events: * * { L1-D, L1-I, LLC, ITLB, DTLB, BPU, NODE } x * { read, write, prefetch } x * { accesses, misses } */enum perf_hw_cache_id { PERF_COUNT_HW_CACHE_L1D = 0, PERF_COUNT_HW_CACHE_L1I = 1, PERF_COUNT_HW_CACHE_LL = 2, PERF_COUNT_HW_CACHE_DTLB = 3, PERF_COUNT_HW_CACHE_ITLB = 4, PERF_COUNT_HW_CACHE_BPU = 5, PERF_COUNT_HW_CACHE_NODE = 6, PERF_COUNT_HW_CACHE_MAX, /* non-ABI */};enum perf_hw_cache_op_id { PERF_COUNT_HW_CACHE_OP_READ = 0, PERF_COUNT_HW_CACHE_OP_WRITE = 1, PERF_COUNT_HW_CACHE_OP_PREFETCH = 2, PERF_COUNT_HW_CACHE_OP_MAX, /* non-ABI */};enum perf_hw_cache_op_result_id { PERF_COUNT_HW_CACHE_RESULT_ACCESS = 0, PERF_COUNT_HW_CACHE_RESULT_MISS = 1, PERF_COUNT_HW_CACHE_RESULT_MAX, /* non-ABI */};/* * Special &quot;software&quot; events provided by the kernel, even if the hardware * does not support performance events. These events measure various * physical and sw events of the kernel (and allow the profiling of them as * well): */enum perf_sw_ids { PERF_COUNT_SW_CPU_CLOCK = 0, PERF_COUNT_SW_TASK_CLOCK = 1, PERF_COUNT_SW_PAGE_FAULTS = 2, PERF_COUNT_SW_CONTEXT_SWITCHES = 3, PERF_COUNT_SW_CPU_MIGRATIONS = 4, PERF_COUNT_SW_PAGE_FAULTS_MIN = 5, PERF_COUNT_SW_PAGE_FAULTS_MAJ = 6, PERF_COUNT_SW_ALIGNMENT_FAULTS = 7, PERF_COUNT_SW_EMULATION_FAULTS = 8, PERF_COUNT_SW_DUMMY = 9, PERF_COUNT_SW_BPF_OUTPUT = 10, PERF_COUNT_SW_CGROUP_SWITCHES = 11, PERF_COUNT_SW_MAX, /* non-ABI */}; pid cpu 参数123456789101112131415161718pid == 0 and cpu == -1 This measures the calling process/thread on any CPU.pid == 0 and cpu &gt;= 0 This measures the calling process/thread only when running on the specified CPU.pid &gt; 0 and cpu == -1 This measures the specified process/thread on any CPU.pid &gt; 0 and cpu &gt;= 0 This measures the specified process/thread only when running on the specified CPU.pid == -1 and cpu &gt;= 0 This measures all processes/threads on the specified CPU. This requires CAP_PERFMON (since Linux 5.8) or CAP_SYS_ADMIN capability or a /proc/sys/kernel/perf_event_paranoid value of less than 1.pid == -1 and cpu == -1 This setting is invalid and will return an error. perf tool 分析perf tool 提供了多种多样的功能，是使用 perf tool的关键 123456789101112131415161718192021222324252627282930313233343536static struct cmd_struct commands[] = { { &quot;buildid-cache&quot;, cmd_buildid_cache, 0 }, { &quot;buildid-list&quot;, cmd_buildid_list, 0 }, { &quot;config&quot;, cmd_config, 0 }, { &quot;c2c&quot;, cmd_c2c, 0 }, { &quot;diff&quot;, cmd_diff, 0 }, { &quot;evlist&quot;, cmd_evlist, 0 }, { &quot;help&quot;, cmd_help, 0 }, { &quot;kallsyms&quot;, cmd_kallsyms, 0 }, { &quot;list&quot;, cmd_list, 0 }, { &quot;record&quot;, cmd_record, 0 }, { &quot;report&quot;, cmd_report, 0 }, { &quot;bench&quot;, cmd_bench, 0 }, { &quot;stat&quot;, cmd_stat, 0 }, { &quot;timechart&quot;, cmd_timechart, 0 }, { &quot;top&quot;, cmd_top, 0 }, { &quot;annotate&quot;, cmd_annotate, 0 }, { &quot;version&quot;, cmd_version, 0 }, { &quot;script&quot;, cmd_script, 0 }, { &quot;sched&quot;, cmd_sched, 0 },#ifdef HAVE_LIBELF_SUPPORT { &quot;probe&quot;, cmd_probe, 0 },#endif { &quot;kmem&quot;, cmd_kmem, 0 }, { &quot;lock&quot;, cmd_lock, 0 }, { &quot;kvm&quot;, cmd_kvm, 0 }, { &quot;test&quot;, cmd_test, 0 },#if defined(HAVE_LIBAUDIT_SUPPORT) || defined(HAVE_SYSCALL_TABLE_SUPPORT) { &quot;trace&quot;, cmd_trace, 0 },#endif { &quot;inject&quot;, cmd_inject, 0 }, { &quot;mem&quot;, cmd_mem, 0 }, { &quot;data&quot;, cmd_data, 0 }, { &quot;ftrace&quot;, cmd_ftrace, 0 }, { &quot;daemon&quot;, cmd_daemon, 0 },}; 参考perf-perf stat用户层代码分析 参考design.txt","link":"/2021/07/15/%E5%86%85%E6%A0%B8%E8%A7%82%E6%B5%8B/perf%E7%9B%B8%E5%85%B3/perf%20example/"},{"title":"ext2 数据结构","text":"struct对于文件系统 struct，首先要区分好 他们在 磁盘上，还是在 memory中。 磁盘上 ext2 inode 相关结构ext2_inode 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253struct ext2_inode { __le16 i_mode; /* File mode */ __le16 i_uid; /* Low 16 bits of Owner Uid */ __le32 i_size; /* Size in bytes */ __le32 i_atime; /* Access time */ __le32 i_ctime; /* Creation time */ __le32 i_mtime; /* Modification time */ __le32 i_dtime; /* Deletion Time */ __le16 i_gid; /* Low 16 bits of Group Id */ __le16 i_links_count; /* Links count */ __le32 i_blocks; /* Blocks count */ __le32 i_flags; /* File flags */ union { struct { __le32 l_i_reserved1; } linux1; struct { __le32 h_i_translator; } hurd1; struct { __le32 m_i_reserved1; } masix1; } osd1; /* OS dependent 1 */ __le32 i_block[EXT2_N_BLOCKS];/* Pointers to blocks */ __le32 i_generation; /* File version (for NFS) */ __le32 i_file_acl; /* File ACL */ __le32 i_dir_acl; /* Directory ACL */ __le32 i_faddr; /* Fragment address */ union { struct { __u8 l_i_frag; /* Fragment number */ __u8 l_i_fsize; /* Fragment size */ __u16 i_pad1; __le16 l_i_uid_high; /* these 2 fields */ __le16 l_i_gid_high; /* were reserved2[0] */ __u32 l_i_reserved2; } linux2; struct { __u8 h_i_frag; /* Fragment number */ __u8 h_i_fsize; /* Fragment size */ __le16 h_i_mode_high; __le16 h_i_uid_high; __le16 h_i_gid_high; __le32 h_i_author; } hurd2; struct { __u8 m_i_frag; /* Fragment number */ __u8 m_i_fsize; /* Fragment size */ __u16 m_pad1; __u32 m_i_reserved2[2]; } masix2; } osd2; /* OS dependent 2 */}; 磁盘上 ext2 super_block 相关结构ext2_super_block 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071struct ext2_super_block { __le32 s_inodes_count; /* Inodes count */ __le32 s_blocks_count; /* Blocks count */ __le32 s_r_blocks_count; /* Reserved blocks count */ __le32 s_free_blocks_count; /* Free blocks count */ __le32 s_free_inodes_count; /* Free inodes count */ __le32 s_first_data_block; /* First Data Block */ __le32 s_log_block_size; /* Block size */ __le32 s_log_frag_size; /* Fragment size */ __le32 s_blocks_per_group; /* # Blocks per group */ __le32 s_frags_per_group; /* # Fragments per group */ __le32 s_inodes_per_group; /* # Inodes per group */ __le32 s_mtime; /* Mount time */ __le32 s_wtime; /* Write time */ __le16 s_mnt_count; /* Mount count */ __le16 s_max_mnt_count; /* Maximal mount count */ __le16 s_magic; /* Magic signature */ __le16 s_state; /* File system state */ __le16 s_errors; /* Behaviour when detecting errors */ __le16 s_minor_rev_level; /* minor revision level */ __le32 s_lastcheck; /* time of last check */ __le32 s_checkinterval; /* max. time between checks */ __le32 s_creator_os; /* OS */ __le32 s_rev_level; /* Revision level */ __le16 s_def_resuid; /* Default uid for reserved blocks */ __le16 s_def_resgid; /* Default gid for reserved blocks */ /* * These fields are for EXT2_DYNAMIC_REV superblocks only. * * Note: the difference between the compatible feature set and * the incompatible feature set is that if there is a bit set * in the incompatible feature set that the kernel doesn't * know about, it should refuse to mount the filesystem. * * e2fsck's requirements are more strict; if it doesn't know * about a feature in either the compatible or incompatible * feature set, it must abort and not try to meddle with * things it doesn't understand... */ __le32 s_first_ino; /* First non-reserved inode */ __le16 s_inode_size; /* size of inode structure */ __le16 s_block_group_nr; /* block group # of this superblock */ __le32 s_feature_compat; /* compatible feature set */ __le32 s_feature_incompat; /* incompatible feature set */ __le32 s_feature_ro_compat; /* readonly-compatible feature set */ __u8 s_uuid[16]; /* 128-bit uuid for volume */ char s_volume_name[16]; /* volume name */ char s_last_mounted[64]; /* directory where last mounted */ __le32 s_algorithm_usage_bitmap; /* For compression */ /* * Performance hints. Directory preallocation should only * happen if the EXT2_COMPAT_PREALLOC flag is on. */ __u8 s_prealloc_blocks; /* Nr of blocks to try to preallocate*/ __u8 s_prealloc_dir_blocks; /* Nr to preallocate for dirs */ __u16 s_padding1; /* * Journaling support valid if EXT3_FEATURE_COMPAT_HAS_JOURNAL set. */ __u8 s_journal_uuid[16]; /* uuid of journal superblock */ __u32 s_journal_inum; /* inode number of journal file */ __u32 s_journal_dev; /* device number of journal file */ __u32 s_last_orphan; /* start of list of inodes to delete */ __u32 s_hash_seed[4]; /* HTREE hash seed */ __u8 s_def_hash_version; /* Default hash version to use */ __u8 s_reserved_char_pad; __u16 s_reserved_word_pad; __le32 s_default_mount_opts; __le32 s_first_meta_bg; /* First metablock block group */ __u32 s_reserved[190]; /* Padding to the end of the block */}; 磁盘上 ext2 dir_entry 相关结构ext2_dir_entry ext2_dir_entry_2 1234567891011121314151617181920struct ext2_dir_entry { __le32 inode; /* Inode number */ __le16 rec_len; /* Directory entry length */ __le16 name_len; /* Name length */ char name[]; /* File name, up to EXT2_NAME_LEN */};/* * The new version of the directory entry. Since EXT2 structures are * stored in intel byte order, and the name_len field could never be * bigger than 255 chars, it's safe to reclaim the extra byte for the * file_type field. */struct ext2_dir_entry_2 { __le32 inode; /* Inode number */ __le16 rec_len; /* Directory entry length */ __u8 name_len; /* Name length */ __u8 file_type; char name[]; /* File name, up to EXT2_NAME_LEN */}; memory中 ext2 inode data 相关结构ext2_inode_info 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152struct ext2_inode_info { __le32 i_data[15]; __u32 i_flags; __u32 i_faddr; __u8 i_frag_no; __u8 i_frag_size; __u16 i_state; __u32 i_file_acl; __u32 i_dir_acl; __u32 i_dtime; /* * i_block_group is the number of the block group which contains * this file's inode. Constant across the lifetime of the inode, * it is used for making block allocation decisions - we try to * place a file's data blocks near its inode block, and new inodes * near to their parent directory's inode. */ __u32 i_block_group; /* block reservation info */ struct ext2_block_alloc_info *i_block_alloc_info; __u32 i_dir_start_lookup;#ifdef CONFIG_EXT2_FS_XATTR /* * Extended attributes can be read independently of the main file * data. Taking i_mutex even when reading would cause contention * between readers of EAs and writers of regular file data, so * instead we synchronize on xattr_sem when reading or changing * EAs. */ struct rw_semaphore xattr_sem;#endif rwlock_t i_meta_lock;#ifdef CONFIG_FS_DAX struct rw_semaphore dax_sem;#endif /* * truncate_mutex is for serialising ext2_truncate() against * ext2_getblock(). It also protects the internals of the inode's * reservation data structures: ext2_reserve_window and * ext2_reserve_window_node. */ struct mutex truncate_mutex; struct inode vfs_inode; struct list_head i_orphan; /* unlinked but open inodes */#ifdef CONFIG_QUOTA struct dquot *i_dquot[MAXQUOTAS];#endif}; struct opssuper block 相关ext2的 super_operations 123456789101112131415161718static const struct super_operations ext2_sops = { .alloc_inode = ext2_alloc_inode, .free_inode = ext2_free_in_core_inode, .write_inode = ext2_write_inode, .evict_inode = ext2_evict_inode, .put_super = ext2_put_super, .sync_fs = ext2_sync_fs, .freeze_fs = ext2_freeze, .unfreeze_fs = ext2_unfreeze, .statfs = ext2_statfs, .remount_fs = ext2_remount, .show_options = ext2_show_options,#ifdef CONFIG_QUOTA .quota_read = ext2_quota_read, .quota_write = ext2_quota_write, .get_dquots = ext2_get_dquots,#endif}; nfsd 相关for nfsd to communicate with file systems 12345static const struct export_operations ext2_export_ops = { .fh_to_dentry = ext2_fh_to_dentry, .fh_to_parent = ext2_fh_to_parent, .get_parent = ext2_get_parent,}; quotactl_ops 相关Operations handling requests from userspace 12345678910static const struct quotactl_ops ext2_quotactl_ops = { .quota_on = ext2_quota_on, .quota_off = ext2_quota_off, .quota_sync = dquot_quota_sync, .get_state = dquot_get_state, .set_info = dquot_set_dqinfo, .get_dqblk = dquot_get_dqblk, .set_dqblk = dquot_set_dqblk, .get_nextdqblk = dquot_get_next_dqblk,}; inode 属性相关file 12345678910const struct inode_operations ext2_file_inode_operations = { .listxattr = ext2_listxattr, .getattr = ext2_getattr, .setattr = ext2_setattr, .get_acl = ext2_get_acl, .set_acl = ext2_set_acl, .fiemap = ext2_fiemap, .fileattr_get = ext2_fileattr_get, .fileattr_set = ext2_fileattr_set,}; dir 12345678910111213141516171819const struct inode_operations ext2_dir_inode_operations = { .create = ext2_create, .lookup = ext2_lookup, .link = ext2_link, .unlink = ext2_unlink, .symlink = ext2_symlink, .mkdir = ext2_mkdir, .rmdir = ext2_rmdir, .mknod = ext2_mknod, .rename = ext2_rename, .listxattr = ext2_listxattr, .getattr = ext2_getattr, .setattr = ext2_setattr, .get_acl = ext2_get_acl, .set_acl = ext2_set_acl, .tmpfile = ext2_tmpfile, .fileattr_get = ext2_fileattr_get, .fileattr_set = ext2_fileattr_set,}; special inode 1234567const struct inode_operations ext2_special_inode_operations = { .listxattr = ext2_listxattr, .getattr = ext2_getattr, .setattr = ext2_setattr, .get_acl = ext2_get_acl, .set_acl = ext2_set_acl,}; symlink 123456const struct inode_operations ext2_symlink_inode_operations = { .get_link = page_get_link, .getattr = ext2_getattr, .setattr = ext2_setattr, .listxattr = ext2_listxattr,}; fast_symlink 123456const struct inode_operations ext2_fast_symlink_inode_operations = { .get_link = simple_get_link, .getattr = ext2_getattr, .setattr = ext2_setattr, .listxattr = ext2_listxattr,}; file 操作相关ext2_file_operations 12345678910111213141516const struct file_operations ext2_file_operations = { .llseek = generic_file_llseek, .read_iter = ext2_file_read_iter, .write_iter = ext2_file_write_iter, .unlocked_ioctl = ext2_ioctl,#ifdef CONFIG_COMPAT .compat_ioctl = ext2_compat_ioctl,#endif .mmap = ext2_file_mmap, .open = dquot_file_open, .release = ext2_release_file, .fsync = ext2_fsync, .get_unmapped_area = thp_get_unmapped_area, .splice_read = generic_file_splice_read, .splice_write = iter_file_splice_write,}; ext2_dir_operations 12345678910const struct file_operations ext2_dir_operations = { .llseek = generic_file_llseek, .read = generic_read_dir, .iterate_shared = ext2_readdir, .unlocked_ioctl = ext2_ioctl,#ifdef CONFIG_COMPAT .compat_ioctl = ext2_compat_ioctl,#endif .fsync = ext2_fsync,}; 地址空间相关ext2_aops 1234567891011121314const struct address_space_operations ext2_aops = { .set_page_dirty = __set_page_dirty_buffers, .readpage = ext2_readpage, .readahead = ext2_readahead, .writepage = ext2_writepage, .write_begin = ext2_write_begin, .write_end = ext2_write_end, .bmap = ext2_bmap, .direct_IO = ext2_direct_IO, .writepages = ext2_writepages, .migratepage = buffer_migrate_page, .is_partially_uptodate = block_is_partially_uptodate, .error_remove_page = generic_error_remove_page,}; ext2_nobh_aops 12345678910111213const struct address_space_operations ext2_nobh_aops = { .set_page_dirty = __set_page_dirty_buffers, .readpage = ext2_readpage, .readahead = ext2_readahead, .writepage = ext2_nobh_writepage, .write_begin = ext2_nobh_write_begin, .write_end = nobh_write_end, .bmap = ext2_bmap, .direct_IO = ext2_direct_IO, .writepages = ext2_writepages, .migratepage = buffer_migrate_page, .error_remove_page = generic_error_remove_page,}; ext2_dax_aops 123456static const struct address_space_operations ext2_dax_aops = { .writepages = ext2_dax_writepages, .direct_IO = noop_direct_IO, .set_page_dirty = __set_page_dirty_no_writeback, .invalidatepage = noop_invalidatepage,};","link":"/2021/07/12/filesystem/ext2/ext2%20%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"title":"perf kernel subsys","text":"perf 架构perf的基本包装模型是这样的，对每一个event分配一个对应的perf_event结构。所有对event的操作都是围绕perf_event来展开的： 通过perf_event_open系统调用分配到perf_event以后，会返回一个文件句柄fd，这样这个perf_event结构可以通过read/write/ioctl/mmap通用文件接口来操作。perf_event提供两种类型的trace数据：count和sample。count只是记录了event的发生次数，sample记录了大量信息(比如：IP、ADDR、TID、TIME、CPU、BT)。如果需要使用sample功能，需要给perf_event分配ringbuffer空间，并且把这部分空间通过mmap映射到用户空间。这和定位问题时从粗到细的思路是相符的，首先从counter的比例上找出问题热点在哪个模块，再使用详细记录抓取更多信息来进一步定位。具体分别对应“perf stat”和“perf record/report”命令。perf的开销应该是比ftrace要大的，因为它给每个event都独立一套数据结构perf_event，对应独立的attr和pmu。在数据记录时的开销肯定大于ftrace，但是每个event的ringbuffer是独立的所以也不需要ftrace复杂的ringbuffer操作。perf也有比ftrace开销小的地方，它的sample数据存储的ringbuffer空间会通过mmap映射到到用户态，这样在读取数据的时候就会少一次拷贝。不过perf的设计初衷也不是让成百上千的event同时使用，只会挑出一些event重点debug。 pmu 硬件performance monitor unit armarm 有很多pmu，大致分为 普通PMU、DSU-PMU、SPE-PMU、SMMU-PMU等。 SPE(Statistical Profiling Extension (SPE) Performance Monitor Units): 1234spe-pmu { compatible = &quot;arm,statistical-profiling-extension-v1&quot;; interrupts = &lt;GIC_PPI 05 IRQ_TYPE_LEVEL_HIGH &amp;part1&gt;;}; 参考spe-pmu DSU(DynamIQ Shared Unit (DSU) Performance Monitor Unit): 12345dsu-pmu-0 { compatible = &quot;arm,dsu-pmu&quot;; interrupts = &lt;GIC_SPI 02 IRQ_TYPE_LEVEL_HIGH&gt;; cpus = &lt;&amp;cpu_0&gt;, &lt;&amp;cpu_1&gt;;}; 参考dsu-pmu arm v9 2021 新feature Branch events Bulk memory operations Stall events Atomics events Data source or target (cache hit) events Cache line state tracking TLB events Latency events SPE 的扩展 Counter packet size Hardware management of the dirty state and Access Flag External abort handling Second address packet generation Sampling Tag operations Sampling memory copy/set operations Trace and Branch Recording Extensions Hardware management of the dirty state and Access Flag External abort handling Trace Architecture updates for the 2021 Architecture Extensions Self-hosted branch recording of EL3 using BRBE perf 软件结构12345678910111213141516171819202122232425 +-----------------+ | | | Application | userspace +-----------------+ +-----------------------------+ | | | perf | | | +-----------------------------+--------------------------------------------------------------------------------------------------- +------------------------+ +-------------------------------------+ | +-----&gt;| | | kernel/event/core.c | | arch/arm64/kernel/perf_event.c | +------------------------+&lt;-----+ |kernelspace +-------------------------------------+ +-----------------------------------------------------+ | | | PMU counters | | | +-----------------------------------------------------+ userspace perf 代码在 tools/perf 目录下 kernelspace perf 框架代码在 kernel/events 目录下，kernel/Makefile 中可以看出events目录都是和 perf相关的 1obj-$(CONFIG_PERF_EVENTS) += events/ arm64 架构相关代码在 arch/arm64/kernel/perf_event.c 中，arch/arm64/kernel/Makefile 可以看出 12obj-$(CONFIG_PERF_EVENTS) += perf_regs.o perf_callchain.oobj-$(CONFIG_HW_PERF_EVENTS) += perf_event.o arm 相关drivers 代码在 drivers/perf 目录中drivers/perf/Makefile 可以看到如下各种 PMU CONFIG，有DSU SPE 等 123456789101112131415obj-$(CONFIG_ARM_CCI_PMU) += arm-cci.oobj-$(CONFIG_ARM_CCN) += arm-ccn.oobj-$(CONFIG_ARM_CMN) += arm-cmn.oobj-$(CONFIG_ARM_DSU_PMU) += arm_dsu_pmu.oobj-$(CONFIG_ARM_PMU) += arm_pmu.o arm_pmu_platform.oobj-$(CONFIG_ARM_PMU_ACPI) += arm_pmu_acpi.oobj-$(CONFIG_ARM_SMMU_V3_PMU) += arm_smmuv3_pmu.oobj-$(CONFIG_FSL_IMX8_DDR_PMU) += fsl_imx8_ddr_perf.oobj-$(CONFIG_HISI_PMU) += hisilicon/obj-$(CONFIG_QCOM_L2_PMU) += qcom_l2_pmu.oobj-$(CONFIG_QCOM_L3_PMU) += qcom_l3_pmu.oobj-$(CONFIG_THUNDERX2_PMU) += thunderx2_pmu.oobj-$(CONFIG_XGENE_PMU) += xgene_pmu.oobj-$(CONFIG_ARM_SPE_PMU) += arm_spe_pmu.oobj-$(CONFIG_ARM_DMC620_PMU) += arm_dmc620_pmu.o perf initperf_event_init注册不同 类型(sw)的 pmu，如perf_swevent，perf_tracepoint，perf_kprobe，perf_uprobe，perf_cpu_clock等 1234567891011121314151617void __init perf_event_init(void){ perf_event_init_all_cpus(); init_srcu_struct(&amp;pmus_srcu); perf_pmu_register(&amp;perf_swevent, &quot;software&quot;, PERF_TYPE_SOFTWARE); perf_pmu_register(&amp;perf_cpu_clock, NULL, -1); perf_pmu_register(&amp;perf_task_clock, NULL, -1); perf_tp_register(); perf_event_init_cpu(smp_processor_id()); register_reboot_notifier(&amp;perf_reboot_notifier); ret = init_hw_breakpoint(); WARN(ret, &quot;hw_breakpoint initialization failed with: %d&quot;, ret); perf_event_cache = KMEM_CACHE(perf_event, SLAB_PANIC); ......} arm 硬件架构相关的pmu 注册在 12345678910111213141516171819202122232425static const struct of_device_id armv8_pmu_of_device_ids[] = { {.compatible = &quot;arm,armv8-pmuv3&quot;, .data = armv8_pmuv3_init}, {.compatible = &quot;arm,cortex-a34-pmu&quot;, .data = armv8_a34_pmu_init}, {.compatible = &quot;arm,cortex-a35-pmu&quot;, .data = armv8_a35_pmu_init}, {.compatible = &quot;arm,cortex-a53-pmu&quot;, .data = armv8_a53_pmu_init}, {.compatible = &quot;arm,cortex-a55-pmu&quot;, .data = armv8_a55_pmu_init}, {.compatible = &quot;arm,cortex-a57-pmu&quot;, .data = armv8_a57_pmu_init}, {.compatible = &quot;arm,cortex-a65-pmu&quot;, .data = armv8_a65_pmu_init}, {.compatible = &quot;arm,cortex-a72-pmu&quot;, .data = armv8_a72_pmu_init}, {.compatible = &quot;arm,cortex-a73-pmu&quot;, .data = armv8_a73_pmu_init}, {.compatible = &quot;arm,cortex-a75-pmu&quot;, .data = armv8_a75_pmu_init}, {.compatible = &quot;arm,cortex-a76-pmu&quot;, .data = armv8_a76_pmu_init}, {.compatible = &quot;arm,cortex-a77-pmu&quot;, .data = armv8_a77_pmu_init}, {.compatible = &quot;arm,cortex-a78-pmu&quot;, .data = armv8_a78_pmu_init}, {.compatible = &quot;arm,neoverse-e1-pmu&quot;, .data = armv8_e1_pmu_init}, {.compatible = &quot;arm,neoverse-n1-pmu&quot;, .data = armv8_n1_pmu_init}, {.compatible = &quot;cavium,thunder-pmu&quot;, .data = armv8_thunder_pmu_init}, {.compatible = &quot;brcm,vulcan-pmu&quot;, .data = armv8_vulcan_pmu_init}, {},};static int armv8_pmu_device_probe(struct platform_device *pdev){ return arm_pmu_device_probe(pdev, armv8_pmu_of_device_ids, NULL);} 在 arm_pmu_device_probe 中，最终会调用到perf_pmu_register 1234567891011121314151617181920212223242526272829303132333435int armpmu_register(struct arm_pmu *pmu){ int ret; ret = cpu_pmu_init(pmu); if (ret) return ret; if (!pmu-&gt;set_event_filter) pmu-&gt;pmu.capabilities |= PERF_PMU_CAP_NO_EXCLUDE; ret = perf_pmu_register(&amp;pmu-&gt;pmu, pmu-&gt;name, -1); if (ret) goto out_destroy; pr_info(&quot;enabled with %s PMU driver, %d counters available%s\\n&quot;, pmu-&gt;name, pmu-&gt;num_events, has_nmi ? &quot;, using NMIs&quot; : &quot;&quot;); return 0;}int arm_pmu_device_probe(struct platform_device *pdev, const struct of_device_id *of_table, const struct pmu_probe_info *probe_table){...... ret = armpmu_request_irqs(pmu); ret = armpmu_register(pmu); return 0;......} 参考Linux perf 1.1、perf_event内核框架参考linaroOrg视频","link":"/2021/07/15/%E5%86%85%E6%A0%B8%E8%A7%82%E6%B5%8B/perf%E7%9B%B8%E5%85%B3/perf%20kernel%20subsys/"},{"title":"dd dumpe2fs debugfs 探索文件系统","text":"环境准备以 ext2 filesystem 为例。在qemu环境中 12345678910111213141516171819202122232425stable_kernel@kernel: ~/workspace/fs# mkdir ext2_dirstable_kernel@kernel: ~/workspace/fs# dd if=/dev/zero of=ext2.img bs=4k count=10241024+0 records in1024+0 records out4194304 bytes (4.2 MB, 4.0 MiB) copied, 0.00530677 s, 790 MB/sstable_kernel@kernel: ~/workspace/fs# mkfs.ext2 ext2.imgmke2fs 1.45.5 (07-Jan-2020)Discarding device blocks: doneCreating filesystem with 1024 4k blocks and 1024 inodesAllocating group tables: doneWriting inode tables: doneWriting superblocks and filesystem accounting information: donestable_kernel@kernel: ~/workspace/fs# sudo mount ext2.img ext2_dirstable_kernel@kernel: ~/workspace/fs#stable_kernel@kernel: ~/workspace/fs# sudo suoot@rlk-Standard-PC-i440FX-PIIX-1996:/home/rlk/workspace/fs/ext2_dir# dmesg &gt; dmesgroot@rlk-Standard-PC-i440FX-PIIX-1996:/home/rlk/workspace/fs/ext2_dir# cp dmesg dmesg1root@rlk-Standard-PC-i440FX-PIIX-1996:/home/rlk/workspace/fs/ext2_dir# echo suhui &gt; fileroot@rlk-Standard-PC-i440FX-PIIX-1996:/home/rlk/workspace/fs/ext2_dir# pwd/home/rlk/workspace/fs/ext2_dirroot@rlk-Standard-PC-i440FX-PIIX-1996:/home/rlk/workspace/fs/ext2_dir# ln -s /home/rlk/workspace/fs/ext2_dir/file file_linkroot@rlk-Standard-PC-i440FX-PIIX-1996:/home/rlk/workspace/fs/ext2_dir# lsdmesg dmesg1 file file_link lost+foundroot@rlk-Standard-PC-i440FX-PIIX-1996:/home/rlk/workspace/fs/ext2_dir# demo123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960stable_kernel@130kernel: ~/workspace/fs/ext2_dir# dfFilesystem 1K-blocks Used Available Use% Mounted on/dev/root 32377648 21931288 8778624 72% /devtmpfs 1873504 0 1873504 0% /devtmpfs 1875824 0 1875824 0% /dev/shmtmpfs 375168 1088 374080 1% /runtmpfs 5120 0 5120 0% /run/lockhost_share 244568380 149197884 82877440 65% /tmp/share/dev/sda1 523248 4 523244 1% /boot/efitmpfs 375164 20 375144 1% /run/user/1000/dev/loop0 3952 60 3688 2% /home/rlk/workspace/fs/ext2_dirstable_kernel@kernel: ~/workspace/fs/ext2_dir# sudo dumpe2fs /dev/loop0dumpe2fs 1.45.5 (07-Jan-2020)Filesystem volume name: &lt;none&gt;Last mounted on: &lt;not available&gt;Filesystem UUID: 7e4b766b-1cd0-4bb4-b53f-6cf250f4d1d0Filesystem magic number: 0xEF53Filesystem revision #: 1 (dynamic)Filesystem features: ext_attr resize_inode dir_index filetype sparse_super large_fileFilesystem flags: signed_directory_hashDefault mount options: user_xattr aclFilesystem state: not cleanErrors behavior: ContinueFilesystem OS type: LinuxInode count: 1024Block count: 1024Reserved block count: 51Free blocks: 973Free inodes: 1009First block: 0Block size: 4096Fragment size: 4096Blocks per group: 32768Fragments per group: 32768Inodes per group: 1024Inode blocks per group: 32Filesystem created: Tue Jul 13 15:53:45 2021Last mount time: n/aLast write time: Tue Jul 13 15:54:49 2021Mount count: 2Maximum mount count: -1Last checked: Tue Jul 13 15:53:45 2021Check interval: 0 (&lt;none&gt;)Reserved blocks uid: 0 (user root)Reserved blocks gid: 0 (group root)First inode: 11Inode size: 128Default directory hash: half_md4Directory Hash Seed: a894ce52-595d-4d99-aa0e-10289f427598Group 0: (Blocks 0-1023) Primary superblock at 0, Group descriptors at 1-1 Block bitmap at 2 (+2) Inode bitmap at 3 (+3) Inode table at 4-35 (+4) 973 free blocks, 1009 free inodes, 2 directories Free blocks: 42-50, 60-1023 Free inodes: 16-1024stable_kernel@kernel: ~/workspace/fs/ext2_dir# inode可以通过 ls 看到 file的 inode 号 1234567891011LS(1) User Commands LS(1)NAME ls - list directory contentsSYNOPSIS ls [OPTION]... [FILE]......... -i, --inode print the index number of each file...... 12345678910stable_kernel@kernel: ~/workspace/fs/ext2_dir# ls -alitotal 60 2 drwxr-xr-x 3 root root 4096 7月 13 15:58 .791692 drwxrwxr-x 3 rlk rlk 4096 7月 13 15:56 .. 12 -rw-r--r-- 1 root root 0 7月 13 15:57 dmesg 13 -rw-r--r-- 1 root root 34001 7月 13 15:57 dmesg1 14 -rw-r--r-- 1 root root 0 7月 13 15:57 file 15 lrwxrwxrwx 1 root root 36 7月 13 15:58 file_link -&gt; /home/rlk/workspace/fs/ext2_dir/file 11 drwx------ 2 root root 16384 7月 13 15:53 lost+foundstable_kernel@kernel: ~/workspace/fs/ext2_dir# 可以看到 dmesg 的 inode号 是 12 regular file可以通过 debugfs 看到 inode 号为 12的 文件详细信息 1234567891011121314stable_kernel@kernel: ~/workspace/fs/ext2_dir# sudo debugfs -R &quot;stat &lt;12&gt;&quot; /dev/loop0debugfs 1.45.5 (07-Jan-2020)Inode: 12 Type: regular Mode: 0644 Flags: 0x0Generation: 2322689703 Version: 0x00000000User: 0 Group: 0 Size: 34178File ACL: 0Links: 1 Blockcount: 72Fragment: Address: 0 Number: 0 Size: 0ctime: 0x60ed4ae9 -- Tue Jul 13 16:12:25 2021atime: 0x60ed4753 -- Tue Jul 13 15:57:07 2021mtime: 0x60ed4ae9 -- Tue Jul 13 16:12:25 2021BLOCKS:(0-8):42-50TOTAL: 9 可以看到文件类型是 regular，权限是 0644，size 是 34178;其中 BLOCKS 是 42-50 之间。 通过dd 工具 将 block-42 内容dump出来： 123456789101112131415stable_kernel@kernel: ~/workspace/fs/ext2_dir# sudo dd if=/dev/loop0 bs=4096 skip=42 count=1 | xxd -b -l 321+0 records in1+0 records out4096 bytes (4.1 kB, 4.0 KiB) copied, 4.6639e-05 s, 87.8 MB/s00000000: 01011011 00100000 00100000 00100000 00100000 00110000 [ 000000006: 00101110 00110000 00110000 00110000 00110000 00110000 .000000000000c: 00110000 01011101 00100000 01001100 01101001 01101110 0] Lin00000012: 01110101 01111000 00100000 01110110 01100101 01110010 ux ver00000018: 01110011 01101001 01101111 01101110 00100000 00110101 sion 50000001e: 00101110 00110001 .1stable_kernel@kernel: ~/workspace/fs/ext2_dir# cat dmesg | head -n 3[ 0.000000] Linux version 5.14.0-rc1+ (ubuntu@ubuntu-HP-ProDesk-680-G4-MT) (gcc (Ubuntu 10.3.0-1ubuntu1) 10.3.0, GNU ld (GNU Binutils for Ubuntu) 2.36.1) #48 SMP Tue Jul 13 10:42:02 CST 2021[ 0.000000] Command line: root=/dev/sda5 console=ttyS0 crashkernel=256M systemd.unified_cgroup_hierarchy=1[ 0.000000] x86/fpu: x87 FPU will use FXSAVEstable_kernel@kernel: ~/workspace/fs/ext2_dir# 可以看到 skip 42 个 block之后，dd 出来一个 block 内容就是 dmesg 文件的内容。 directory在目录下创建 目录，并新建两个文件 1234567891011121314stable_kernel@1kernel: ~/workspace/fs/ext2_dir# sudo mkdir tmpstable_kernel@kernel: ~/workspace/fs/ext2_dir# sudo touch tmp/filename_1stable_kernel@kernel: ~/workspace/fs/ext2_dir# sudo touch tmp/filename_2stable_kernel@kernel: ~/workspace/fs/ext2_dir# ls -alhitotal 104K 2 drwxr-xr-x 4 root root 4.0K 7月 13 16:53 .791692 drwxrwxr-x 3 rlk rlk 4.0K 7月 13 15:56 .. 12 -rw-r--r-- 1 root root 35K 7月 13 16:45 dmesg 13 -rw-r--r-- 1 root root 35K 7月 13 16:45 dmesg1 14 -rw-r--r-- 1 root root 6 7月 13 16:45 file 15 lrwxrwxrwx 1 root root 36 7月 13 16:46 file_link -&gt; /home/rlk/workspace/fs/ext2_dir/file 11 drwx------ 2 root root 16K 7月 13 16:45 lost+found 16 drwxr-xr-x 2 root root 4.0K 7月 13 16:55 tmpstable_kernel@kernel: ~/workspace/fs/ext2_dir# 可以看到 tmp dir的 inode号 是 16，通过 debugfs 看到 inode 号为 16的 文件详细信息 1234567891011121314stable_kernel@kernel: ~/workspace/fs/ext2_dir# sudo debugfs -R &quot;stat &lt;16&gt;&quot; /dev/loop0debugfs 1.45.5 (07-Jan-2020)Inode: 16 Type: directory Mode: 0755 Flags: 0x0Generation: 2149799966 Version: 0x00000000User: 0 Group: 0 Size: 4096File ACL: 0Links: 2 Blockcount: 8Fragment: Address: 0 Number: 0 Size: 0ctime: 0x60ed550e -- Tue Jul 13 16:55:42 2021atime: 0x60ed547e -- Tue Jul 13 16:53:18 2021mtime: 0x60ed550e -- Tue Jul 13 16:55:42 2021BLOCKS:(0):61TOTAL: 1 可以看到文件类型是 directory，权限是 0755，size 是 4096;文件比较小，只包含一个 block，位于 block 61位置处。 通过dd 工具 将 block-61 内容dump出来： 1234567891011121314stable_kernel@kernel: ~/workspace/fs/ext2_dir# sudo dd if=/dev/loop0 bs=4096 skip=61 count=1 | xxd -u -l 1501+0 records in1+0 records out4096 bytes (4.1 kB, 4.0 KiB) copied, 7.5436e-05 s, 54.3 MB/s00000000: 1000 0000 0C00 0102 2E00 0000 0200 0000 ................00000010: 0C00 0202 2E2E 0000 1100 0000 1400 0A01 ................00000020: 6669 6C65 6E61 6D65 5F31 0000 1200 0000 filename_1......00000030: D40F 0A01 6669 6C65 6E61 6D65 5F32 0000 ....filename_2..00000040: 0000 0000 0000 0000 0000 0000 0000 0000 ................00000050: 0000 0000 0000 0000 0000 0000 0000 0000 ................00000060: 0000 0000 0000 0000 0000 0000 0000 0000 ................00000070: 0000 0000 0000 0000 0000 0000 0000 0000 ................00000080: 0000 0000 0000 0000 0000 0000 0000 0000 ................00000090: 0000 0000 0000 ...... . ascii 是 2E.. ascii 是 2E2Efilename_1 ascii 是 6669 6C65 6E61 6D65 5F31filename_1 ascii 是 6669 6C65 6E61 6D65 5F32 dir entry 的 data block 数据需要结合 struct ext2_dir_entry_2 来看： 1234567struct ext2_dir_entry_2 { __le32 inode; /* Inode number */ __le16 rec_len; /* Directory entry length */ __u8 name_len; /* Name length */ __u8 file_type; char name[]; /* File name, up to EXT2_NAME_LEN */}; 在 filename 之前有 8 bytes file name的属性 filename_1 对应的是 1100 0000 1400 0A01， inode是 0x11，即 17；rec_len是 0x14；name_len 是 0x0A，即10 byte, file type是 1。 . 对应的是 1000 0000 0C00 0102， inode是 0x10，即 16，与 tmp 的inode号一致；rec_len是 0x0c；name_len 是 0x01，即1 byte, file type是 2。 .. 对应的是 0200 0000 0C00 0202， inode是 0x02，即 2，与 ext2_dir 的inode号一致；rec_len是 0x0c；name_len 是 0x02，即1 byte, file type是 2。 每个filename结尾都自动加了 \\0\\0 directory 下目录文件太多怎么办通过上面 demo可以看出，dir的 data block 是存储的目录的 inode 和 filename 等信息，按照文件名字 10 byte,大概一个文件占用 20byte空间，而 data block 大小是 4096 byte，所以至少创建 200+ 文件才能使得 dir inode 的 data block 数量大于1；通过命令for i in $(seq 1 250); do sudo touch filename_$i; done自动创建 250 个文件 通过debugfs 查看此时inode-16的信息： 1234567891011121314stable_kernel@kernel: ~/workspace/fs/ext2_dir/tmp# sudo debugfs -R &quot;stat &lt;16&gt;&quot; /dev/loop0debugfs 1.45.5 (07-Jan-2020)Inode: 16 Type: directory Mode: 0755 Flags: 0x0Generation: 2149799966 Version: 0x00000000User: 0 Group: 0 Size: 8192File ACL: 0Links: 2 Blockcount: 16Fragment: Address: 0 Number: 0 Size: 0ctime: 0x60ed7ae7 -- Tue Jul 13 19:37:11 2021atime: 0x60ed5917 -- Tue Jul 13 17:12:55 2021mtime: 0x60ed7ae7 -- Tue Jul 13 19:37:11 2021BLOCKS:(0):61, (1):64TOTAL: 2 由于此时tmp 目录下文件很多，导致此时一个 data block 无法存储所有目录项的inode 和 filename 信息等，扩展到了 两个 data block。 通过dd查看 block-61 尾部数据 123456789101112131415stable_kernel@kernel: ~/workspace/fs/ext2_dir/tmp# sudo dd if=/dev/loop0 bs=4096 skip=61 count=1 | xxd -u -l 4096 | tail -n 101+0 records in1+0 records out4096 bytes (4.1 kB, 4.0 KiB) copied, 3.5019e-05 s, 117 MB/s00000f60: 6E61 6D65 5F31 3936 D600 0000 1400 0C01 name_196........00000f70: 6669 6C65 6E61 6D65 5F31 3937 D700 0000 filename_197....00000f80: 1400 0C01 6669 6C65 6E61 6D65 5F31 3938 ....filename_19800000f90: D800 0000 1400 0C01 6669 6C65 6E61 6D65 ........filename00000fa0: 5F31 3939 D900 0000 1400 0C01 6669 6C65 _199........file00000fb0: 6E61 6D65 5F32 3030 DA00 0000 1400 0C01 name_200........00000fc0: 6669 6C65 6E61 6D65 5F32 3031 DB00 0000 filename_201....00000fd0: 1400 0C01 6669 6C65 6E61 6D65 5F32 3032 ....filename_20200000fe0: DC00 0000 2000 0C01 6669 6C65 6E61 6D65 .... ...filename00000ff0: 5F32 3033 0000 0000 0000 0000 0000 0000 _203............stable_kernel@kernel: ~/workspace/fs/ext2_dir/tmp# 通过dd查看 block-64 123456789101112131415stable_kernel@kernel: ~/workspace/fs/ext2_dir/tmp# sudo dd if=/dev/loop0 bs=4096 skip=64 count=1 | xxd -u -l 1501+0 records in1+0 records out4096 bytes (4.1 kB, 4.0 KiB) copied, 3.4552e-05 s, 119 MB/s00000000: DD00 0000 1400 0C01 6669 6C65 6E61 6D65 ........filename00000010: 5F32 3034 DE00 0000 1400 0C01 6669 6C65 _204........file00000020: 6E61 6D65 5F32 3035 DF00 0000 1400 0C01 name_205........00000030: 6669 6C65 6E61 6D65 5F32 3036 E000 0000 filename_206....00000040: 1400 0C01 6669 6C65 6E61 6D65 5F32 3037 ....filename_20700000050: E100 0000 1400 0C01 6669 6C65 6E61 6D65 ........filename00000060: 5F32 3038 E200 0000 1400 0C01 6669 6C65 _208........file00000070: 6E61 6D65 5F32 3039 E300 0000 1400 0C01 name_209........00000080: 6669 6C65 6E61 6D65 5F32 3130 E400 0000 filename_210....00000090: 1400 0C01 6669 ....fistable_kernel@kernel: ~/workspace/fs/ext2_dir/tmp# data block-61 和 data block-64 数据内容是正好相连的。 linklink 在 linux上也分为两种soft link 和 hard link。 1234567stable_kernel@kernel: ~/workspace/fs/ext2_dir# ls -alitotal 120 2 drwxr-xr-x 5 root root 8192 7月 13 20:29 .791692 drwxrwxr-x 3 rlk rlk 4096 7月 13 15:56 .. 14 -rw-r--r-- 2 root root 6 7月 13 16:45 file 15 lrwxrwxrwx 1 root root 36 7月 13 16:46 file_link -&gt; /home/rlk/workspace/fs/ext2_dir/file 14 -rw-r--r-- 2 root root 6 7月 13 16:45 file_link_hard file 是实际文件，inode号是 14file_link 是软链接，inode号是 15file_link_hard 是硬链接，inode号是 14 soft link通过debugfs 查看此时file_link的 inode，即inode-15的信息： 12345678910Inode: 15 Type: symlink Mode: 0777 Flags: 0x0Generation: 2149799965 Version: 0x00000000User: 0 Group: 0 Size: 36File ACL: 0Links: 1 Blockcount: 0Fragment: Address: 0 Number: 0 Size: 0ctime: 0x60ed52ce -- Tue Jul 13 16:46:06 2021atime: 0x60ed52d0 -- Tue Jul 13 16:46:08 2021mtime: 0x60ed52ce -- Tue Jul 13 16:46:06 2021Fast link dest: &quot;/home/rlk/workspace/fs/ext2_dir/file&quot; 可以看到文件类型是 symlink，权限是 0777，size 是 36，就是软链接地址字符的长度;并不包含 data block 区域。 hard link通过debugfs 查看此时file_link_hard的 inode，即inode-14的信息： 1234567891011121314stable_kernel@kernel: ~/workspace/fs/ext2_dir# sudo debugfs -R &quot;stat &lt;14&gt;&quot; /dev/loop0debugfs 1.45.5 (07-Jan-2020)Inode: 14 Type: regular Mode: 0644 Flags: 0x0Generation: 2149799964 Version: 0x00000000User: 0 Group: 0 Size: 6File ACL: 0Links: 2 Blockcount: 8Fragment: Address: 0 Number: 0 Size: 0ctime: 0x60ed8722 -- Tue Jul 13 20:29:22 2021atime: 0x60ed52c6 -- Tue Jul 13 16:45:58 2021mtime: 0x60ed52c6 -- Tue Jul 13 16:45:58 2021BLOCKS:(0):60TOTAL: 1 和 file 的inode号一样，所以 hardlink 在删除源文件之后，仍然是存在的。 1234567891011121314151617181920stable_kernel@kernel: ~/workspace/fs/ext2_dir# ls -alhitotal 120K 2 drwxr-xr-x 5 root root 8.0K 7月 13 20:43 .791692 drwxrwxr-x 3 rlk rlk 4.0K 7月 13 15:56 .. 12 -rw-r--r-- 1 root root 35K 7月 13 16:45 dmesg 13 -rw-r--r-- 1 root root 35K 7月 13 16:45 dmesg1 14 -rw-r--r-- 2 root root 6 7月 13 16:45 file 15 lrwxrwxrwx 1 root root 36 7月 13 16:46 file_link -&gt; /home/rlk/workspace/fs/ext2_dir/file 14 -rw-r--r-- 2 root root 6 7月 13 16:45 file_link_hard 19 drwxr-xr-x 2 root root 4.0K 7月 13 17:41 file.txt 11 drwx------ 2 root root 16K 7月 13 16:45 lost+found 16 drwxr-xr-x 2 root root 8.0K 7月 13 19:37 tmpstable_kernel@kernel: ~/workspace/fs/ext2_dir# cat filesuhuistable_kernel@kernel: ~/workspace/fs/ext2_dir# cat file_link_hardsuhuistable_kernel@kernel: ~/workspace/fs/ext2_dir# sudo rm filestable_kernel@kernel: ~/workspace/fs/ext2_dir# cat file_link_hardsuhuistable_kernel@kernel: ~/workspace/fs/ext2_dir# soft link 与 hard link 如何区分在 . 目录中 hard link 和 soft link 如何存储的？ 1234567891011stable_kernel@kernel: ~/workspace/fs/ext2_dir# ls -alitotal 24 2 drwxr-xr-x 3 root root 4096 7月 14 10:52 .791692 drwxrwxr-x 3 rlk rlk 4096 7月 13 15:56 .. 12 -rw-r--r-- 1 root root 0 7月 14 10:51 1 13 -rw-r--r-- 1 root root 0 7月 14 10:51 2 14 -rw-r--r-- 2 root root 0 7月 14 10:51 file 15 lrwxrwxrwx 1 root root 36 7月 14 10:52 file_link -&gt; /home/rlk/workspace/fs/ext2_dir/file 14 -rw-r--r-- 2 root root 0 7月 14 10:51 file_link_hard 11 drwx------ 2 root root 16384 7月 14 10:51 lost+foundstable_kernel@kernel: ~/workspace/fs/ext2_dir# 通过debugfs 看到 inode-2 的详细信息 1234567891011121314stable_kernel@kernel: ~/workspace/fs/ext2_dir# sudo debugfs -R &quot;stat &lt;2&gt;&quot; /dev/loop0debugfs 1.45.5 (07-Jan-2020)Inode: 2 Type: directory Mode: 0755 Flags: 0x0Generation: 0 Version: 0x00000000User: 0 Group: 0 Size: 4096File ACL: 0Links: 3 Blockcount: 8Fragment: Address: 0 Number: 0 Size: 0ctime: 0x60ee516d -- Wed Jul 14 10:52:29 2021atime: 0x60ee5172 -- Wed Jul 14 10:52:34 2021mtime: 0x60ee516d -- Wed Jul 14 10:52:29 2021BLOCKS:(0):36TOTAL: 1 通过dd 工具 将 data block-36 内容dump出来： 12345678910111213stable_kernel@kernel: ~/workspace/fs/ext2_dir# sudo dd if=/dev/loop0 bs=4096 skip=36 count=1 | xxd -u -l 1281+0 records in1+0 records out4096 bytes (4.1 kB, 4.0 KiB) copied, 4.8568e-05 s, 84.3 MB/s00000000: 0200 0000 0C00 0102 2E00 0000 0200 0000 ................00000010: 0C00 0202 2E2E 0000 0B00 0000 1400 0A02 ................00000020: 6C6F 7374 2B66 6F75 6E64 0000 0C00 0000 lost+found......00000030: 0C00 0101 3100 0000 0D00 0000 0C00 0101 ....1...........00000040: 3200 0000 0E00 0000 0C00 0401 6669 6C65 2...........file00000050: 0E00 0000 1800 0E01 6669 6C65 5F6C 696E ........file_lin00000060: 6B5F 6861 7264 0000 0F00 0000 980F 0907 k_hard..........00000070: 6669 6C65 5F6C 696E 6B00 0000 0000 0000 file_link.......stable_kernel@kernel: ~/workspace/fs/ext2_dir# 可以看到 各个文件属性 file: 0E00 0000 0C00 0401 file_link_hard: 0E00 0000 1800 0E01 file_link: 0F00 0000 980F 0907 file 对应的是 0E00 0000 0C00 0401， inode是 0x0E，即 14，与 file 的inode号一致；rec_len是 0x0c；name_len 是 0x04，即4 byte, file type是 1。 file_link_hard 对应的是 0E00 0000 1800 0E01， inode是 0x0E，即 14，与 file_link_hard 的inode号一致；rec_len是 0x18；name_len 是 0x0E，即14 byte, file type是 1。 file_link 对应的是 0F00 0000 980F 0907， inode是 0x0F，即 15，与 file_link 的inode号一致；rec_len是 0x980F；name_len 是 0x09，即9 byte, file type是 7。 empty dir entry众所周知，linux 文件系统中，至少包含两个文件. 和 ..。所以empty dir entry 和普通 dir entry 没有什么区别。 empty filefilename_1 是一个空文件 1234stable_kernel@kernel: ~/workspace/fs/ext2_dir/tmp# cat filename_1stable_kernel@kernel: ~/workspace/fs/ext2_dir/tmp# ls -i filename_117 filename_1stable_kernel@kernel: ~/workspace/fs/ext2_dir/tmp# 通过debugfs 看到 inode-17 的详细信息 1234567891011121314stable_kernel@kernel: ~/workspace/fs/ext2_dir/tmp# sudo debugfs -R &quot;stat &lt;17&gt;&quot; /dev/loop0debugfs 1.45.5 (07-Jan-2020)Inode: 17 Type: regular Mode: 0644 Flags: 0x0Generation: 2853187968 Version: 0x00000000User: 0 Group: 0 Size: 0File ACL: 0Links: 1 Blockcount: 0Fragment: Address: 0 Number: 0 Size: 0ctime: 0x60ee5464 -- Wed Jul 14 11:05:08 2021atime: 0x60ee559d -- Wed Jul 14 11:10:21 2021mtime: 0x60ee5464 -- Wed Jul 14 11:05:08 2021BLOCKS:(END) 可以看到 file type等，但是最后 BLOCKS是 空的，意味着 inode-17 对应的文件是没有 data block 的，没有数据块，也就是空文件。 ext2 磁盘文件系统布局分析通过 dumpe2fs 可以看到系统 inode-size Block-size 等信息 1234567stable_kernel@kernel: ~/workspace/fs/ext2_dir/tmp# sudo dumpe2fs /dev/loop0 | grep sizedumpe2fs 1.45.5 (07-Jan-2020)Filesystem features: ext_attr resize_inode dir_index filetype sparse_super large_fileBlock size: 4096Fragment size: 4096Inode size: 128stable_kernel@kernel: ~/workspace/fs/ext2_dir/tmp# 还可以看到 group 块组的信息 1234567891011stable_kernel@kernel: ~/workspace/fs/ext2_dir/tmp# sudo dumpe2fs /dev/loop0 | tail -n 8dumpe2fs 1.45.5 (07-Jan-2020)Group 0: (Blocks 0-1023) Primary superblock at 0, Group descriptors at 1-1 Block bitmap at 2 (+2) Inode bitmap at 3 (+3) Inode table at 4-35 (+4) 980 free blocks, 758 free inodes, 3 directories Free blocks: 44-1023 Free inodes: 267-1024stable_kernel@kernel: ~/workspace/fs/ext2_dir/tmp# 可以将 ext2.img 分成 若干个4k 小块: 1234567891011 group description inode bitmap+-----------+----------+-----------+------------+---------------------------------+----------+-----------+------------------------------+| | | | | | | | || | | | | | | | || | | | | | | | |+-----------+----------+-----------+------------+---------------------------------+----------+-----------+------------------------------+ super block block bitmap inode table= (35 - 4 +1) * 4096 = 131072 131072 / 128 = 1024 block-0: super block block-1: group description block-2: block bitmap block-3: inode bitmap block-4&lt;--&gt;35: inode table, inode-size = 128，这个 group中 有 32个 block 作为 inode-table，总size 是 32 * 4096 = 131072，inode 个数是 131072 / 128 = 1024 block-36&lt;--&gt;1023: data block，是实际存储数据的block，empty file 没有 data block 从 inode 到 data block通过 dumpe2fs 可以看到系统 inode-size Block-size 等信息 1234567891011stable_kernel@kernel: ~/workspace/fs/ext2_dir/tmp# sudo dumpe2fs /dev/loop0 | tail -n 8dumpe2fs 1.45.5 (07-Jan-2020)Group 0: (Blocks 0-1023) Primary superblock at 0, Group descriptors at 1-1 Block bitmap at 2 (+2) Inode bitmap at 3 (+3) Inode table at 4-35 (+4) 980 free blocks, 758 free inodes, 3 directories Free blocks: 44-1023 Free inodes: 267-1024stable_kernel@kernel: ~/workspace/fs/ext2_dir/tmp# 可以看到从 block-4 开始是 inode table 123stable_kernel@kernel: ~/workspace/fs/ext2_dir# ls -ailtotal 32 2 drwxr-xr-x 4 root root 4096 7月 14 11:04 .791692 drwxrwxr-x 3 rlk rlk 4096 7月 13 15:56 .. . 的 inode号是 2 inode-size 是 128，将 block-4 前 256 bytes dump出来： 123456789101112131415161718192021stable_kernel@kernel: ~/workspace/fs/ext2_dir# sudo dd if=/dev/loop0 bs=4096 skip=4 count=1 | xxd -u -l 2561+0 records in1+0 records out4096 bytes (4.1 kB, 4.0 KiB) copied, 4.924e-05 s, 83.2 MB/s00000000: 0000 0000 0000 0000 3151 EE60 3151 EE60 ........1Q.`1Q.`00000010: 3151 EE60 0000 0000 0000 0000 0000 0000 1Q.`............00000020: 0000 0000 0000 0000 0000 0000 0000 0000 ................00000030: 0000 0000 0000 0000 0000 0000 0000 0000 ................00000040: 0000 0000 0000 0000 0000 0000 0000 0000 ................00000050: 0000 0000 0000 0000 0000 0000 0000 0000 ................00000060: 0000 0000 0000 0000 0000 0000 0000 0000 ................00000070: 0000 0000 0000 0000 0000 0000 0000 0000 ................00000080: ED41 0000 0010 0000 4F54 EE60 4E54 EE60 .A......OT.`NT.`00000090: 4E54 EE60 0000 0000 0000 0400 0800 0000 NT.`............000000a0: 0000 0000 0000 0000 2400 0000 0000 0000 ........$.......000000b0: 0000 0000 0000 0000 0000 0000 0000 0000 ................000000c0: 0000 0000 0000 0000 0000 0000 0000 0000 ................000000d0: 0000 0000 0000 0000 0000 0000 0000 0000 ................000000e0: 0000 0000 0000 0000 0000 0000 0000 0000 ................000000f0: 0000 0000 0000 0000 0000 0000 0000 0000 ................stable_kernel@kernel: ~/workspace/fs/ext2_dir# 在对 inode 磁盘结构不清楚的情况下，只知道 inode-2 对应的 data block 是 36，即0x24 inode-2 完整数据是 1234567800000080: ED41 0000 0010 0000 4F54 EE60 4E54 EE60 .A......OT.`NT.`00000090: 4E54 EE60 0000 0000 0000 0400 0800 0000 NT.`............000000a0: 0000 0000 0000 0000 2400 0000 0000 0000 ........$.......000000b0: 0000 0000 0000 0000 0000 0000 0000 0000 ................000000c0: 0000 0000 0000 0000 0000 0000 0000 0000 ................000000d0: 0000 0000 0000 0000 0000 0000 0000 0000 ................000000e0: 0000 0000 0000 0000 0000 0000 0000 0000 ................000000f0: 0000 0000 0000 0000 0000 0000 0000 0000 ................ 结合 struct ext2_inode 来看 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556/* * Structure of an inode on the disk */struct ext2_inode { __le16 i_mode; /* File mode */ __le16 i_uid; /* Low 16 bits of Owner Uid */ __le32 i_size; /* Size in bytes */ __le32 i_atime; /* Access time */ __le32 i_ctime; /* Creation time */ __le32 i_mtime; /* Modification time */ __le32 i_dtime; /* Deletion Time */ __le16 i_gid; /* Low 16 bits of Group Id */ __le16 i_links_count; /* Links count */ __le32 i_blocks; /* Blocks count */ __le32 i_flags; /* File flags */ union { struct { __le32 l_i_reserved1; } linux1; struct { __le32 h_i_translator; } hurd1; struct { __le32 m_i_reserved1; } masix1; } osd1; /* OS dependent 1 */ __le32 i_block[EXT2_N_BLOCKS];/* Pointers to blocks */ __le32 i_generation; /* File version (for NFS) */ __le32 i_file_acl; /* File ACL */ __le32 i_dir_acl; /* Directory ACL */ __le32 i_faddr; /* Fragment address */ union { struct { __u8 l_i_frag; /* Fragment number */ __u8 l_i_fsize; /* Fragment size */ __u16 i_pad1; __le16 l_i_uid_high; /* these 2 fields */ __le16 l_i_gid_high; /* were reserved2[0] */ __u32 l_i_reserved2; } linux2; struct { __u8 h_i_frag; /* Fragment number */ __u8 h_i_fsize; /* Fragment size */ __le16 h_i_mode_high; __le16 h_i_uid_high; __le16 h_i_gid_high; __le32 h_i_author; } hurd2; struct { __u8 m_i_frag; /* Fragment number */ __u8 m_i_fsize; /* Fragment size */ __u16 m_pad1; __u32 m_i_reserved2[2]; } masix2; } osd2; /* OS dependent 2 */}; 第41 byte 是 block pointers，即 0x24(36) 1__le32 i_block[EXT2_N_BLOCKS];/* Pointers to blocks */ 第33 byte 是 block count，即 0x8(8) (按照 block-size == 512 计算) 1__le32 i_blocks; /* Blocks count */ 和 dir-&gt;i_blocks &gt;&gt; (PAGE_SHIFT - 9) dir-&gt;i_blocks &gt;&gt; (12 - 9)计算 方式一致 第29 byte 是 links count，即 0x4(4) 1__le16 i_links_count; /* Links count */ 与 debugfs 结果一致 12345678910111213141516stable_kernel@kernel: ~/workspace/fs/ext2_dir# sudo debugfs -R &quot;stat &lt;2&gt;&quot; /dev/loop0debugfs 1.45.5 (07-Jan-2020)Inode: 2 Type: directory Mode: 0755 Flags: 0x0Generation: 0 Version: 0x00000000User: 0 Group: 0 Size: 4096File ACL: 0Links: 4 Blockcount: 8Fragment: Address: 0 Number: 0 Size: 0ctime: 0x60ee6d97 -- Wed Jul 14 12:52:39 2021atime: 0x60ee6e96 -- Wed Jul 14 12:56:54 2021mtime: 0x60ee6d97 -- Wed Jul 14 12:52:39 2021BLOCKS:(0):36TOTAL: 1(END) super blockGroup descriptorsGDT（Group Descriptor Table）组描述符表：由多组描述符组成，整个分区分成多少个组就对应多少个组描述符。每个组描述符（Group Descriptor）存储一个组的描述信息，例如这个组中从哪里是inode表、哪里开始是数据块、空闲的inode和数据块还有多少等。 Block bitmap 与 Inode bitmap重新格式化 ext2.img，之后mount由于 都只有 1024 个 inode 和 block，所以只需要 1024 / 8 = 128 byte 即可查看 Block bitmap 12345678910111213stable_kernel@kernel: ~/workspace/fs/ext2_dir# sudo dd if=/dev/loop0 bs=4096 skip=2 count=1 | xxd -u -l 1281+0 records in1+0 records out4096 bytes (4.1 kB, 4.0 KiB) copied, 6.4301e-05 s, 63.7 MB/s00000000: FFFF FFFF FF03 0000 0000 0000 0000 0000 ................00000010: 0000 0000 0000 0000 0000 0000 0000 0000 ................00000020: 0000 0000 0000 0000 0000 0000 0000 0000 ................00000030: 0000 0000 0000 0000 0000 0000 0000 0000 ................00000040: 0000 0000 0000 0000 0000 0000 0000 0000 ................00000050: 0000 0000 0000 0000 0000 0000 0000 0000 ................00000060: 0000 0000 0000 0000 0000 0000 0000 0000 ................00000070: 0000 0000 0000 0000 0000 0000 0000 0000 ................stable_kernel@kernel: ~/workspace/fs/ext2_dir# 看到 block(0-41) 都已经被使用了 查看 Inode bitmap 12345678910111213stable_kernel@kernel: ~/workspace/fs/ext2_dir# sudo dd if=/dev/loop0 bs=4096 skip=3 count=1 | xxd -u -l 1281+0 records in1+0 records out4096 bytes (4.1 kB, 4.0 KiB) copied, 0.000236682 s, 17.3 MB/s00000000: FF07 0000 0000 0000 0000 0000 0000 0000 ................00000010: 0000 0000 0000 0000 0000 0000 0000 0000 ................00000020: 0000 0000 0000 0000 0000 0000 0000 0000 ................00000030: 0000 0000 0000 0000 0000 0000 0000 0000 ................00000040: 0000 0000 0000 0000 0000 0000 0000 0000 ................00000050: 0000 0000 0000 0000 0000 0000 0000 0000 ................00000060: 0000 0000 0000 0000 0000 0000 0000 0000 ................00000070: 0000 0000 0000 0000 0000 0000 0000 0000 ................stable_kernel@kernel: ~/workspace/fs/ext2_dir# 看到 inode(1-11) 都已经被使用了 此时 目录下 123456stable_kernel@kernel: ~/workspace/fs/ext2_dir# ls -alitotal 24 2 drwxr-xr-x 3 root root 4096 7月 14 14:50 .791692 drwxrwxr-x 3 rlk rlk 4096 7月 13 15:56 .. 11 drwx------ 2 root root 16384 7月 14 14:50 lost+foundstable_kernel@kernel: ~/workspace/fs/ext2_dir# 创建空文件sudo touch 1 1234567stable_kernel@kernel: ~/workspace/fs/ext2_dir# ls -alitotal 24 2 drwxr-xr-x 3 root root 4096 7月 14 14:58 .791692 drwxrwxr-x 3 rlk rlk 4096 7月 13 15:56 .. 12 -rw-r--r-- 1 root root 0 7月 14 14:58 1 11 drwx------ 2 root root 16384 7月 14 14:50 lost+foundstable_kernel@kernel: ~/workspace/fs/ext2_dir# 查看此时 Indoe bitmap 123456789101112stable_kernel@kernel: ~/workspace/fs/ext2_dir# sudo dd if=/dev/loop0 bs=4096 skip=3 count=1 | xxd -u -l 1281+0 records in1+0 records out4096 bytes (4.1 kB, 4.0 KiB) copied, 4.8758e-05 s, 84.0 MB/s00000000: FF0F 0000 0000 0000 0000 0000 0000 0000 ................00000010: 0000 0000 0000 0000 0000 0000 0000 0000 ................00000020: 0000 0000 0000 0000 0000 0000 0000 0000 ................00000030: 0000 0000 0000 0000 0000 0000 0000 0000 ................00000040: 0000 0000 0000 0000 0000 0000 0000 0000 ................00000050: 0000 0000 0000 0000 0000 0000 0000 0000 ................00000060: 0000 0000 0000 0000 0000 0000 0000 0000 ................00000070: 0000 0000 0000 0000 0000 0000 0000 0000 ................ bit-12 已经 set为1了。 查看此时 Block bitmap，由于是空文件，Block bitmap 和之前保持一致 12345678910111213stable_kernel@kernel: ~/workspace/fs/ext2_dir# sudo dd if=/dev/loop0 bs=4096 skip=2 count=1 | xxd -u -l 1281+0 records in1+0 records out4096 bytes (4.1 kB, 4.0 KiB) copied, 4.8515e-05 s, 84.4 MB/s00000000: FFFF FFFF FF03 0000 0000 0000 0000 0000 ................00000010: 0000 0000 0000 0000 0000 0000 0000 0000 ................00000020: 0000 0000 0000 0000 0000 0000 0000 0000 ................00000030: 0000 0000 0000 0000 0000 0000 0000 0000 ................00000040: 0000 0000 0000 0000 0000 0000 0000 0000 ................00000050: 0000 0000 0000 0000 0000 0000 0000 0000 ................00000060: 0000 0000 0000 0000 0000 0000 0000 0000 ................00000070: 0000 0000 0000 0000 0000 0000 0000 0000 ................stable_kernel@kernel: ~/workspace/fs/ext2_dir# 向空文件中写数据向文件1 写入数据 12345678910root@rlk-Standard-PC-i440FX-PIIX-1996:/home/rlk/workspace/fs/ext2_dir# ls1 lost+foundroot@rlk-Standard-PC-i440FX-PIIX-1996:/home/rlk/workspace/fs/ext2_dir# dmesg &gt; 1stable_kernel@kernel: ~/workspace/fs/ext2_dir# ls -alitotal 64 2 drwxr-xr-x 3 root root 4096 7月 14 14:58 .791692 drwxrwxr-x 3 rlk rlk 4096 7月 13 15:56 .. 12 -rw-r--r-- 1 root root 38922 7月 14 15:06 1 11 drwx------ 2 root root 16384 7月 14 14:50 lost+foundstable_kernel@kernel: ~/workspace/fs/ext2_dir# 通过debugfs 看到 inode-12 的详细信息 123456789101112131415stable_kernel@kernel: ~/workspace/fs/ext2_dir# sudo debugfs -R &quot;stat &lt;12&gt;&quot; /dev/loop0debugfs 1.45.5 (07-Jan-2020)Inode: 12 Type: regular Mode: 0644 Flags: 0x0Generation: 1697274386 Version: 0x00000000User: 0 Group: 0 Size: 38922File ACL: 0Links: 1 Blockcount: 80Fragment: Address: 0 Number: 0 Size: 0ctime: 0x60ee8d09 -- Wed Jul 14 15:06:49 2021atime: 0x60ee8afb -- Wed Jul 14 14:58:03 2021mtime: 0x60ee8d09 -- Wed Jul 14 15:06:49 2021BLOCKS:(0-9):42-51TOTAL: 10(END) 可以看出来，inode-12 占用了10个 data block, 从 block-42 –&gt; block-51。 从 Block bitmap 数据看 12345678910111213stable_kernel@kernel: ~/workspace/fs/ext2_dir# sudo dd if=/dev/loop0 bs=4096 skip=2 count=1 | xxd -u -l 1281+0 records in1+0 records out4096 bytes (4.1 kB, 4.0 KiB) copied, 4.911e-05 s, 83.4 MB/s00000000: FFFF FFFF FFFF 0F00 0000 0000 0000 0000 ................00000010: 0000 0000 0000 0000 0000 0000 0000 0000 ................00000020: 0000 0000 0000 0000 0000 0000 0000 0000 ................00000030: 0000 0000 0000 0000 0000 0000 0000 0000 ................00000040: 0000 0000 0000 0000 0000 0000 0000 0000 ................00000050: 0000 0000 0000 0000 0000 0000 0000 0000 ................00000060: 0000 0000 0000 0000 0000 0000 0000 0000 ................00000070: 0000 0000 0000 0000 0000 0000 0000 0000 ................stable_kernel@kernel: ~/workspace/fs/ext2_dir# 从 FFFF FFFF FF03 0000 到 FFFF FFFF FFFF 0F00 正好有 10 bit 数据被设置为了1。 这里可以看到 Inode bitmap 与 Inode, Block bitmap 与 data block 之间关系。这里引申出两个问题：1.为什么用df命令比du命令统计整个磁盘的已用空间非常快呢？ 因为df命令只需要查看每个块组的块位图即可，而不需要搜遍整个分区。 相反，用du命令查看一个较大目录的已用空间就非常慢，因为不可避免地要搜遍整个目录的所有文件。 df命令用于显示磁盘上的可使用的磁盘空间。du命令是对文件和目录磁盘使用的空间的查看。 在格式化一个分区时究竟会划出多少个块组呢？ 格式化一个分区有多少个块组，主要取决于分区大小和块的大小。因为Block Bitmap占一个块，即4K字节，其有4K * 8bit，可以标注32K个块，每个块的大小为4K，即标注区域为32K * 4K = 128MB，即一个块组Group的上限为128M。若分区为32G，则ext2文件系统需要32 G / 128M = 256个块组。 small file常见的磁盘空间不够 是因为 单个文件数据过多，导致data block 区域被用完了，无法再分配 data block 区域。还有一种是因为磁盘上存在着太多的小文件，导致 inode 被耗尽，而不能创建新文件的情况 先 dump 整个文件系统信息 1234567891011stable_kernel@kernel: ~/workspace/fs# sudo dumpe2fs /dev/loop0 | tail -n 8dumpe2fs 1.45.5 (07-Jan-2020)Group 0: (Blocks 0-1023) Primary superblock at 0, Group descriptors at 1-1 Block bitmap at 2 (+2) Inode bitmap at 3 (+3) Inode table at 4-35 (+4) 981 free blocks, 1012 free inodes, 2 directories Free blocks: 43-1023 Free inodes: 13-1024stable_kernel@kernel: ~/workspace/fs# 可以知道还剩余 1012个 inodes，创建 1024个 文件 123456789stable_kernel@kernel: ~/workspace/fs/ext2_dir# mkdir tmpstable_kernel@kernel: ~/workspace/fs/ext2_dir# cd tmpstable_kernel@kernel: ~/workspace/fs/ext2_dir/tmp# for i in $(seq 1 1024); do sudo touch filename_$i; donetouch: cannot touch 'filename_1012': No space left on devicetouch: cannot touch 'filename_1013': No space left on device......touch: cannot touch 'filename_1023': No space left on devicetouch: cannot touch 'filename_1024': No space left on devicestable_kernel@1kernel: ~/workspace/fs/ext2_dir/tmp# 会发现最后面的几个文件都没法创建，显示 No space left on device，dump整个文件系统状态来看： 1234567891011stable_kernel@1kernel: ~/workspace/fs/ext2_dir/tmp# sudo dumpe2fs /dev/loop0 | tail -n 8dumpe2fs 1.45.5 (07-Jan-2020)Group 0: (Blocks 0-1023) Primary superblock at 0, Group descriptors at 1-1 Block bitmap at 2 (+2) Inode bitmap at 3 (+3) Inode table at 4-35 (+4) 976 free blocks, 0 free inodes, 3 directories Free blocks: 48-1023 Free inodes:stable_kernel@kernel: ~/workspace/fs/ext2_dir/tmp# 会发现此时 Free inodes 已经是0了。 big image之前ext2.img 只有4MB, 导致只有一个 group重新dd 一个 ext2.img 123456789101112131415stable_kernel@kernel: ~/workspace/fs# dd if=/dev/zero of=ext2.img bs=4k count=102400102400+0 records in102400+0 records out419430400 bytes (419 MB, 400 MiB) copied, 0.890318 s, 471 MB/sstable_kernel@kernel: ~/workspace/fs# mkfs.ext2 ext2.imgmke2fs 1.45.5 (07-Jan-2020)Discarding device blocks: doneCreating filesystem with 102400 4k blocks and 102400 inodesFilesystem UUID: 4529647b-d4f6-4110-bf12-27b6e6df472dSuperblock backups stored on blocks: 32768, 98304Allocating group tables: doneWriting inode tables: doneWriting superblocks and filesystem accounting information: done 重新 mount 之后，dumpe2fs 看： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475stable_kernel@kernel: ~/workspace/fs# sudo dumpe2fs /dev/loop0dumpe2fs 1.45.5 (07-Jan-2020)Filesystem volume name: &lt;none&gt;Last mounted on: &lt;not available&gt;Filesystem UUID: 4529647b-d4f6-4110-bf12-27b6e6df472dFilesystem magic number: 0xEF53Filesystem revision #: 1 (dynamic)Filesystem features: ext_attr resize_inode dir_index filetype sparse_super large_fileFilesystem flags: signed_directory_hashDefault mount options: user_xattr aclFilesystem state: not cleanErrors behavior: ContinueFilesystem OS type: LinuxInode count: 102400Block count: 102400Reserved block count: 5120Free blocks: 99108Free inodes: 102389First block: 0Block size: 4096Fragment size: 4096Reserved GDT blocks: 24Blocks per group: 32768Fragments per group: 32768Inodes per group: 25600Inode blocks per group: 800Filesystem created: Wed Jul 14 15:30:15 2021Last mount time: n/aLast write time: Wed Jul 14 15:30:21 2021Mount count: 1Maximum mount count: -1Last checked: Wed Jul 14 15:30:15 2021Check interval: 0 (&lt;none&gt;)Reserved blocks uid: 0 (user root)Reserved blocks gid: 0 (group root)First inode: 11Inode size: 128Default directory hash: half_md4Directory Hash Seed: 12d83817-68c9-4e5c-a665-94e370a4eb80Group 0: (Blocks 0-32767) Primary superblock at 0, Group descriptors at 1-1 Reserved GDT blocks at 2-25 Block bitmap at 26 (+26) Inode bitmap at 27 (+27) Inode table at 28-827 (+28) 31934 free blocks, 25589 free inodes, 2 directories Free blocks: 834-32767 Free inodes: 12-25600Group 1: (Blocks 32768-65535) Backup superblock at 32768, Group descriptors at 32769-32769 Reserved GDT blocks at 32770-32793 Block bitmap at 32794 (+26) Inode bitmap at 32795 (+27) Inode table at 32796-33595 (+28) 31940 free blocks, 25600 free inodes, 0 directories Free blocks: 33596-65535 Free inodes: 25601-51200Group 2: (Blocks 65536-98303) Block bitmap at 65536 (+0) Inode bitmap at 65537 (+1) Inode table at 65538-66337 (+2) 31966 free blocks, 25600 free inodes, 0 directories Free blocks: 66338-98303 Free inodes: 51201-76800Group 3: (Blocks 98304-102399) Backup superblock at 98304, Group descriptors at 98305-98305 Reserved GDT blocks at 98306-98329 Block bitmap at 98330 (+26) Inode bitmap at 98331 (+27) Inode table at 98332-99131 (+28) 3268 free blocks, 25600 free inodes, 0 directories Free blocks: 99132-102399 Free inodes: 76801-102400 生成一个文件之后 12345678910111213141516171819202122232425262728293031323334Group 0: (Blocks 0-32767) Primary superblock at 0, Group descriptors at 1-1 Reserved GDT blocks at 2-25 Block bitmap at 26 (+26) Inode bitmap at 27 (+27) Inode table at 28-827 (+28) 21694 free blocks, 25588 free inodes, 2 directories Free blocks: 834-22527 Free inodes: 13-25600Group 1: (Blocks 32768-65535) Backup superblock at 32768, Group descriptors at 32769-32769 Reserved GDT blocks at 32770-32793 Block bitmap at 32794 (+26) Inode bitmap at 32795 (+27) Inode table at 32796-33595 (+28) 10213 free blocks, 25600 free inodes, 0 directories Free blocks: 33596-33599, 55327-65535 Free inodes: 25601-51200Group 2: (Blocks 65536-98303) Block bitmap at 65536 (+0) Inode bitmap at 65537 (+1) Inode table at 65538-66337 (+2) 31966 free blocks, 25600 free inodes, 0 directories Free blocks: 66338-98303 Free inodes: 51201-76800Group 3: (Blocks 98304-102399) Backup superblock at 98304, Group descriptors at 98305-98305 Reserved GDT blocks at 98306-98329 Block bitmap at 98330 (+26) Inode bitmap at 98331 (+27) Inode table at 98332-99131 (+28) 3268 free blocks, 25600 free inodes, 0 directories Free blocks: 99132-102399 Free inodes: 76801-102400 参考Ext2文件系统布局及核心数据结构","link":"/2021/07/14/filesystem/dd%20dumpe2fs%20debugfs%20%E6%8E%A2%E7%B4%A2%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/"},{"title":"aarch64 中断处理","text":"介绍","link":"/2021/09/14/interrupt/aarch64%20%E4%B8%AD%E6%96%AD%E5%A4%84%E7%90%86/"},{"title":"和中断抢占相关api","text":"开关关中断apiapi1: 12#define local_irq_enable() do { raw_local_irq_enable(); } while (0)#define local_irq_disable() do { raw_local_irq_disable(); } while (0) api2: 12#define local_irq_save(flags) do { raw_local_irq_save(flags); } while (0)#define local_irq_restore(flags) do { raw_local_irq_restore(flags); } while (0) __this_cpu_read &amp;&amp; this_cpu_read中断上下文可以在中断上下文中使用，无需考虑被中断或者进程强占 123456789101112131415/* * Operations for contexts that are safe from preemption/interrupts. These * operations verify that preemption is disabled. */#define __this_cpu_read(pcp) \\({ \\ __this_cpu_preempt_check(&quot;read&quot;); \\ raw_cpu_read(pcp); \\})#define __this_cpu_write(pcp, val) \\({ \\ __this_cpu_preempt_check(&quot;write&quot;); \\ raw_cpu_write(pcp, val); \\}) 进程上下文可以在进程上下文中使用，实现了 强占、中断保护 123456/* * Operations with implied preemption/interrupt protection. These * operations can be used without worrying about preemption or interrupt. */#define this_cpu_read(pcp) __pcpu_size_call_return(this_cpu_read_, pcp)#define this_cpu_write(pcp, val) __pcpu_size_call(this_cpu_write_, pcp, val) 参考patch1参考patch2","link":"/2021/03/04/interrupt/%E5%92%8C%E4%B8%AD%E6%96%AD%E6%8A%A2%E5%8D%A0%E7%9B%B8%E5%85%B3api/"},{"title":"地址空间布局随机化","text":"ASLR(Address Space Layout Randomization)在2005年被引入到Linux的内核。 proc 接口sysctl可以通过设置kernel.randomize_va_space参数来设置地址随机化机制。randomize_va_space 的值有三种，分别是[0, 1, 2]：0 - 表示关闭进程地址空间随机化。1 - 表示将栈（stack）随机化。2 - 表示在1的基础上增加堆（heap）的随机化。 也可以直接操作 /proc/sys/kernel/randomize_va_space 文件来使得 demo使用这段代码来分析 randomize_va_space 效果： 12345678910111213141516#include&lt;stdio.h&gt;unsigned long sp(void){ #if defined(__i386__) asm(&quot;mov %esp, %eax&quot;); #elif defined(__x86_64__) asm(&quot;mov %rsp, %rax&quot;); #endif}int main(int argc, const char *argv[]){ unsigned long esp = sp(); printf(&quot;Stack pointer (ESP : 0x%lx)\\n&quot;,esp); return 0;} 开启随机化123tencent_clould@ubuntu: ~/workspace# cat /proc/sys/kernel/randomize_va_space2tencent_clould@ubuntu: ~/workspace# demo code 每次运行 sp 指针地址都不相同 1234567891011121314[Running] cd &quot;/tmp/&quot; &amp;&amp; gcc 123.c -o 123 &amp;&amp; &quot;/tmp/&quot;123Stack pointer (ESP : 0x7ffe548d3a50)[Done] exited with code=0 in 1.375 seconds[Running] cd &quot;/tmp/&quot; &amp;&amp; gcc 123.c -o 123 &amp;&amp; &quot;/tmp/&quot;123Stack pointer (ESP : 0x7fff1e5f1850)[Done] exited with code=0 in 0.23 seconds[Running] cd &quot;/tmp/&quot; &amp;&amp; gcc 123.c -o 123 &amp;&amp; &quot;/tmp/&quot;123Stack pointer (ESP : 0x7ffe65433d80)[Done] exited with code=0 in 0.064 seconds 关闭随机化1234VM-0-11-ubuntu# echo 0 &gt; /proc/sys/kernel/randomize_va_spaceVM-0-11-ubuntu# cat /proc/sys/kernel/randomize_va_space0VM-0-11-ubuntu# 连续运行多次，每次 sp 指针地址都是一致的： 1234567891011121314[Running] cd &quot;/tmp/&quot; &amp;&amp; gcc 123.c -o 123 &amp;&amp; &quot;/tmp/&quot;123Stack pointer (ESP : 0x7fffffffe280)[Done] exited with code=0 in 0.054 seconds[Running] cd &quot;/tmp/&quot; &amp;&amp; gcc 123.c -o 123 &amp;&amp; &quot;/tmp/&quot;123Stack pointer (ESP : 0x7fffffffe280)[Done] exited with code=0 in 0.05 seconds[Running] cd &quot;/tmp/&quot; &amp;&amp; gcc 123.c -o 123 &amp;&amp; &quot;/tmp/&quot;123Stack pointer (ESP : 0x7fffffffe280)[Done] exited with code=0 in 0.051 seconds","link":"/2021/09/23/%E7%94%A8%E6%88%B7%E7%A9%BA%E9%97%B4/%E5%9C%B0%E5%9D%80%E7%A9%BA%E9%97%B4%E5%B8%83%E5%B1%80%E9%9A%8F%E6%9C%BA%E5%8C%96/"},{"title":"gnu built_in","text":"简介__builtin_开头的符号其实是一些编译器内置的函数或者编译优化处理开关等，其作用类似于宏。宏是高级语言用于预编译时进行替换的源代码块，而内置函数则是用于在编译阶段进行替换的机器指令块。因此编译器的这些内置函数其实并不是真实的函数，而只是一段指令块，起到编译时的内联功能。 __builtin_types_compatible_p比较两个 变量类型是否相同，__builtin_types_compatible_p, 如果一致返回true否则返回false, 在编译阶段就出了结果。 123456789101112131415#include &lt;stdio.h&gt;#include &lt;stdbool.h&gt;#include &lt;stdlib.h&gt;int main(int argc, char **argv){ int a = 1, b = 2, c = 3; char e = 4, d = 5; bool f = false; f = __builtin_types_compatible_p(int, typeof(a)); printf(&quot;a is int %d\\n&quot;, f); f = __builtin_types_compatible_p(typeof(e), typeof(a)); printf(&quot;a &amp; e is same type %d\\n&quot;, f);} objdump -S a.out 之后 12345678910111213141516171819202122232425262728290000000000001149 &lt;main&gt;: 1149: f3 0f 1e fa endbr64 114d: 55 push %rbp 114e: 48 89 e5 mov %rsp,%rbp 1151: 48 83 ec 20 sub $0x20,%rsp 1155: 89 7d ec mov %edi,-0x14(%rbp) 1158: 48 89 75 e0 mov %rsi,-0x20(%rbp) 115c: c7 45 f4 01 00 00 00 movl $0x1,-0xc(%rbp) 1163: c7 45 f8 02 00 00 00 movl $0x2,-0x8(%rbp) 116a: c7 45 fc 03 00 00 00 movl $0x3,-0x4(%rbp) 1171: c6 45 f1 04 movb $0x4,-0xf(%rbp) 1175: c6 45 f2 05 movb $0x5,-0xe(%rbp) 1179: c6 45 f3 00 movb $0x0,-0xd(%rbp) 117d: c6 45 f3 01 movb $0x1,-0xd(%rbp) 1181: 0f b6 45 f3 movzbl -0xd(%rbp),%eax 1185: 89 c6 mov %eax,%esi 1187: 48 8d 3d 76 0e 00 00 lea 0xe76(%rip),%rdi # 2004 &lt;_IO_stdin_used+0x4&gt; 118e: b8 00 00 00 00 mov $0x0,%eax 1193: e8 b8 fe ff ff call 1050 &lt;printf@plt&gt; 1198: c6 45 f3 00 movb $0x0,-0xd(%rbp) 119c: 0f b6 45 f3 movzbl -0xd(%rbp),%eax 11a0: 89 c6 mov %eax,%esi 11a2: 48 8d 3d 68 0e 00 00 lea 0xe68(%rip),%rdi # 2011 &lt;_IO_stdin_used+0x11&gt; 11a9: b8 00 00 00 00 mov $0x0,%eax 11ae: e8 9d fe ff ff call 1050 &lt;printf@plt&gt; 11b3: b8 00 00 00 00 mov $0x0,%eax 11b8: c9 leave 11b9: c3 ret 11ba: 66 0f 1f 44 00 00 nopw 0x0(%rax,%rax,1) 可以看到 -0xd(%rbp) 这个地址存放的就是 变量 f 的值， f = __builtin_types_compatible_p(int, typeof(a)); 编译之后成为了 117d: c6 45 f3 01 movb $0x1,-0xd(%rbp) f = __builtin_types_compatible_p(typeof(e), typeof(a)); 编译之后成为了 1198: c6 45 f3 00 movb $0x0,-0xd(%rbp) 可以看出 __builtin_types_compatible_p 结果在编译时就已经确定了。 这个在 static-key 机制中使用。 __builtin_constant_p来判断某个表达式是否是一个常量，如果是常量返回true否则返回false 1 后续补充。。 参考LLVM编译器中的内置(built-in)函数 参考llvm 编译器学习之路","link":"/2021/09/14/%E7%BC%96%E8%AF%91%E5%99%A8/gnu%20built_in/"},{"title":"linux native aio","text":"api io_setup io_cancel io_destroy io_getevents io_submit 12345678IO_SETUP(2) Linux Programmer's Manual IO_SETUP(2)NAME io_setup - create an asynchronous I/O context......NOTES Glibc does not provide a wrapper function for this system call. You could invoke it using syscall(2). But instead, you probably want to use the io_setup() wrapper function provided by libaio. open 打开文件的时候，必须要以 O_DIRECT 方式打开 demo12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576#include &lt;stdlib.h&gt;#include &lt;stdbool.h&gt;#include &lt;string.h&gt;#include &lt;errno.h&gt;#include &lt;stdio.h&gt;#include &lt;unistd.h&gt;#include &lt;fcntl.h&gt;#include &lt;time.h&gt;#include &lt;sys/types.h&gt;#include &lt;sys/stat.h&gt;#include &lt;libaio.h&gt;#define FILEPATH &quot;/home/ubuntu/workspace/share/test_modules/filesystem/aio/kernel-aio/test.txt&quot;int main(void){ io_context_t context; struct iocb io[2], *p[2] = {&amp;io[0], &amp;io[1]}; struct io_event e[2]; unsigned nr_events = 10; struct timespec timeout; timeout.tv_sec = 0; timeout.tv_nsec = 1000 * 1000 *10; //10ms char *wbuf = 0, *rbuf = 0; int wbuflen = 512 * 1024 * 1024; int rbuflen = wbuflen + 1; posix_memalign((void **)&amp;wbuf, 512, wbuflen); posix_memalign((void **)&amp;rbuf, 512, rbuflen); memset(wbuf, 'a', wbuflen); memset(rbuf, 0, rbuflen); memset(&amp;context, 0, sizeof(io_context_t)); int ret = 0, comp_num = 0, i = 0; int fd = open(FILEPATH, O_CREAT | O_RDWR | O_DIRECT, 0644); if (fd &lt; 0) { printf(&quot;open file failed ！fd = %d\\n&quot;, fd); return 0; } if (0 != io_setup(nr_events, &amp;context)) { printf(&quot;io_setup error:%d\\n&quot;, errno); } io_prep_pwrite(&amp;io[0], fd, wbuf, wbuflen, 0); io_prep_pread(&amp;io[1], fd, rbuf, rbuflen - 1, 0); if ((ret = io_submit(context, 2, p)) != 2) { printf(&quot;io_submit error:%d\\n&quot;, ret); io_destroy(context); return -1; } while (true) { ret = io_getevents(context, 2, 2, e, &amp;timeout); if (ret &lt; 0) { printf(&quot;io_getevents error:%d\\n&quot;, ret); break; } else if (ret &gt; 0) { comp_num += ret; for (i = 0; i &lt; ret; ++i) { printf(&quot;result,res2:%d, res:%d\\n&quot;, e[i].res2, e[i].res); } } if (comp_num &gt;= 2) { printf(&quot;done !\\n&quot;); break; } printf(&quot;have not done !\\n&quot;); } return 0;} 编译 gcc main.c -laio -D_GNU_SOURCE a 12345678910111213ubuntu@zeku_server:/tmp/123/2 $ touch dubuntu@zeku_server:/tmp/123/2 $ cd ../ubuntu@zeku_server:/tmp/123 $ tree.├── 1│ ├── a│ └── b└── 2 ├── c └── d2 directories, 4 filesubuntu@zeku_server:/tmp/123 $","link":"/2021/07/27/filesystem/aio/linux%20native%20aio/"},{"title":"psoix aio","text":"linux上的 aio 方案： posix aio：posix 自己实现的 aio native aio：内核5.1之前实现的aio，与 posix aio 相比性能更好，但是要求打开文件必须是O_DIRECT io_uring: 5.1 版本之后实现的，配合 liburing 使用 api aio_read aio_write aio_error aio_cancel aio_fsync aio_return aio_suspend lio_listio posix 提供了很多 aio api， 123int aio_read(struct aiocb *aiocbp); Link with -lrt. 编译链接时 需要添加 -lrt normal demo1234567891011121314151617181920212223242526272829303132333435363738void test_normal(struct aiocb *my_aiocb){ int ret; ret = aio_read(my_aiocb); if (ret &lt; 0) { printf(&quot;test_normal aio_read error.\\n&quot;); return; } printf(&quot;test_normal aio_error = %d\\n&quot;, aio_error(my_aiocb)); while (aio_error(my_aiocb) == EINPROGRESS) { write(STDOUT_FILENO, &quot;.&quot;, 1); sleep(1); } printf(&quot;test_normal content: %s\\n&quot;, (char *)(my_aiocb-&gt;aio_buf)); printf(&quot;test_normal aio_error=%d\\n&quot;, aio_error(my_aiocb));}int main(void){ int fd, ret; struct aiocb my_aiocb; memset(&amp;my_aiocb, 0, sizeof(struct aiocb)); my_aiocb.aio_buf = buf; my_aiocb.aio_fildes = STDIN_FILENO; my_aiocb.aio_nbytes = 64; my_aiocb.aio_offset = 0; test_normal(&amp;my_aiocb); sleep(5); return 0;} signal demo1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465void signal_handler(int sig, siginfo_t *info, void *ctx){ int ret; struct aiocb *my_aiocb = (struct aiocb *)info-&gt;si_value.sival_ptr; printf(&quot;receive signal: %d\\n&quot;, sig); while (aio_error(my_aiocb) == EINPROGRESS) { write(STDOUT_FILENO, &quot;.&quot;, 1); sleep(1); } printf(&quot;signal_handler aio_error = %d errno = %d\\n&quot;, aio_error(my_aiocb), errno); ret = aio_return(my_aiocb); if (ret &lt; 0) { printf(&quot;signal_handler aio_return = %d\\n&quot;, ret); } printf(&quot;signal_handler content: %s\\n&quot;, (char *)(my_aiocb-&gt;aio_buf));}void test_signal(struct aiocb *my_aiocb){ int ret; struct sigaction sa; sigemptyset(&amp;sa.sa_mask); sa.sa_flags = SA_SIGINFO; // SA_RESTART, SA_NODEFER sa.sa_sigaction = signal_handler; sigaction(IO_SIGNAL, &amp;sa, NULL); my_aiocb-&gt;aio_sigevent.sigev_notify = SIGEV_SIGNAL; my_aiocb-&gt;aio_sigevent.sigev_signo = IO_SIGNAL; my_aiocb-&gt;aio_sigevent.sigev_value.sival_ptr = my_aiocb; ret = aio_read(my_aiocb); if (ret &lt; 0) { printf(&quot;test_signal aio_read() ret = %d\\n&quot;, ret); return; } printf(&quot;test_signal aio_error = %d\\n&quot;, aio_error(my_aiocb)); printf(&quot;test_signal waiting for aio...\\n&quot;); pause();}int main(void){ int fd, ret; struct aiocb my_aiocb; memset(&amp;my_aiocb, 0, sizeof(struct aiocb)); my_aiocb.aio_buf = buf; my_aiocb.aio_fildes = STDIN_FILENO; my_aiocb.aio_nbytes = 64; my_aiocb.aio_offset = 0; test_signal(&amp;my_aiocb); sleep(5); return 0;} thread demo12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667void thread_handler(union sigval val){ int ret; struct aiocb *my_aiocb = (struct aiocb *)val.sival_ptr; printf(&quot;thread handler()\\n&quot;); while (aio_error(my_aiocb) == EINPROGRESS) { write(STDOUT_FILENO, &quot;.&quot;, 1); sleep(1); } printf(&quot;thread_handler aio_error = %d\\n&quot;, aio_error(my_aiocb)); ret = aio_return(my_aiocb); if (ret &lt; 0) { printf(&quot;thread_handler aio_return = %d\\n&quot;, ret); } printf(&quot;thread_handler content: %s\\n&quot;, (char *)(my_aiocb-&gt;aio_buf)); // printf(&quot;content: %s\\n&quot;, buf);}void test_thread(struct aiocb *my_aiocb){ int ret; const struct aiocb *aio_list[1] = {my_aiocb}; my_aiocb-&gt;aio_sigevent.sigev_notify = SIGEV_THREAD; my_aiocb-&gt;aio_sigevent.sigev_notify_function = thread_handler; my_aiocb-&gt;aio_sigevent.sigev_value.sival_ptr = my_aiocb; ret = aio_read(my_aiocb); if (ret &lt; 0) { printf(&quot;test_thread aio_read() ret = %d\\n&quot;, ret); return; } printf(&quot;test_thread aio_error = %d\\n&quot;, aio_error(my_aiocb)); ret = aio_suspend(aio_list, 1, NULL); if (ret &lt; 0) { printf(&quot;test_thread aio_suspend() ret = %d\\n&quot;, ret); return; } printf(&quot;test_thread waiting for aio...\\n&quot;);}int main(){ int fd, ret; struct aiocb my_aiocb; memset(&amp;my_aiocb, 0, sizeof(struct aiocb)); my_aiocb.aio_buf = buf; my_aiocb.aio_fildes = STDIN_FILENO; my_aiocb.aio_nbytes = 64; my_aiocb.aio_offset = 0; test_thread(&amp;my_aiocb); sleep(5); return 0;}","link":"/2021/07/27/filesystem/aio/psoix%20aio/"},{"title":"tracepoint 原理与使用","text":"使用数据结构每个 tracepoint 对应一个 struct tracepoint 结构： 12345678910struct tracepoint { const char *name; /* Tracepoint name */ struct static_key key; struct static_call_key *static_call_key; void *static_call_tramp; void *iterator; int (*regfunc)(void); void (*unregfunc)(void); struct tracepoint_func __rcu *funcs;}; 1234567891011enum { TRACE_EVENT_FL_FILTERED = (1 &lt;&lt; TRACE_EVENT_FL_FILTERED_BIT), TRACE_EVENT_FL_CAP_ANY = (1 &lt;&lt; TRACE_EVENT_FL_CAP_ANY_BIT), TRACE_EVENT_FL_NO_SET_FILTER = (1 &lt;&lt; TRACE_EVENT_FL_NO_SET_FILTER_BIT), TRACE_EVENT_FL_IGNORE_ENABLE = (1 &lt;&lt; TRACE_EVENT_FL_IGNORE_ENABLE_BIT), TRACE_EVENT_FL_TRACEPOINT = (1 &lt;&lt; TRACE_EVENT_FL_TRACEPOINT_BIT), TRACE_EVENT_FL_DYNAMIC = (1 &lt;&lt; TRACE_EVENT_FL_DYNAMIC_BIT), TRACE_EVENT_FL_KPROBE = (1 &lt;&lt; TRACE_EVENT_FL_KPROBE_BIT), TRACE_EVENT_FL_UPROBE = (1 &lt;&lt; TRACE_EVENT_FL_UPROBE_BIT), TRACE_EVENT_FL_EPROBE = (1 &lt;&lt; TRACE_EVENT_FL_EPROBE_BIT),}; 12345678910111213enum { EVENT_FILE_FL_ENABLED = (1 &lt;&lt; EVENT_FILE_FL_ENABLED_BIT), EVENT_FILE_FL_RECORDED_CMD = (1 &lt;&lt; EVENT_FILE_FL_RECORDED_CMD_BIT), EVENT_FILE_FL_RECORDED_TGID = (1 &lt;&lt; EVENT_FILE_FL_RECORDED_TGID_BIT), EVENT_FILE_FL_FILTERED = (1 &lt;&lt; EVENT_FILE_FL_FILTERED_BIT), EVENT_FILE_FL_NO_SET_FILTER = (1 &lt;&lt; EVENT_FILE_FL_NO_SET_FILTER_BIT), EVENT_FILE_FL_SOFT_MODE = (1 &lt;&lt; EVENT_FILE_FL_SOFT_MODE_BIT), EVENT_FILE_FL_SOFT_DISABLED = (1 &lt;&lt; EVENT_FILE_FL_SOFT_DISABLED_BIT), EVENT_FILE_FL_TRIGGER_MODE = (1 &lt;&lt; EVENT_FILE_FL_TRIGGER_MODE_BIT), EVENT_FILE_FL_TRIGGER_COND = (1 &lt;&lt; EVENT_FILE_FL_TRIGGER_COND_BIT), EVENT_FILE_FL_PID_FILTER = (1 &lt;&lt; EVENT_FILE_FL_PID_FILTER_BIT), EVENT_FILE_FL_WAS_ENABLED = (1 &lt;&lt; EVENT_FILE_FL_WAS_ENABLED_BIT),}; https://www.cnblogs.com/honpey/p/9256279.html https://www.cnblogs.com/honpey/p/9012016.html","link":"/2021/09/14/%E5%86%85%E6%A0%B8%E8%A7%82%E6%B5%8B/%E6%80%A7%E8%83%BD%E7%A8%B3%E5%AE%9A%E6%80%A7/tracepoint%20%E5%8E%9F%E7%90%86%E4%B8%8E%E4%BD%BF%E7%94%A8/"},{"title":"perf-tools","text":"介绍翻译自github文档用于 Linux ftrace 和 perf_events（又名“perf”命令）的一系列开发中和不受支持的性能分析工具。 ftrace 和 perf 都是内核源代码中包含的核心 Linux 跟踪工具。 您的系统可能已经有 ftrace，而 perf 通常只是一个包添加（请参阅先决条件）。 这些工具被设计为易于安装（最少的依赖项），提供高级性能可观察性，并且易于使用：做一件事并做好。 该集合由 Brendan Gregg（DTraceToolkit 的作者）创建。 这些工具中的许多都采用了变通方法，以便在现有 Linux 内核上实现功能。 因此，许多工具都有警告（请参阅手册页），在添加未来的内核功能或新的跟踪子系统之前，应将它们的实现视为占位符。 在 仓库根目录还有各个目录下，有多个 shell 脚本: 123456789101112131415iosnoop: trace disk I/O with details including latency. Examples.iolatency: summarize disk I/O latency as a histogram. Examples.execsnoop: trace process exec() with command line argument details. Examples.opensnoop: trace open() syscalls showing filenames. Examples.killsnoop: trace kill() signals showing process and signal details. Examples.fs/cachestat: basic cache hit/miss statistics for the Linux page cache. Examples.net/tcpretrans: show TCP retransmits, with address and other details. Examples.system/tpoint: trace a given tracepoint. Examples.kernel/funccount: count kernel function calls, matching a string with wildcards. Examples.kernel/functrace: trace kernel function calls, matching a string with wildcards. Examples.kernel/funcslower: trace kernel functions slower than a threshold. Examples.kernel/funcgraph: trace a graph of kernel function calls, showing children and times. Examples.kernel/kprobe: dynamically trace a kernel function call or its return, with variables. Examples.user/uprobe: dynamically trace a user-level function call or its return, with variables. Examples.tools/reset-ftrace: reset ftrace state if needed. Examples. 在 ./bin 目录下有以上各个shell 的软连接，所以执行这些工具直接到 bin目录即可： 12345root@ubuntu-HP-ProDesk-680-G4-MT:/home/ubuntu/workspace/perf-tools/bin# lsbitesize funccount functrace killsnoop perf-stat-hist tcpretranscachestat funcgraph iolatency kprobe reset-ftrace tpointexecsnoop funcslower iosnoop opensnoop syscount uproberoot@ubuntu-HP-ProDesk-680-G4-MT:/home/ubuntu/workspace/perf-tools/bin# demoperf-tools 里面的一些工具在 trace 方面很实用，相见恨晚！下面记录几个 demo: funccountcount kernel function calls, matching a string with wildcards.主要用来统计 函数调用，可以搭配通配符来使用 123456789101112131415161718192021root@ubuntu-HP-ProDesk-680-G4-MT:/home/ubuntu/workspace/perf-tools/bin# ./funccount &quot;bio_*&quot;Tracing &quot;bio_*&quot;... Ctrl-C to end.^CFUNC COUNTbio_attempt_front_merge 1bio_attempt_back_merge 82bio_crypt_ctx_mergeable 83bio_crypt_rq_ctx_compatible 364bio_alloc_bioset 378bio_associate_blkg 378bio_endio 378bio_free 378bio_integrity_prep 378bio_put 378bio_advance 382bio_associate_blkg_from_css 442bio_uninit 756bio_add_page 815Ending tracing...root@ubuntu-HP-ProDesk-680-G4-MT:/home/ubuntu/workspace/perf-tools/bin# -i : 间隔s输出结果 -t : 输出 top x 的结果 -d : 总共trace 的时间多久 12345678910111213root@ubuntu-HP-ProDesk-680-G4-MT:/home/ubuntu/workspace/perf-tools/bin# ./funccount -i 2 -t 2 -d 4 &quot;bio_*&quot;Tracing &quot;bio_*&quot; for 4 seconds. Top 2 only...FUNC COUNTbio_uninit 22bio_crypt_rq_ctx_compatible 55FUNC COUNTbio_associate_blkg_from_css 11bio_uninit 20Ending tracing...root@ubuntu-HP-ProDesk-680-G4-MT:/home/ubuntu/workspace/perf-tools/bin# 为啥可以支持通配符？在 /sys/kernel/debug/tracing/available_filter_functions 中记录了所有可以被 trace的 symbol. 123root@ubuntu-HP-ProDesk-680-G4-MT:/home/ubuntu/workspace/perf-tools/bin# cat /sys/kernel/debug/tracing/available_filter_functions | wc -l67030root@ubuntu-HP-ProDesk-680-G4-MT:/home/ubuntu/workspace/perf-tools/bin# 可以使用带通配符的 string 到 这个文件中去查找符号条件的 function。 functracetrace kernel function calls, matching a string with wildcards,支持通配符 12345678root@ubuntu-HP-ProDesk-680-G4-MT:/home/ubuntu/workspace/perf-tools/bin# ./functrace &quot;kmem_cache_alloc_*&quot;Tracing &quot;kmem_cache_alloc_*&quot;... Ctrl-C to end. functrace-1512675 [009] .... 1298127.500687: kmem_cache_alloc_node &lt;-dup_task_struct functrace-1512675 [009] .... 1298127.500709: kmem_cache_alloc_trace &lt;-alloc_fdtable &lt;...&gt;-1512680 [001] .... 1298127.501196: kmem_cache_alloc_trace &lt;-alloc_bprm &lt;...&gt;-1512680 [001] .... 1298127.501320: kmem_cache_alloc_trace &lt;-load_elf_binary node-3706960 [008] .... 1298127.503100: kmem_cache_alloc_node &lt;-__alloc_skb node-3706960 [008] .... 1298127.503428: kmem_cache_alloc_node &lt;-__alloc_skb 使用 -H 显示 header: 12345678910111213141516root@ubuntu-HP-ProDesk-680-G4-MT:/home/ubuntu/workspace/perf-tools/bin# ./functrace -H &quot;kmem_cache_alloc_*&quot;Tracing &quot;kmem_cache_alloc_*&quot;... Ctrl-C to end.^C# tracer: function## entries-in-buffer/entries-written: 30/30 #P:12## _-----=&gt; irqs-off# / _----=&gt; need-resched# | / _---=&gt; hardirq/softirq# || / _--=&gt; preempt-depth# ||| / delay# TASK-PID CPU# |||| TIMESTAMP FUNCTION# | | | |||| | | functrace-1512958 [003] .... 1298157.804761: kmem_cache_alloc_node &lt;-dup_task_struct functrace-1512958 [003] .... 1298157.804774: kmem_cache_alloc_trace &lt;-alloc_fdtable cat-1512960 [011] .... 1298157.805012: kmem_cache_alloc_trace &lt;-alloc_bprm funcslower trace kernel functions slower than a threshold. 123456789101112root@ubuntu-HP-ProDesk-680-G4-MT:/home/ubuntu/workspace/perf-tools/bin# ./funcslower ext4_mpage_readpages 10Tracing &quot;ext4_mpage_readpages&quot; slower than 10 us... Ctrl-C to end. 11) + 12.260 us | } /* ext4_mpage_readpages */ 8) + 30.049 us | } /* ext4_mpage_readpages */ 8) + 13.164 us | } /* ext4_mpage_readpages */ 8) + 21.117 us | } /* ext4_mpage_readpages */ 9) + 17.850 us | } /* ext4_mpage_readpages */ 9) + 13.449 us | } /* ext4_mpage_readpages */ 10) + 16.533 us | } /* ext4_mpage_readpages */ 0) + 32.654 us | } /* ext4_mpage_readpages */ 0) + 12.850 us | } /* ext4_mpage_readpages */ 0) + 20.576 us | } /* ext4_mpage_readpages */ 假设 你怀疑系统中某个问题是由于 ext4_mpage_readpages 执行慢导致的，那么可以使用这个 cmd 来确认一下 可以使用 -Pt 显示更详细信息 1234root@ubuntu-HP-ProDesk-680-G4-MT:/home/ubuntu/workspace/perf-tools/bin# ./funcslower -Pt ext4_mpage_readpages 10Tracing &quot;ext4_mpage_readpages&quot; slower than 10 us... Ctrl-C to end.1298799.300724 | 7) bash-1518301 | + 25.581 us | } /* ext4_mpage_readpages */1298799.305213 | 7) cksum-1518301 | + 14.441 us | } /* ext4_mpage_readpages */ funcgraphtrace a graph of kernel function calls, showing children and times. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182root@ubuntu-HP-ProDesk-680-G4-MT:/home/ubuntu/workspace/perf-tools/bin# ./funcgraph scheduleTracing &quot;schedule&quot;... Ctrl-C to end. 4) 1.768 us | file_ra_state_init(); 4) | schedule() { 4) | rcu_note_context_switch() { 4) 0.382 us | rcu_qs(); 4) 0.998 us | } 4) 0.320 us | _raw_spin_lock(); 4) 0.351 us | update_rq_clock(); 4) | psi_task_change() { 4) 0.285 us | psi_flags_change(); 4) | psi_group_change() { 4) 0.430 us | record_times(); 4) 1.344 us | } 4) | psi_group_change() { 4) 0.333 us | record_times(); 4) 0.909 us | } 4) | psi_group_change() { 4) 0.384 us | record_times(); 4) 0.965 us | } 4) | psi_group_change() { 4) 0.337 us | record_times(); 4) 0.900 us | } 4) 6.000 us | } 4) | dequeue_task_fair() { 4) | dequeue_entity() { 4) | update_curr() { 4) 0.277 us | update_min_vruntime(); 4) | cpuacct_charge() { 4) 0.323 us | rcu_read_unlock_strict(); 4) 0.874 us | } 4) | __cgroup_account_cputime() { 4) 0.283 us | cgroup_rstat_updated(); 4) 0.814 us | } 4) 0.288 us | rcu_read_unlock_strict(); 4) 3.616 us | } 4) 0.488 us | __update_load_avg_se(); 4) 0.518 us | __update_load_avg_cfs_rq(); 4) 0.284 us | clear_buddies(); 4) 0.318 us | update_cfs_group(); 4) 0.304 us | update_min_vruntime(); 4) 7.470 us | } 4) | dequeue_entity() { 4) | update_curr() { 4) 0.328 us | __calc_delta(); 4) 0.280 us | update_min_vruntime(); 4) 1.505 us | } 4) 0.422 us | __update_load_avg_se(); 4) 0.332 us | __update_load_avg_cfs_rq(); 4) 0.284 us | clear_buddies(); 4) | update_cfs_group() { 4) 0.309 us | reweight_entity(); 4) 0.876 us | } 4) 0.293 us | update_min_vruntime(); 4) 5.538 us | } 4) 0.282 us | hrtick_update(); 4) + 14.506 us | } 4) | pick_next_task_fair() { 4) | newidle_balance() { 4) 0.276 us | __msecs_to_jiffies(); 4) 0.271 us | rcu_read_unlock_strict(); 4) 0.347 us | nohz_newidle_balance(); 4) 2.079 us | } 4) 2.616 us | } 4) | put_prev_task_fair() { 4) | put_prev_entity() { 4) 0.283 us | check_cfs_rq_runtime(); 4) 0.927 us | } 4) | put_prev_entity() { 4) 0.283 us | check_cfs_rq_runtime(); 4) 0.807 us | } 4) 2.607 us | } 4) | pick_next_task_idle() { 4) | __update_idle_core() { 4) 0.281 us | rcu_read_unlock_strict(); 4) 1.031 us | } 4) 1.668 us | } 4) 0.306 us | psi_task_switch(); 4) 0.897 us | __traceiter_sched_switch(); 4) 0.315 us | enter_lazy_tlb(); 4) 0.403 us | copy_fpregs_to_fpstate(); 0) 1.234 us | finish_task_switch.isra.0(); 可以看到和 ftrace 很类似，但是这样的 输出很难看，可以 使用 -m 精简一下输出 12345678910111213141516171819root@ubuntu-HP-ProDesk-680-G4-MT:/home/ubuntu/workspace/perf-tools/bin# ./funcgraph -m2 scheduleTracing &quot;schedule&quot;... Ctrl-C to end. 0) 1.265 us | file_ra_state_init(); 0) | schedule() { 0) 0.308 us | rcu_note_context_switch(); 0) 0.177 us | _raw_spin_lock(); 0) 0.190 us | update_rq_clock(); 0) 1.267 us | psi_task_change(); 0) 3.115 us | dequeue_task_fair(); 0) + 16.366 us | pick_next_task_fair(); 0) 0.677 us | put_prev_task_fair(); 0) 0.520 us | pick_next_task_idle(); 0) 0.206 us | psi_task_switch(); 0) 0.550 us | __traceiter_sched_switch(); 0) 0.176 us | enter_lazy_tlb(); 0) 0.241 us | copy_fpregs_to_fpstate(); 9) 0.903 us | finish_task_switch.isra.0(); 9) 0.567 us | wq_worker_running(); 9) @ 103141.5 us | } /* schedule */ 可以看到 每个函数的耗费时间，比如这个例子中 可以看到 pick_next_task_fair 耗时较大，可以进一步分析 还可以使用 -p 来追踪制定 进程的目的 kprobe dynamically trace a kernel function call or its return, with variables. 与前面的工具不一样的是， kprobe 可以跟踪 return, 还有 内部variables。 1234567891011121314151617root@ubuntu-HP-ProDesk-680-G4-MT:/home/ubuntu/workspace/perf-tools/bin# ./kprobe p:do_sys_openat2Tracing kprobe do_sys_openat2. Ctrl-C to end. &lt;...&gt;-1890130 [003] .... 1304707.196057: do_sys_openat2: (do_sys_openat2+0x0/0x150) &lt;...&gt;-1890130 [003] .... 1304707.196106: do_sys_openat2: (do_sys_openat2+0x0/0x150) &lt;...&gt;-1890130 [003] .... 1304707.196676: do_sys_openat2: (do_sys_openat2+0x0/0x150) &lt;...&gt;-1890130 [003] .... 1304707.196870: do_sys_openat2: (do_sys_openat2+0x0/0x150) node-3706960 [008] .... 1304707.252605: do_sys_openat2: (do_sys_openat2+0x0/0x150) node-3706960 [008] .... 1304707.252721: do_sys_openat2: (do_sys_openat2+0x0/0x150) node-3706960 [008] .... 1304707.252766: do_sys_openat2: (do_sys_openat2+0x0/0x150) node-3706960 [008] .... 1304707.252802: do_sys_openat2: (do_sys_openat2+0x0/0x150) node-3706960 [008] .... 1304707.252849: do_sys_openat2: (do_sys_openat2+0x0/0x150) node-3706960 [008] .... 1304707.252887: do_sys_openat2: (do_sys_openat2+0x0/0x150) node-3706960 [008] .... 1304707.252930: do_sys_openat2: (do_sys_openat2+0x0/0x150) node-3706960 [008] .... 1304707.253007: do_sys_openat2: (do_sys_openat2+0x0/0x150)^CEnding tracing...root@ubuntu-HP-ProDesk-680-G4-MT:/home/ubuntu/workspace/perf-tools/bin# The “p:” is for creating a probe. Use “r:” to probe the return of the function:“p:” 是创建一个kprobe. “r:” 是创建一个kretprobe 12345678910111213141516171819root@ubuntu-HP-ProDesk-680-G4-MT:/home/ubuntu/workspace/perf-tools/bin# ./kprobe r:do_sys_openat2Tracing kprobe do_sys_openat2. Ctrl-C to end. kprobe-1892194 [007] .... 1305064.017143: do_sys_openat2: (__x64_sys_openat+0x56/0x90 &lt;- do_sys_openat2) &lt;...&gt;-1892196 [009] .... 1305064.018585: do_sys_openat2: (__x64_sys_openat+0x56/0x90 &lt;- do_sys_openat2) &lt;...&gt;-1892196 [009] .... 1305064.018633: do_sys_openat2: (__x64_sys_openat+0x56/0x90 &lt;- do_sys_openat2) &lt;...&gt;-1892196 [009] .... 1305064.019216: do_sys_openat2: (__x64_sys_openat+0x56/0x90 &lt;- do_sys_openat2) &lt;...&gt;-1892196 [009] .... 1305064.019410: do_sys_openat2: (__x64_sys_openat+0x56/0x90 &lt;- do_sys_openat2) node-3706960 [004] .... 1305064.032758: do_sys_openat2: (__x64_sys_openat+0x56/0x90 &lt;- do_sys_openat2) node-3706960 [004] .... 1305064.033256: do_sys_openat2: (__x64_sys_openat+0x56/0x90 &lt;- do_sys_openat2) node-3706960 [004] .... 1305064.033325: do_sys_openat2: (__x64_sys_openat+0x56/0x90 &lt;- do_sys_openat2) node-3706960 [004] .... 1305064.033363: do_sys_openat2: (__x64_sys_openat+0x56/0x90 &lt;- do_sys_openat2) node-3706960 [004] .... 1305064.033396: do_sys_openat2: (__x64_sys_openat+0x56/0x90 &lt;- do_sys_openat2) node-3706960 [004] .... 1305064.033428: do_sys_openat2: (__x64_sys_openat+0x56/0x90 &lt;- do_sys_openat2) node-3706960 [004] .... 1305064.034963: do_sys_openat2: (__x64_sys_openat+0x56/0x90 &lt;- do_sys_openat2) node-3706960 [004] .... 1305064.035047: do_sys_openat2: (__x64_sys_openat+0x56/0x90 &lt;- do_sys_openat2) node-1358618 [000] .... 1305064.191464: do_sys_openat2: (__x64_sys_openat+0x56/0x90 &lt;- do_sys_openat2)^CEnding tracing...root@ubuntu-HP-ProDesk-680-G4-MT:/home/ubuntu/workspace/perf-tools/bin# -H 是 显示 headers 可以显示 返回值 12345678root@ubuntu-HP-ProDesk-680-G4-MT:/home/ubuntu/workspace/perf-tools/bin# ./kprobe 'r:myopen do_sys_openat2 $retval'Tracing kprobe myopen. Ctrl-C to end. kprobe-1892927 [000] .... 1305161.519664: myopen: (__x64_sys_openat+0x56/0x90 &lt;- do_sys_openat2) arg1=0x3 &lt;...&gt;-1892933 [002] .... 1305161.520039: myopen: (__x64_sys_openat+0x56/0x90 &lt;- do_sys_openat2) arg1=0x3 &lt;...&gt;-1892933 [002] .... 1305161.520050: myopen: (__x64_sys_openat+0x56/0x90 &lt;- do_sys_openat2) arg1=0x3 &lt;...&gt;-1892933 [002] .... 1305161.520178: myopen: (__x64_sys_openat+0x56/0x90 &lt;- do_sys_openat2) arg1=0x3 &lt;...&gt;-1892933 [002] .... 1305161.520227: myopen: (__x64_sys_openat+0x56/0x90 &lt;- do_sys_openat2) arg1=0x3 sh-1892932 [010] .... 1305161.521214: myopen: (__x64_sys_openat+0x56/0x90 &lt;- do_sys_openat2) arg1=0x3 ‘r:myopen do_sys_open $retval’ 是一个 kprobe 定义，和 kernel 文档 Documentation/trace/kprobetrace.txt 里面定义的一样；除了使用 probe 别名，还可以为参数提供任意名称。例如，将默认值”arg1”改为”rval”: 12345root@ubuntu-HP-ProDesk-680-G4-MT:/home/ubuntu/workspace/perf-tools/bin# ./kprobe 'r:myopen do_sys_openat2 ret=$retval'Tracing kprobe myopen. Ctrl-C to end. kprobe-1894758 [009] .... 1305461.694802: myopen: (__x64_sys_openat+0x56/0x90 &lt;- do_sys_openat2) ret=0x3 &lt;...&gt;-1894760 [005] .... 1305461.696161: myopen: (__x64_sys_openat+0x56/0x90 &lt;- do_sys_openat2) ret=0x3 node-3706960 [006] .... 1305461.705250: myopen: (__x64_sys_openat+0x56/0x90 &lt;- do_sys_openat2) ret=0x19 还可以trace 函数func的入口参数的值，比如 1234567891011121314root@ubuntu-HP-ProDesk-680-G4-MT:/home/ubuntu/workspace/perf-tools/bin# ./kprobe 'p:myopen do_sys_openat2 filename=+0(%si):string'Tracing kprobe myopen. Ctrl-C to end. &lt;...&gt;-1897778 [011] .... 1305798.993302: myopen: (do_sys_openat2+0x0/0x150) filename=&quot;/etc/ld.so.cache&quot; &lt;...&gt;-1897778 [011] .... 1305798.993349: myopen: (do_sys_openat2+0x0/0x150) filename=&quot;/lib/x86_64-linux-gnu/libc.so.6&quot; &lt;...&gt;-1897778 [011] .... 1305798.993863: myopen: (do_sys_openat2+0x0/0x150) filename=&quot;/usr/lib/locale/locale-archive&quot; &lt;...&gt;-1897778 [011] .... 1305798.994040: myopen: (do_sys_openat2+0x0/0x150) filename=&quot;trace_pipe&quot; node-3706960 [003] .... 1305799.184729: myopen: (do_sys_openat2+0x0/0x150) filename=&quot;/proc/1452624/cmdline&quot; node-3706960 [003] .... 1305799.184848: myopen: (do_sys_openat2+0x0/0x150) filename=&quot;/proc/1368025/cmdline&quot; node-3706960 [003] .... 1305799.186491: myopen: (do_sys_openat2+0x0/0x150) filename=&quot;/proc/29675/cmdline&quot; node-3706960 [003] .... 1305799.186586: myopen: (do_sys_openat2+0x0/0x150) filename=&quot;/proc/660625/cmdline&quot; node-3706960 [003] .... 1305799.186638: myopen: (do_sys_openat2+0x0/0x150) filename=&quot;/proc/1368168/cmdline&quot; node-3706960 [003] .... 1305799.186678: myopen: (do_sys_openat2+0x0/0x150) filename=&quot;/proc/1368591/cmdline&quot; node-3706960 [003] .... 1305799.186723: myopen: (do_sys_openat2+0x0/0x150) filename=&quot;/proc/1897776/cmdline&quot;^C 因为 do_sys_openat2 原型是： 1234567static long do_sys_openat2(int dfd, const char __user *filename, struct open_how *how){ struct open_flags op; int fd = build_open_flags(how, &amp;op); struct filename *tmp;} 第二个参数是 filename，且类型是 char*, 根据 syscall 的约定，man syscall 可以看到： 12345678910111213Arch/ABI arg1 arg2 arg3 arg4 arg5 arg6 arg7 Notes ────────────────────────────────────────────────────────────── alpha a0 a1 a2 a3 a4 a5 - arc r0 r1 r2 r3 r4 r5 - arm/OABI r0 r1 r2 r3 r4 r5 r6 arm/EABI r0 r1 r2 r3 r4 r5 r6 arm64 x0 x1 x2 x3 x4 x5 - sparc/32 o0 o1 o2 o3 o4 o5 - sparc/64 o0 o1 o2 o3 o4 o5 - tile R00 R01 R02 R03 R04 R05 - x86-64 rdi rsi rdx r10 r8 r9 - x32 rdi rsi rdx r10 r8 r9 - xtensa a6 a3 a4 a5 a8 a9 - 在 x86-64 中，第二个参数存放在 rsi中。。所以命令是 kprobe 'p:myopen do_sys_openat2 filename=+0(%si):string' 也可以通过 -p 指定特定进程来 做trace. uprobedynamically trace a user-level function call or its return, with variables. 12345678910root@ubuntu-HP-ProDesk-680-G4-MT:/home/ubuntu/workspace/perf-tools/bin# ./uprobe p:bash:readlineTracing uprobe readline (p:readline /usr/bin/bash:0xd4950). Ctrl-C to end. bash-1452624 [003] d... 1306503.199858: readline: (0x55621dedf950) bash-1452624 [003] d... 1306503.443193: readline: (0x55621dedf950) bash-1452624 [003] d... 1306503.615018: readline: (0x55621dedf950) bash-1452624 [003] d... 1306503.772181: readline: (0x55621dedf950) bash-1452624 [003] d... 1306503.954737: readline: (0x55621dedf950) bash-1452624 [003] d... 1306505.194449: readline: (0x55621dedf950)^CEnding tracing... reset-ftracereset ftrace state if needed. tpoint有较多语法需要学习。 参考Linux Performance Analysis: New Tools and Old Secrets","link":"/2021/01/28/%E5%86%85%E6%A0%B8%E8%A7%82%E6%B5%8B/perf%E7%9B%B8%E5%85%B3/perf-tools/"},{"title":"static_key机制","text":"还没有完全看懂 5.14 版本的 static-key 机制，这里仅仅做个记录 概览内核中有很多判断条件在正常情况下的结果都是固定的，除非极其罕见的场景才会改变，通常单个的这种判断的代价很低可以忽略，但是如果这种判断数量巨大且被频繁执行，那就会带来性能损失了。内核的static-key机制就是为了优化这种场景,其优化的结果是：对于大多数情况，对应的判断被优化为一个NOP指令，在非常有限的场景就变成jump XXX一类的指令，使得对应的代码段得到执行。 典型场景就是 tracepoint 机制中。 demo在 mm 目录下找一个内有两个 tracepoint 的函数–compact_zone compact_zone 内部有三个tracepoint: 1234567891011121314static enum compact_resultcompact_zone(struct compact_control *cc, struct capture_control *capc){ ...... trace_mm_compaction_begin(start_pfn, cc-&gt;migrate_pfn, cc-&gt;free_pfn, end_pfn, sync); ...... trace_mm_compaction_migratepages(cc-&gt;nr_migratepages, err, &amp;cc-&gt;migratepages); ...... trace_mm_compaction_end(start_pfn, cc-&gt;migrate_pfn, cc-&gt;free_pfn, end_pfn, sync, ret); return ret; normal 情况下 crash, compact_zone 的汇编是： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596970xffffffff8242f820 &lt;compact_zone&gt;: push %r150xffffffff8242f822 &lt;compact_zone+2&gt;: push %r140xffffffff8242f824 &lt;compact_zone+4&gt;: mov %rdi,%r140xffffffff8242f827 &lt;compact_zone+7&gt;: push %r130xffffffff8242f829 &lt;compact_zone+9&gt;: push %r120xffffffff8242f82b &lt;compact_zone+11&gt;: push %rbp0xffffffff8242f82c &lt;compact_zone+12&gt;: push %rbx0xffffffff8242f82d &lt;compact_zone+13&gt;: sub $0x88,%rsp0xffffffff8242f834 &lt;compact_zone+20&gt;: mov 0x40(%rdi),%rdi0xffffffff8242f838 &lt;compact_zone+24&gt;: mov 0x70(%r14),%ebp0xffffffff8242f83c &lt;compact_zone+28&gt;: mov %rsi,0x28(%rsp)0xffffffff8242f841 &lt;compact_zone+33&gt;: mov 0x80(%rdi),%rbx0xffffffff8242f848 &lt;compact_zone+40&gt;: mov %gs:0x28,%rax0xffffffff8242f851 &lt;compact_zone+49&gt;: mov %rax,0x80(%rsp)0xffffffff8242f859 &lt;compact_zone+57&gt;: xor %eax,%eax0xffffffff8242f85b &lt;compact_zone+59&gt;: mov 0x70(%rdi),%rax0xffffffff8242f85f &lt;compact_zone+63&gt;: mov %r14,(%r14)0xffffffff8242f862 &lt;compact_zone+66&gt;: mov 0x18b66d7(%rip),%r12d # 0xffffffff83ce5f400xffffffff8242f869 &lt;compact_zone+73&gt;: movq $0x0,0x48(%r14)0xffffffff8242f871 &lt;compact_zone+81&gt;: mov %rax,0x30(%rsp)0xffffffff8242f876 &lt;compact_zone+86&gt;: lea 0x10(%r14),%rax0xffffffff8242f87a &lt;compact_zone+90&gt;: mov %rax,0x20(%rsp)0xffffffff8242f87f &lt;compact_zone+95&gt;: mov %rax,0x10(%r14)0xffffffff8242f883 &lt;compact_zone+99&gt;: mov %rax,0x18(%r14)0xffffffff8242f887 &lt;compact_zone+103&gt;: mov 0x5c(%r14),%eax0xffffffff8242f88b &lt;compact_zone+107&gt;: movq $0x0,0x50(%r14)0xffffffff8242f893 &lt;compact_zone+115&gt;: movq $0x0,0x20(%r14)0xffffffff8242f89b &lt;compact_zone+123&gt;: mov %r14,0x8(%r14)0xffffffff8242f89f &lt;compact_zone+127&gt;: test %r12d,%r12d0xffffffff8242f8a2 &lt;compact_zone+130&gt;: jne 0xffffffff8243062d &lt;compact_zone+3597&gt;0xffffffff8242f8a8 &lt;compact_zone+136&gt;: shr $0x3,%eax0xffffffff8242f8ab &lt;compact_zone+139&gt;: and $0x3,%eax0xffffffff8242f8ae &lt;compact_zone+142&gt;: mov 0x68(%r14),%edx0xffffffff8242f8b2 &lt;compact_zone+146&gt;: mov 0x6c(%r14),%ecx0xffffffff8242f8b6 &lt;compact_zone+150&gt;: mov %eax,0x64(%r14)0xffffffff8242f8ba &lt;compact_zone+154&gt;: mov 0x60(%r14),%esi0xffffffff8242f8be &lt;compact_zone+158&gt;: callq 0xffffffff8242f710 &lt;compaction_suitable&gt;0xffffffff8242f8c3 &lt;compact_zone+163&gt;: cmp $0x8,%eax0xffffffff8242f8c6 &lt;compact_zone+166&gt;: mov %eax,%r12d0xffffffff8242f8c9 &lt;compact_zone+169&gt;: sete %dl0xffffffff8242f8cc &lt;compact_zone+172&gt;: cmp $0x1,%eax0xffffffff8242f8cf &lt;compact_zone+175&gt;: sete %al0xffffffff8242f8d2 &lt;compact_zone+178&gt;: or %al,%dl0xffffffff8242f8d4 &lt;compact_zone+180&gt;: jne 0xffffffff8242fccd &lt;compact_zone+1197&gt;0xffffffff8242f8da &lt;compact_zone+186&gt;: mov 0x40(%r14),%rdi0xffffffff8242f8de &lt;compact_zone+190&gt;: mov 0x478(%rdi),%eax0xffffffff8242f8e4 &lt;compact_zone+196&gt;: cmp %eax,0x60(%r14)0xffffffff8242f8e8 &lt;compact_zone+200&gt;: jl 0xffffffff8242f8f7 &lt;compact_zone+215&gt;0xffffffff8242f8ea &lt;compact_zone+202&gt;: cmpl $0x6,0x474(%rdi)0xffffffff8242f8f1 &lt;compact_zone+209&gt;: je 0xffffffff8243050d &lt;compact_zone+3309&gt;0xffffffff8242f8f7 &lt;compact_zone+215&gt;: mov 0x30(%rsp),%rdx0xffffffff8242f8fc &lt;compact_zone+220&gt;: mov %rbx,%rax0xffffffff8242f8ff &lt;compact_zone+223&gt;: xor %edi,%edi0xffffffff8242f901 &lt;compact_zone+225&gt;: movq $0x0,0x38(%r14)0xffffffff8242f909 &lt;compact_zone+233&gt;: add %rdx,%rax0xffffffff8242f90c &lt;compact_zone+236&gt;: test %ebp,%ebp0xffffffff8242f90e &lt;compact_zone+238&gt;: setne %dil0xffffffff8242f912 &lt;compact_zone+242&gt;: cmpb $0x0,0x79(%r14)0xffffffff8242f917 &lt;compact_zone+247&gt;: mov %rax,0x50(%rsp)0xffffffff8242f91c &lt;compact_zone+252&gt;: mov %edi,0x68(%rsp)0xffffffff8242f920 &lt;compact_zone+256&gt;: je 0xffffffff8242fcf9 &lt;compact_zone+1241&gt;0xffffffff8242f926 &lt;compact_zone+262&gt;: lea -0x1(%rax),%rcx0xffffffff8242f92a &lt;compact_zone+266&gt;: mov %rdx,0x30(%r14)0xffffffff8242f92e &lt;compact_zone+270&gt;: and $0xfffffffffffffe00,%rcx0xffffffff8242f935 &lt;compact_zone+277&gt;: mov %rcx,0x28(%r14)0xffffffff8242f939 &lt;compact_zone+281&gt;: movb $0x0,0x4b(%rsp)0xffffffff8242f93e &lt;compact_zone+286&gt;: test %ebp,%ebp0xffffffff8242f940 &lt;compact_zone+288&gt;: jne 0xffffffff8242f959 &lt;compact_zone+313&gt;0xffffffff8242f942 &lt;compact_zone+290&gt;: mov 0x40(%r14),%rax0xffffffff8242f946 &lt;compact_zone+294&gt;: mov 0x458(%rax),%rbx0xffffffff8242f94d &lt;compact_zone+301&gt;: cmp %rbx,0x450(%rax)0xffffffff8242f954 &lt;compact_zone+308&gt;: sete 0x4b(%rsp)0xffffffff8242f959 &lt;compact_zone+313&gt;: nopl 0x0(%rax,%rax,1)0xffffffff8242f95e &lt;compact_zone+318&gt;: mov %gs:0x7dbe1c03(%rip),%eax # 0x115680xffffffff8242f965 &lt;compact_zone+325&gt;: mov %eax,%eax......0xffffffff8242fc92 &lt;compact_zone+1138&gt;: add %rax,%gs:0x7ddb566e(%rip) # 0x1e53080xffffffff8242fc9a &lt;compact_zone+1146&gt;: nopl 0x0(%rax,%rax,1)0xffffffff8242fc9f &lt;compact_zone+1151&gt;: mov %gs:0x7dbe18c2(%rip),%eax # 0x115680xffffffff8242fca6 &lt;compact_zone+1158&gt;: mov %eax,%eax0xffffffff8242fca8 &lt;compact_zone+1160&gt;: bt %rax,0x18b59a0(%rip) # 0xffffffff83ce56500xffffffff8242fcb0 &lt;compact_zone+1168&gt;: jae 0xffffffff8242fccd &lt;compact_zone+1197&gt;0xffffffff8242fcb2 &lt;compact_zone+1170&gt;: incl %gs:0x7dbe7247(%rip) # 0x16f000xffffffff8242fcb9 &lt;compact_zone+1177&gt;: mov 0x188ed40(%rip),%rax # 0xffffffff83cbea000xffffffff8242fcc0 &lt;compact_zone+1184&gt;: decl %gs:0x7dbe7239(%rip) # 0x16f000xffffffff8242fcc7 &lt;compact_zone+1191&gt;: je 0xffffffff8243063e &lt;compact_zone+3614&gt;0xffffffff8242fccd &lt;compact_zone+1197&gt;: mov 0x80(%rsp),%rax0xffffffff8242fcd5 &lt;compact_zone+1205&gt;: sub %gs:0x28,%rax0xffffffff8242fcde &lt;compact_zone+1214&gt;: jne 0xffffffff82430687 &lt;compact_zone+3687&gt;0xffffffff8242fce4 &lt;compact_zone+1220&gt;: add $0x88,%rsp0xffffffff8242fceb &lt;compact_zone+1227&gt;: mov %r12d,%eax0xffffffff8242fcee &lt;compact_zone+1230&gt;: pop %rbx0xffffffff8242fcef &lt;compact_zone+1231&gt;: pop %rbp0xffffffff8242fcf0 &lt;compact_zone+1232&gt;: pop %r120xffffffff8242fcf2 &lt;compact_zone+1234&gt;: pop %r130xffffffff8242fcf4 &lt;compact_zone+1236&gt;: pop %r140xffffffff8242fcf6 &lt;compact_zone+1238&gt;: pop %r15 接下来打开 trace_mm_compaction_begin 和 trace_mm_compaction_end 两个tracepoint 点，然后crash 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596970xffffffff8ce2f820 &lt;compact_zone&gt;: push %r150xffffffff8ce2f822 &lt;compact_zone+2&gt;: push %r140xffffffff8ce2f824 &lt;compact_zone+4&gt;: mov %rdi,%r140xffffffff8ce2f827 &lt;compact_zone+7&gt;: push %r130xffffffff8ce2f829 &lt;compact_zone+9&gt;: push %r120xffffffff8ce2f82b &lt;compact_zone+11&gt;: push %rbp0xffffffff8ce2f82c &lt;compact_zone+12&gt;: push %rbx0xffffffff8ce2f82d &lt;compact_zone+13&gt;: sub $0x88,%rsp0xffffffff8ce2f834 &lt;compact_zone+20&gt;: mov 0x40(%rdi),%rdi0xffffffff8ce2f838 &lt;compact_zone+24&gt;: mov 0x70(%r14),%ebp0xffffffff8ce2f83c &lt;compact_zone+28&gt;: mov %rsi,0x28(%rsp)0xffffffff8ce2f841 &lt;compact_zone+33&gt;: mov 0x80(%rdi),%rbx0xffffffff8ce2f848 &lt;compact_zone+40&gt;: mov %gs:0x28,%rax0xffffffff8ce2f851 &lt;compact_zone+49&gt;: mov %rax,0x80(%rsp)0xffffffff8ce2f859 &lt;compact_zone+57&gt;: xor %eax,%eax0xffffffff8ce2f85b &lt;compact_zone+59&gt;: mov 0x70(%rdi),%rax0xffffffff8ce2f85f &lt;compact_zone+63&gt;: mov %r14,(%r14)0xffffffff8ce2f862 &lt;compact_zone+66&gt;: mov 0x18b66d7(%rip),%r12d # 0xffffffff8e6e5f400xffffffff8ce2f869 &lt;compact_zone+73&gt;: movq $0x0,0x48(%r14)0xffffffff8ce2f871 &lt;compact_zone+81&gt;: mov %rax,0x30(%rsp)0xffffffff8ce2f876 &lt;compact_zone+86&gt;: lea 0x10(%r14),%rax0xffffffff8ce2f87a &lt;compact_zone+90&gt;: mov %rax,0x20(%rsp)0xffffffff8ce2f87f &lt;compact_zone+95&gt;: mov %rax,0x10(%r14)0xffffffff8ce2f883 &lt;compact_zone+99&gt;: mov %rax,0x18(%r14)0xffffffff8ce2f887 &lt;compact_zone+103&gt;: mov 0x5c(%r14),%eax0xffffffff8ce2f88b &lt;compact_zone+107&gt;: movq $0x0,0x50(%r14)0xffffffff8ce2f893 &lt;compact_zone+115&gt;: movq $0x0,0x20(%r14)0xffffffff8ce2f89b &lt;compact_zone+123&gt;: mov %r14,0x8(%r14)0xffffffff8ce2f89f &lt;compact_zone+127&gt;: test %r12d,%r12d0xffffffff8ce2f8a2 &lt;compact_zone+130&gt;: jne 0xffffffff8ce3062d &lt;compact_zone+3597&gt;0xffffffff8ce2f8a8 &lt;compact_zone+136&gt;: shr $0x3,%eax0xffffffff8ce2f8ab &lt;compact_zone+139&gt;: and $0x3,%eax0xffffffff8ce2f8ae &lt;compact_zone+142&gt;: mov 0x68(%r14),%edx0xffffffff8ce2f8b2 &lt;compact_zone+146&gt;: mov 0x6c(%r14),%ecx0xffffffff8ce2f8b6 &lt;compact_zone+150&gt;: mov %eax,0x64(%r14)0xffffffff8ce2f8ba &lt;compact_zone+154&gt;: mov 0x60(%r14),%esi0xffffffff8ce2f8be &lt;compact_zone+158&gt;: callq 0xffffffff8ce2f710 &lt;compaction_suitable&gt;0xffffffff8ce2f8c3 &lt;compact_zone+163&gt;: cmp $0x8,%eax0xffffffff8ce2f8c6 &lt;compact_zone+166&gt;: mov %eax,%r12d0xffffffff8ce2f8c9 &lt;compact_zone+169&gt;: sete %dl0xffffffff8ce2f8cc &lt;compact_zone+172&gt;: cmp $0x1,%eax0xffffffff8ce2f8cf &lt;compact_zone+175&gt;: sete %al0xffffffff8ce2f8d2 &lt;compact_zone+178&gt;: or %al,%dl0xffffffff8ce2f8d4 &lt;compact_zone+180&gt;: jne 0xffffffff8ce2fccd &lt;compact_zone+1197&gt;0xffffffff8ce2f8da &lt;compact_zone+186&gt;: mov 0x40(%r14),%rdi0xffffffff8ce2f8de &lt;compact_zone+190&gt;: mov 0x478(%rdi),%eax0xffffffff8ce2f8e4 &lt;compact_zone+196&gt;: cmp %eax,0x60(%r14)0xffffffff8ce2f8e8 &lt;compact_zone+200&gt;: jl 0xffffffff8ce2f8f7 &lt;compact_zone+215&gt;0xffffffff8ce2f8ea &lt;compact_zone+202&gt;: cmpl $0x6,0x474(%rdi)0xffffffff8ce2f8f1 &lt;compact_zone+209&gt;: je 0xffffffff8ce3050d &lt;compact_zone+3309&gt;0xffffffff8ce2f8f7 &lt;compact_zone+215&gt;: mov 0x30(%rsp),%rdx0xffffffff8ce2f8fc &lt;compact_zone+220&gt;: mov %rbx,%rax0xffffffff8ce2f8ff &lt;compact_zone+223&gt;: xor %edi,%edi0xffffffff8ce2f901 &lt;compact_zone+225&gt;: movq $0x0,0x38(%r14)0xffffffff8ce2f909 &lt;compact_zone+233&gt;: add %rdx,%rax0xffffffff8ce2f90c &lt;compact_zone+236&gt;: test %ebp,%ebp0xffffffff8ce2f90e &lt;compact_zone+238&gt;: setne %dil0xffffffff8ce2f912 &lt;compact_zone+242&gt;: cmpb $0x0,0x79(%r14)0xffffffff8ce2f917 &lt;compact_zone+247&gt;: mov %rax,0x50(%rsp)0xffffffff8ce2f91c &lt;compact_zone+252&gt;: mov %edi,0x68(%rsp)0xffffffff8ce2f920 &lt;compact_zone+256&gt;: je 0xffffffff8ce2fcf9 &lt;compact_zone+1241&gt;0xffffffff8ce2f926 &lt;compact_zone+262&gt;: lea -0x1(%rax),%rcx0xffffffff8ce2f92a &lt;compact_zone+266&gt;: mov %rdx,0x30(%r14)0xffffffff8ce2f92e &lt;compact_zone+270&gt;: and $0xfffffffffffffe00,%rcx0xffffffff8ce2f935 &lt;compact_zone+277&gt;: mov %rcx,0x28(%r14)0xffffffff8ce2f939 &lt;compact_zone+281&gt;: movb $0x0,0x4b(%rsp)0xffffffff8ce2f93e &lt;compact_zone+286&gt;: test %ebp,%ebp0xffffffff8ce2f940 &lt;compact_zone+288&gt;: jne 0xffffffff8ce2f959 &lt;compact_zone+313&gt;0xffffffff8ce2f942 &lt;compact_zone+290&gt;: mov 0x40(%r14),%rax0xffffffff8ce2f946 &lt;compact_zone+294&gt;: mov 0x458(%rax),%rbx0xffffffff8ce2f94d &lt;compact_zone+301&gt;: cmp %rbx,0x450(%rax)0xffffffff8ce2f954 &lt;compact_zone+308&gt;: sete 0x4b(%rsp)0xffffffff8ce2f959 &lt;compact_zone+313&gt;: jmpq 0xffffffff8ce30375 &lt;compact_zone+2901&gt;0xffffffff8ce2f95e &lt;compact_zone+318&gt;: mov %gs:0x731e1c03(%rip),%eax # 0x115680xffffffff8ce2f965 &lt;compact_zone+325&gt;: mov %eax,%eax......0xffffffff8ce2fc92 &lt;compact_zone+1138&gt;: add %rax,%gs:0x733b566e(%rip) # 0x1e53080xffffffff8ce2fc9a &lt;compact_zone+1146&gt;: jmpq 0xffffffff8ce303ce &lt;compact_zone+2990&gt;0xffffffff8ce2fc9f &lt;compact_zone+1151&gt;: mov %gs:0x731e18c2(%rip),%eax # 0x115680xffffffff8ce2fca6 &lt;compact_zone+1158&gt;: mov %eax,%eax0xffffffff8ce2fca8 &lt;compact_zone+1160&gt;: bt %rax,0x18b59a0(%rip) # 0xffffffff8e6e56500xffffffff8ce2fcb0 &lt;compact_zone+1168&gt;: jae 0xffffffff8ce2fccd &lt;compact_zone+1197&gt;0xffffffff8ce2fcb2 &lt;compact_zone+1170&gt;: incl %gs:0x731e7247(%rip) # 0x16f000xffffffff8ce2fcb9 &lt;compact_zone+1177&gt;: mov 0x188ed40(%rip),%rax # 0xffffffff8e6bea000xffffffff8ce2fcc0 &lt;compact_zone+1184&gt;: decl %gs:0x731e7239(%rip) # 0x16f000xffffffff8ce2fcc7 &lt;compact_zone+1191&gt;: je 0xffffffff8ce3063e &lt;compact_zone+3614&gt;0xffffffff8ce2fccd &lt;compact_zone+1197&gt;: mov 0x80(%rsp),%rax0xffffffff8ce2fcd5 &lt;compact_zone+1205&gt;: sub %gs:0x28,%rax0xffffffff8ce2fcde &lt;compact_zone+1214&gt;: jne 0xffffffff8ce30687 &lt;compact_zone+3687&gt;0xffffffff8ce2fce4 &lt;compact_zone+1220&gt;: add $0x88,%rsp0xffffffff8ce2fceb &lt;compact_zone+1227&gt;: mov %r12d,%eax0xffffffff8ce2fcee &lt;compact_zone+1230&gt;: pop %rbx0xffffffff8ce2fcef &lt;compact_zone+1231&gt;: pop %rbp0xffffffff8ce2fcf0 &lt;compact_zone+1232&gt;: pop %r120xffffffff8ce2fcf2 &lt;compact_zone+1234&gt;: pop %r130xffffffff8ce2fcf4 &lt;compact_zone+1236&gt;: pop %r140xffffffff8ce2fcf6 &lt;compact_zone+1238&gt;: pop %r15 可以看出来两处不一样 120xffffffff8242f959 &lt;compact_zone+313&gt;: nopl 0x0(%rax,%rax,1)0xffffffff8242fc9a &lt;compact_zone+1146&gt;: nopl 0x0(%rax,%rax,1) 120xffffffff8ce2f959 &lt;compact_zone+313&gt;: jmpq 0xffffffff8ce30375 &lt;compact_zone+2901&gt;0xffffffff8ce2fc9a &lt;compact_zone+1146&gt;: jmpq 0xffffffff8ce303ce &lt;compact_zone+2990&gt; demo2总觉得用 tracepoint 来实现有些不好，还是手动写个demo 12345678910111213141516171819202122232425262728293031323334353637383940ubuntu@zeku_server:~/workspace/linux $ git diffdiff --git a/mm/compaction.c b/mm/compaction.cindex bfc93da1c2c7..13513d807f86 100644--- a/mm/compaction.c+++ b/mm/compaction.c@@ -2896,6 +2896,25 @@ void wakeup_kcompactd(pg_data_t *pgdat, int order, int highest_zoneidx) wake_up_interruptible(&amp;pgdat-&gt;kcompactd_wait); }+DEFINE_STATIC_KEY_TRUE(sh_true);+DEFINE_STATIC_KEY_TRUE(sh_false);++static noinline void check_compact_true(void)+{+ if (static_branch_likely(&amp;sh_true)) {+ pr_err(&quot;check_compact_true\\n&quot;);+ }++}++static noinline void check_compact_false(void)+{+ if (static_branch_unlikely(&amp;sh_false)) {+ pr_err(&quot;check_compact_false\\n&quot;);+ }++}+ /* * The background compaction daemon, started as a kernel thread * from the init process.@@ -2920,6 +2939,10 @@ static int kcompactd(void *p) while (!kthread_should_stop()) { unsigned long pflags;+ static_key_enable(&amp;sh_true.key);+ static_key_disable(&amp;sh_false.key);+ check_compact_true();+ check_compact_false(); /* crash 结果是 123456789101112131415crash&gt; lsdump.202109261702 vmlinuxcrash&gt; dis check_compact_true0xffffffff85e2ce00 &lt;check_compact_true&gt;: xchg %ax,%ax0xffffffff85e2ce02 &lt;check_compact_true+2&gt;: jmpq 0xffffffff86950654 &lt;check_compact_true.cold&gt;0xffffffff85e2ce07 &lt;check_compact_true+7&gt;: retq0xffffffff85e2ce08 &lt;check_compact_true+8&gt;: nopl 0x0(%rax,%rax,1)crash&gt; dis check_compact_false0xffffffff85e2ce10 &lt;check_compact_false&gt;: nopl 0x0(%rax,%rax,1) [FTRACE NOP]0xffffffff85e2ce15 &lt;check_compact_false+5&gt;: retq0xffffffff85e2ce16 &lt;check_compact_false+6&gt;: nopw %cs:0x0(%rax,%rax,1)crash&gt; dis check_compact_true.cold0xffffffff86950654 &lt;check_compact_true.cold&gt;: mov $0xffffffff871b19a4,%rdi0xffffffff8695065b &lt;check_compact_true.cold+7&gt;: jmpq 0xffffffff8694ba28 &lt;_printk&gt;crash&gt; 可以看到： check_compact_true 在被优化之后 只有一个 jmp 指令，没有任何类似于 test 的判断指令 check_compact_false 在被优化之后 只有一个 nopl 指令，没有任何类似于 test 的判断指令 这里需要加上 noinline 修饰符，否则编译器会自动 inline这个函数。 代码分析暂时还没完全看懂，看 demo2 算是 了解基本原理实现。后续补充。。 参考内核基础设施——static_key参考Linux Jump Label/static-key机制详解","link":"/2021/09/14/%E5%86%85%E6%A0%B8%E8%A7%82%E6%B5%8B/%E6%80%A7%E8%83%BD%E7%A8%B3%E5%AE%9A%E6%80%A7/static_key%E6%9C%BA%E5%88%B6/"},{"title":"pagemap","text":"简介pagemap 是 an array mapping virtual pages to pfns，就是一组mapping，是虚拟地址到pfn的一个mapping 映射，是相对于一个进程地址空间来说的。 123456789101112131415161718192021222324* /proc/pid/pagemap - an array mapping virtual pages to pfns** For each page in the address space, this file contains one 64-bit entry* consisting of the following:** Bits 0-54 page frame number (PFN) if present* Bits 0-4 swap type if swapped* Bits 5-54 swap offset if swapped* Bit 55 pte is soft-dirty (see Documentation/admin-guide/mm/soft-dirty.rst)* Bit 56 page exclusively mapped* Bits 57-60 zero* Bit 61 page is file-page or shared-anon* Bit 62 page swapped* Bit 63 page present** If the page is not present but in swap, then the PFN contains an* encoding of the swap file number and the page's offset into the* swap. Unmapped pages return a null PFN. This allows determining* precisely which pages are mapped (or in swap) and comparing mapped* pages between processes.** Efficient users of this interface will use /proc/pid/maps to* determine which areas of memory are actually mapped and llseek to* skip over unmapped regions. 这是 fs/proc/task_mmu.c 中 pagemap_read函数的注释。可以看到比较重要是 12* Bit 62 page swapped* Bit 63 page present code在 kernel 代码中搜索 /proc/self/pagemap 字符，可以看到在 tools/testing/selftests/vm/madv_populate.c目录中有较多使用了 pagemap这个文件。 在 test_modules 这个仓库中 可以看到相关使用 /proc/self/pagemap 的代码。 代码通过 mmap malloc calloc 之后得到的地址来判断这个地址是否映射了物理页帧，还可以获得映射后的物理页帧号，运行之后结果是 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110ubuntu@zeku_root:/home/ubuntu/workspace/share/test_modules/memory/pagemap # ./main[test_mmap]: addr = 0x8d6d5000start = 0x8d6d5000, entry = 0x0080000000000000[test_mmap]: addr: 0x8d6d5000 is not populatedstart = 0x8d6d6000, entry = 0x0080000000000000[test_mmap]: addr: 0x8d6d6000 is not populatedstart = 0x8d6d7000, entry = 0x0080000000000000[test_mmap]: addr: 0x8d6d7000 is not populatedstart = 0x8d6d8000, entry = 0x0080000000000000[test_mmap]: addr: 0x8d6d8000 is not populatedstart = 0x8d6d9000, entry = 0x0080000000000000[test_mmap]: addr: 0x8d6d9000 is not populatedstart = 0x8d6da000, entry = 0x0080000000000000[test_mmap]: addr: 0x8d6da000 is not populatedstart = 0x8d6db000, entry = 0x0080000000000000[test_mmap]: addr: 0x8d6db000 is not populatedstart = 0x8d6dc000, entry = 0x0080000000000000[test_mmap]: addr: 0x8d6dc000 is not populatedstart = 0x8d6dd000, entry = 0x0080000000000000[test_mmap]: addr: 0x8d6dd000 is not populatedstart = 0x8d6de000, entry = 0x0080000000000000[test_mmap]: addr: 0x8d6de000 is not populatedstart = 0x8d6df000, entry = 0x0080000000000000[test_mmap]: addr: 0x8d6df000 is not populatedstart = 0x8d6e0000, entry = 0x0080000000000000[test_mmap]: addr: 0x8d6e0000 is not populated[test_mmap]: memsetstart = 0x8d6d5000, entry = 0x81800000002c53d6start = 0x8d6d6000, entry = 0x818000000026072dstart = 0x8d6d7000, entry = 0x81800000002851f6start = 0x8d6d8000, entry = 0x81800000002e3a6dstart = 0x8d6d9000, entry = 0x818000000036297astart = 0x8d6da000, entry = 0x81800000003ff422start = 0x8d6db000, entry = 0x81800000001f0a10start = 0x8d6dc000, entry = 0x81800000002e3b2cstart = 0x8d6dd000, entry = 0x81800000003c8e7cstart = 0x8d6de000, entry = 0x8180000000347f7fstart = 0x8d6df000, entry = 0x8180000000293003start = 0x8d6e0000, entry = 0x81800000002a69ae[test_malloc]: addr = 0x863056b0start = 0x863056b0, entry = 0x81800000003f4906start = 0x863066b0, entry = 0x0080000000000000[test_malloc]: addr:0x863066b0 is not populatedstart = 0x863076b0, entry = 0x0080000000000000[test_malloc]: addr:0x863076b0 is not populatedstart = 0x863086b0, entry = 0x0080000000000000[test_malloc]: addr:0x863086b0 is not populatedstart = 0x863096b0, entry = 0x0080000000000000[test_malloc]: addr:0x863096b0 is not populatedstart = 0x8630a6b0, entry = 0x0080000000000000[test_malloc]: addr:0x8630a6b0 is not populatedstart = 0x8630b6b0, entry = 0x0080000000000000[test_malloc]: addr:0x8630b6b0 is not populatedstart = 0x8630c6b0, entry = 0x0080000000000000[test_malloc]: addr:0x8630c6b0 is not populatedstart = 0x8630d6b0, entry = 0x0080000000000000[test_malloc]: addr:0x8630d6b0 is not populatedstart = 0x8630e6b0, entry = 0x0080000000000000[test_malloc]: addr:0x8630e6b0 is not populatedstart = 0x8630f6b0, entry = 0x0080000000000000[test_malloc]: addr:0x8630f6b0 is not populatedstart = 0x863106b0, entry = 0x0080000000000000[test_malloc]: addr:0x863106b0 is not populated[test_malloc]: memset 1 / 3start = 0x863056b0, entry = 0x81800000003f4906start = 0x863066b0, entry = 0x81800000003ff7c3start = 0x863076b0, entry = 0x81800000003ff592start = 0x863086b0, entry = 0x8180000000325293start = 0x863096b0, entry = 0x8180000000215ef7start = 0x8630a6b0, entry = 0x0080000000000000[test_malloc]: addr:0x8630a6b0 is not populatedstart = 0x8630b6b0, entry = 0x0080000000000000[test_malloc]: addr:0x8630b6b0 is not populatedstart = 0x8630c6b0, entry = 0x0080000000000000[test_malloc]: addr:0x8630c6b0 is not populatedstart = 0x8630d6b0, entry = 0x0080000000000000[test_malloc]: addr:0x8630d6b0 is not populatedstart = 0x8630e6b0, entry = 0x0080000000000000[test_malloc]: addr:0x8630e6b0 is not populatedstart = 0x8630f6b0, entry = 0x0080000000000000[test_malloc]: addr:0x8630f6b0 is not populatedstart = 0x863106b0, entry = 0x0080000000000000[test_malloc]: addr:0x863106b0 is not populated[test_calloc]: addr = 0x863116c0start = 0x863116c0, entry = 0x818000000040bcb7start = 0x863126c0, entry = 0x8180000000293f9fstart = 0x863136c0, entry = 0x8180000000316b40start = 0x863146c0, entry = 0x81800000002e30ddstart = 0x863156c0, entry = 0x81800000002e2dcdstart = 0x863166c0, entry = 0x818000000030dd5cstart = 0x863176c0, entry = 0x81800000002352b5start = 0x863186c0, entry = 0x81800000001d4da4start = 0x863196c0, entry = 0x818000000033e5c1start = 0x8631a6c0, entry = 0x8180000000184f31start = 0x8631b6c0, entry = 0x818000000039d867start = 0x8631c6c0, entry = 0x81800000002e1a05[test_malloc]: memsetstart = 0x863116c0, entry = 0x818000000040bcb7start = 0x863126c0, entry = 0x8180000000293f9fstart = 0x863136c0, entry = 0x8180000000316b40start = 0x863146c0, entry = 0x81800000002e30ddstart = 0x863156c0, entry = 0x81800000002e2dcdstart = 0x863166c0, entry = 0x818000000030dd5cstart = 0x863176c0, entry = 0x81800000002352b5start = 0x863186c0, entry = 0x81800000001d4da4start = 0x863196c0, entry = 0x818000000033e5c1start = 0x8631a6c0, entry = 0x8180000000184f31start = 0x8631b6c0, entry = 0x818000000039d867start = 0x8631c6c0, entry = 0x81800000002e1a05ubuntu@zeku_root:/home/ubuntu/workspace/share/test_modules/memory/pagemap #","link":"/2021/02/04/memory/pagemap/"},{"title":"cpu调频","text":"cpufreq_update_util sugov_should_update_freq sugov_update_single_common sugov_update_shared trace_cpu_idle","link":"/2022/01/24/schedule/cpu%E8%B0%83%E9%A2%91/"},{"title":"eas","text":"energy_modelmi11 kernel 中 /sys/kernel/debug/energy_model 目录就是 energy_model，是根据opp table 和 dts中信息来的 123venus:/sys/kernel/debug/energy_model # lspd0 pd4 pd7venus:/sys/kernel/debug/energy_model # 是Qcom 888芯片，分为3 cluster，1+3+4架构 小核： 1 小核： 1 小核： 1","link":"/2022/01/05/schedule/eas/"},{"title":"","text":"以arm64为例：preempt_schedule_notracepreempt_schedule_irq","link":"/2022/01/20/schedule/%E5%86%85%E6%A0%B8%E6%8A%A2%E5%8D%A0%E8%B0%83%E5%BA%A6/"},{"title":"中断线程化导致的问题","text":"参考博客","link":"/2020/09/14/%E5%B9%B6%E5%8F%91%E9%97%AE%E9%A2%98/%E4%B8%AD%E6%96%AD%E7%BA%BF%E7%A8%8B%E5%8C%96%E5%AF%BC%E8%87%B4%E7%9A%84%E9%97%AE%E9%A2%98/"},{"title":"acl","text":"简介acl(access control list) 是通过 xattr 机制来实现的一种访问权限控制机制，原理unix系统中的rwx 权限控制机制在面对比较复杂的权限控制时不能满足需求，这就产生了acl。 acl 框架在 userspace中提供了 getfacl 和 setfacl 这样的命令来实现 acl的获取和设置。 ext4 acl123456struct posix_acl *ext4_get_acl(struct inode *inode, int type, bool rcu);intext4_set_acl(struct user_namespace *mnt_userns, struct inode *inode, struct posix_acl *acl, int type); 参考也说下Posix_ACL","link":"/2021/02/01/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/acl/"},{"title":"ext2文件系统","text":"可以直接运行此脚本，会报错 123456789101112131415161718192021222324#!/bin/bashtouch af() { let i=0 while true; do let i++ rm -f b strace -o /tmp/cp${BASHPID}.trace cp a b || break echo $i done}cleanup() { kill -9 %1 %2}f &amp;f &amp;trap cleanup exitwait 报错如下： 12345ubuntu@zeku_server:/tmp $ ./cp.sh1cp: 无法创建普通文件'b': 文件已存在23 参考cp:无法创建普通文件:文件已存在","link":"/2021/02/01/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/cp%20failed/"},{"title":"android12 gdb64调试进程","text":"背景在研究pixel6 双X1调度策略时，发现在render_thread 中最后会通过 reportworkduration来设置 task的uclamp_min。在游戏场景下，由于没有走安卓标准绘制流程，不存在 render_thread，所以肯定不是走的应用滑动场景的 SetUclamp 的代码路径 有两种方式确定，一是读代码，但是这种可能会有多个代码路径。并不能确定到底是具体哪一个代码路径；二是通过gdb调试，打断点，然后查看backtrace，这种方式可以确定实际代码路径，但是较为繁琐。 gdb 调试确定需要被 gdb 断点的进程可以发现 uclamp_min 值不为0的时候，pid为 552的 android.hardware.power-service.pixel-libperffmgr 进程输出的 adpf.2731-10090-d010-min val也是 210。 说明 此次我们需要trace的进程就是 552 设备之间的关系windows wsl android phone 三个设备 我们可以只用两个设备, android phone 是必须的， wsl windows phone 是随便选一个作为host 主机。我们选 wsl 作为host主机，来gdb调试 android phone。 正常gdb调试中，host主机 与 被调试代码都是在 同一个设备上，但是在 调试android phone中，host主机 和 被调试的android phone的代码不在一个设备上，gdb是通过 tcp通信的，所以需要在host主机中将设备的 gdbserver 端口号进行一次转发，这样才能可以debug。 debug步骤wsl 主机转发 android phone的端口可以直接使用 adb forward 来进行转发。 123ubuntu@wsl:~ $ adb forward tcp:23946 tcp:2394623946ubuntu@wsl:~ $ android phone gdbserver attach 被调试的程序正常直接 grep出 需要attach的进程的pid即可，然后使用 gdbserver64 进行attach如： 123456789raven:/ # ps -e | grep powersystem 735 1 10852056 5948 binder_wait_for_work 0 S android.hardware.power.stats-service.pixelroot 11621 2 0 0 worker_thread 0 I [kworker/u17:0-kbase_pm_poweroff_wait]root 11956 2 0 0 worker_thread 0 I [kworker/u17:1-kbase_pm_poweroff_wait]root 11973 1 10902368 5760 binder_wait_for_work 0 S android.hardware.power-service.pixel-libperfmgrraven:/ # gdbserver64 :23946 --attach 11973Attached; pid = 11973Listening on port 23946Remote debugging from host 127.0.0.1, port 57517 android设备上需要安装gdbserver/gdbserver64（调试arm64程序需要使用userdebug固件可能会自带gdbserver/gdbserver64程序，而user版android固件并没有把gdbserver编译到系统中，需要我们手动push。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455raven:/ # gdbserver64 --helpUsage: gdbserver [OPTIONS] COMM PROG [ARGS ...] gdbserver [OPTIONS] --attach COMM PID gdbserver [OPTIONS] --multi COMMCOMM may either be a tty device (for serial debugging),HOST:PORT to listen for a TCP connection, or '-' or 'stdio' to usestdin/stdout of gdbserver.PROG is the executable program. ARGS are arguments passed to inferior.PID is the process ID to attach to, when --attach is specified.Operating modes: --attach Attach to running process PID. --multi Start server without a specific program, and only quit when explicitly commanded. --once Exit after the first connection has closed. --help Print this message and then exit. --version Display version information and exit.Other options: --wrapper WRAPPER -- Run WRAPPER to start new programs. --disable-randomization Run PROG with address space randomization disabled. --no-disable-randomization Don't disable address space randomization when starting PROG. --startup-with-shell Start PROG using a shell. I.e., execs a shell that then execs PROG. (default) --no-startup-with-shell Exec PROG directly instead of using a shell. Disables argument globbing and variable substitution on UNIX-like systems.Debug options: --debug Enable general debugging output. --debug-format=opt1[,opt2,...] Specify extra content in debugging output. Options: all none timestamp --remote-debug Enable remote protocol debugging output. --disable-packet=opt1[,opt2,...] Disable support for RSP packets or features. Options: vCont, Tthread, qC, qfThreadInfo and threads (disable all threading packets).For more information, consult the GDB manual (available as on-lineinfo or a printed manual).Report bugs to &quot;&lt;http://www.gnu.org/software/gdb/bugs/&gt;&quot;. host 主机运行gdb 准备调试1234567891011121314151617181920212223242526ubuntu@wsl:/mnt/d/Users/50001309/AppData/Local/Android/Sdk/ndk/23.1.7779620/prebuilt/windows-x86_64/bin $ ./gdb.exeD:\\Users\\50001309\\AppData\\Local\\Android\\Sdk\\ndk\\23.1.7779620\\prebuilt\\windows-x86_64\\bin\\gdb-orig.exe: warning: Couldn't determine a path forthe index cache directory.GNU gdb (GDB) 8.3Copyright (C) 2019 Free Software Foundation, Inc.License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;This is free software: you are free to change and redistribute it.There is NO WARRANTY, to the extent permitted by law.Type &quot;show copying&quot; and &quot;show warranty&quot; for details.This GDB was configured as &quot;x86_64-w64-mingw32&quot;.Type &quot;show configuration&quot; for configuration details.--Type &lt;RET&gt; for more, q to quit, c to continue without paging--For bug reporting instructions, please see:&lt;http://www.gnu.org/software/gdb/bugs/&gt;.(gdb) file /mnt/d/Users/50001309/Downloads/debug/vendor/bin/hw/android.hardware.power-service.pixel-libperfmgr/mnt/d/Users/50001309/Downloads/debug/vendor/bin/hw/android.hardware.power-service.pixel-libperfmgr: No such file or directory.(gdb) target remote :23946Remote debugging using :23946Reading /vendor/bin/hw/android.hardware.power-service.pixel-libperfmgr from remote target...warning: File transfers from remote targets can be slow. Use &quot;set sysroot&quot; to access files locally instead.BFD: target:/vendor/bin/hw/android.hardware.power-service.pixel-libperfmgr: unknown type [0x13] section `.relr.dyn'Reading /vendor/bin/hw/android.hardware.power-service.pixel-libperfmgr from remote target...BFD: target:/vendor/bin/hw/android.hardware.power-service.pixel-libperfmgr: unknown type [0x13] section `.relr.dyn'Reading symbols from target:/vendor/bin/hw/android.hardware.power-service.pixel-libperfmgr... 然后直接 就是 b c info b 等常见的gdb调试的步骤。 参考使用gdb远程调试android native程序","link":"/2021/09/23/%E7%94%A8%E6%88%B7%E7%A9%BA%E9%97%B4/android12%20gdb64%E8%B0%83%E8%AF%95%E8%BF%9B%E7%A8%8B/"},{"title":"lock_stat","text":"介绍lock_stat 是一款测试查看内核锁状态的工具，可以看出刚刚锁的争用繁忙情况和到底哪个路径上争用最多，从而找到性能优化的思路。但是它会使内核的大小变大很多 使用使用之前需要使能CONFIG_LOCK_STATS，然后重新编译内核 12345678910CONFIG_LOCK_STAT:This feature enables tracking lock contention pointsFor more details, see Documentation/locking/lockstat.rstThis also enables lock events required by &quot;perf lock&quot;,subcommand of perf.If you want to use &quot;perf lock&quot;, you also need to turn onCONFIG_EVENT_TRACING.CONFIG_LOCK_STAT defines &quot;contended&quot; and &quot;acquired&quot; lock events.(CONFIG_LOCKDEP defines &quot;acquire&quot; and &quot;release&quot; events.) enable/disable 12# echo 1 &gt;/proc/sys/kernel/lock_stat# echo 0 &gt;/proc/sys/kernel/lock_stat enable之后，数据在 /proc/lock_stat输出 1234567891011root@rlk-Standard-PC-i440FX-PIIX-1996:/proc# head lock_statlock_stat version 0.4----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- class name con-bounces contentions waittime-min waittime-max waittime-total waittime-avg acq-bounces acquisitions holdtime-min holdtime-max holdtime-total holdtime-avg----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- &amp;dentry-&gt;d_lock: 6614 6650 0.06 23656.56 133605.67 20.09 282765 2391624 0.00 36452.61 529319.05 0.22 --------------- &amp;dentry-&gt;d_lock 1703 [&lt;0000000060216fb7&gt;] lockref_get_not_dead+0xe/0x30 &amp;dentry-&gt;d_lock 747 [&lt;00000000bcf55138&gt;] __d_lookup+0x76/0x160 &amp;dentry-&gt;d_lock 3256 [&lt;000000009f49056e&gt;] dput+0x146/0x380 格式解析看一份完整的log 123456789101112131415161718192021222324252627282930313233lock_stat version 0.4----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- class name con-bounces contentions waittime-min waittime-max waittime-total waittime-avg acq-bounces acquisitions holdtime-min holdtime-max holdtime-total holdtime-avg----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- &amp;dentry-&gt;d_lock: 6606 6642 0.06 23656.56 133603.59 20.11 242023 1893479 0.00 36452.61 427674.18 0.23 --------------- &amp;dentry-&gt;d_lock 1699 [&lt;0000000060216fb7&gt;] lockref_get_not_dead+0xe/0x30 &amp;dentry-&gt;d_lock 747 [&lt;00000000bcf55138&gt;] __d_lookup+0x76/0x160 &amp;dentry-&gt;d_lock 3254 [&lt;000000009f49056e&gt;] dput+0x146/0x380 &amp;dentry-&gt;d_lock 536 [&lt;000000003b543617&gt;] lockref_get+0x9/0x20 --------------- &amp;dentry-&gt;d_lock 1792 [&lt;0000000060216fb7&gt;] lockref_get_not_dead+0xe/0x30 &amp;dentry-&gt;d_lock 764 [&lt;00000000bcf55138&gt;] __d_lookup+0x76/0x160 &amp;dentry-&gt;d_lock 3082 [&lt;000000009f49056e&gt;] dput+0x146/0x380 &amp;dentry-&gt;d_lock 537 [&lt;000000003b543617&gt;] lockref_get+0x9/0x20............................................................................................................................................................................................................................. &amp;mm-&gt;mmap_lock#2-W: 729 1328 0.12 56312.54 243027.65 183.00 3411 138648 0.10 34454.21 1951568.44 14.08 &amp;mm-&gt;mmap_lock#2-R: 883 3858 0.13 11168.49 224472.15 58.18 4642 736764 0.08 79362.93 8514224.22 11.56 ------------------ &amp;mm-&gt;mmap_lock#2 3840 [&lt;00000000346dbc30&gt;] do_user_addr_fault+0x403/0x690 &amp;mm-&gt;mmap_lock#2 73 [&lt;000000007f3a53b5&gt;] mpol_rebind_mm+0x27/0xf0 &amp;mm-&gt;mmap_lock#2 13 [&lt;0000000004a222ed&gt;] __access_remote_vm+0x4d/0x350 &amp;mm-&gt;mmap_lock#2 378 [&lt;00000000e3ffe1c7&gt;] vm_mmap_pgoff+0xa9/0x180 ------------------ &amp;mm-&gt;mmap_lock#2 94 [&lt;000000007f3a53b5&gt;] mpol_rebind_mm+0x27/0xf0 &amp;mm-&gt;mmap_lock#2 73 [&lt;00000000542c73e5&gt;] dup_mm+0xd5/0x640 &amp;mm-&gt;mmap_lock#2 18 [&lt;0000000015e7bc9a&gt;] dup_mm+0x9c/0x640 &amp;mm-&gt;mmap_lock#2 1 [&lt;00000000d9e9fd4c&gt;] prctl_set_mm+0xfa/0x540............................................................................................................................................................................................................................. 可以看出争用的路径，尝试获取锁次数，获取但是需要等待的次数，等待的最大、最小、平均时间等，持有锁的最大、最小、平均时间。 1234567891011121314151617181920212223242526272829----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- class name con-bounces contentions waittime-min waittime-max waittime-total waittime-avg acq-bounces acquisitions holdtime-min holdtime-max holdtime-total holdtime-avg----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- &amp;sem-&gt;waiters: 0 0 0.00 0.00 0.00 0.00 36 618 0.09 1.69 140.32 0.23 callback_lock: 0 0 0.00 0.00 0.00 0.00 28 121 0.10 1.83 67.38 0.56 pmus_lock: 0 0 0.00 0.00 0.00 0.00 20 30 0.00 200.86 231.73 7.72 &amp;rsp-&gt;gp_wait: 0 0 0.00 0.00 0.00 0.00 79 1430 0.07 5.83 265.57 0.19 &amp;cpuset_rwsem-W: 0 0 0.00 0.00 0.00 0.00 0 618 0.64 76605.59 1370704.10 2217.97 &amp;cpuset_rwsem-R: 0 0 0.00 0.00 0.00 0.00 0 413 0.00 21.72 473.62 1.15 &quot;warn_unseeded_randomness&quot;.lock: 0 0 0.00 0.00 0.00 0.00 0 1 0.08 0.08 0.08 0.08 &amp;c-&gt;lock: 0 0 0.00 0.00 0.00 0.00 0 201310 0.00 457.69 38407.85 0.19 cgroup_idr_lock: 0 0 0.00 0.00 0.00 0.00 88 2148 0.09 11.15 939.40 0.44 pin_fs_lock: 0 0 0.00 0.00 0.00 0.00 35 11335 0.00 15.48 1171.10 0.10 tk_core.seq.seqcount-W: 0 0 0.00 0.00 0.00 0.00 0 10049 0.00 894.17 10020.79 1.00 tk_core.seq.seqcount-R: 0 0 0.00 0.00 0.00 0.00 0 1126135 0.00 4915.37 105326.06 0.09 cgroup_file_kn_lock: 0 0 0.00 0.00 0.00 0.00 197 1734 0.06 28.30 1893.58 1.09 cpufreq_governor_mutex: 0 0 0.00 0.00 0.00 0.00 0 6 0.00 0.32 0.95 0.16 &amp;sb-&gt;s_type-&gt;i_lock_key#4: 0 0 0.00 0.00 0.00 0.00 2 69 0.05 0.80 16.10 0.23 timekeeper_lock: 0 0 0.00 0.00 0.00 0.00 4065 10349 0.00 6268.82 26333.01 2.54 resource_lock-W: 0 0 0.00 0.00 0.00 0.00 1 357 0.08 3.13 77.04 0.22 resource_lock-R: 0 0 0.00 0.00 0.00 0.00 24 1601 0.08 29.63 682.58 0.43 &amp;type-&gt;s_umount_key#4/1: 0 0 0.00 0.00 0.00 0.00 0 1 6.78 6.78 6.78 6.78 drivers_lock: 0 0 0.00 0.00 0.00 0.00 2 4 0.09 0.44 0.99 0.25 pernet_ops_rwsem-W: 0 0 0.00 0.00 0.00 0.00 2 115 0.00 367.87 2408.12 20.94 pernet_ops_rwsem-R: 0 0 0.00 0.00 0.00 0.00 5 5 1129.92 83930.70 94433.84 18886.77 proc_subdir_lock-W: 0 0 0.00 0.00 0.00 0.00 4 586 0.00 29.05 195.94 0.33 proc_subdir_lock-R: 0 0 0.00 0.00 0.00 0.00 129 963 0.00 13.59 394.16 0.41 subsys mutex#2: 0 0 0.00 0.00 0.00 0.00 0 1 0.12 0.12 0.12 0.12 这里可以看出系统中最繁忙的锁的数据统计信息 参考使用 ftrace参考Lock Statistics","link":"/2021/01/16/%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86/lock_stat/"},{"title":"linux lockup问题","text":"介绍systrace是Android4.1版本之后推出的，对系统Performance分析的工具。systrace的功能包括跟踪系统的I/O操作、内核工作队列、CPU负载以及Android各个子系统的运行状况等。主要分为三个部分： 内核部分: 利用了Linux Kernel中的ftrace功能。如果要使用systrace的话，必须开启kernel中和ftrace相关的config 数据采集部分:Android定义了一个Trace类。应用程序可利用该类把统计信息输出给ftrace。同时，Android还有一个atrace程序，它可以从ftrace中读取统计信息然后交给数据分析工具来处理。 数据分析工具:Android提供一个systrace.py（python脚本文件，位于Android SDK目录/sdk/platform-tools/systrace中。 systrace 参数|options | 描述 || — | — ||-o &lt; FILE &gt; | 输出的目标文件 ||-t N, –time=N | 执行时间，默认5s ||-b N, –buf-size=N | buffer大小（单位kB),用于限制trace总大小，默认无上限 ||-k &lt; KFUNCS &gt;，–ktrace=&lt; KFUNCS &gt; | 追踪kernel函数，用逗号分隔 ||-a &lt; APP_NAME &gt;,–app=&lt; APP_NAME &gt; | 追踪应用包名，用逗号分隔 ||–from-file=&lt; FROM_FILE &gt; | 从文件中创建互动的systrace ||-l, –list-categories | 列举可用的tags | -l 的tags,可以标识想抓哪些 systrace 12345678910111213141516171819202122232425262728293031323334353637383940λ python systrace.py -l gfx - Graphics input - Input view - View System webview - WebView wm - Window Manager am - Activity Manager sm - Sync Manager audio - Audio video - Video camera - Camera hal - Hardware Modules res - Resource Loading dalvik - Dalvik VM rs - RenderScript bionic - Bionic C Library power - Power Management pm - Package Manager ss - System Server database - Database network - Network adb - ADB vibrator - Vibrator aidl - AIDL calls nnapi - NNAPI rro - Runtime Resource Overlay pdx - PDX services sched - CPU Scheduling freq - CPU Frequency idle - CPU Idle disk - Disk I/O sync - Synchronization memreclaim - Kernel Memory Reclaim binder_driver - Binder Kernel driver binder_lock - Binder global lock trace memory - Memory gfx - Graphics (HAL) ion - ION allocation (HAL)NOTE: more categories may be available with adb root sched: CPU调度的信息，非常重要；你能看到CPU在每个时间段在运行什么线程；线程调度情况，比如锁信息。gfx：Graphic系统的相关信息，包括SerfaceFlinger，VSYNC消息，Texture，RenderThread等；分析卡顿非常依赖这个。view: View绘制系统的相关信息，比如onMeasure，onLayout等；对分析卡顿比较有帮助。am：ActivityManager调用的相关信息；用来分析Activity的启动过程比较有效。dalvik: 虚拟机相关信息，比如GC停顿等。binder_driver: Binder驱动的相关信息，如果你怀疑是Binder IPC的问题，不妨打开这个。core_services: SystemServer中系统核心Service的相关信息，分析特定问题用。 步骤常用的命令行抓取systracepython D:\\Users\\50001309\\AppData\\Local\\Android\\Sdk\\platform-tools\\systrace\\systrace.py -o D:\\systrace\\outfile\\trace.html sched gfx view am core_services freq -t 10 -b 40960 抓取 systrace 之后，在 chrome://tracing/ 或者 https://ui.perfetto.dev/#!/ 地址上解析 快捷键 W: 放大 Systrace , 放大可以更好地看清局部细节 S: 缩小 Systrace, 缩小以查看整体 A: 左移 D: 右移 M: 高亮选中当前鼠标点击的段(这个比较常用,可以快速标识出这个方法的左右边界和执行时间,方便上下查看) 数字键1: 切换到 Selection 模式 , 这个模式下鼠标可以点击某一个段查看其详细信息, 一般打开 Systrace 默认就是这个模式 , 也是最常用的一个模式 , 配合M 和ASDW 可以做基本的操作 数字键2: 切换到 Pan 模式 , 这个模式下长按鼠标可以左右拖动, 有时候会用到 数字键3: 切换到 Zoom 模式 , 这个模式下长按鼠标可以放大和缩小, 有时候会用到 数字键4: 切换到 Timing 模式 , 这个模式下主要是用来衡量时间的,比如选择一个起点, 选择一个终点, 查看起点和终点这中间的操作所花费的时间. 查看当前前台进程的pid1234567raven:/ $ logcat -b events |grep on_resume01-21 17:10:45.959 2236 2236 I wm_on_resume_called: [105694312,com.android.settings.FallbackHome,RESUME_ACTIVITY]01-21 17:10:47.972 2318 2318 I wm_on_resume_called: [50407132,com.android.launcher3.uioverrides.QuickstepLauncher,RESUME_ACTIVITY]01-21 17:10:48.309 2318 2318 I wm_on_resume_called: [50407132,com.android.launcher3.uioverrides.QuickstepLauncher,RESUME_ACTIVITY]01-21 17:10:52.867 3699 3699 I wm_on_resume_called: [43073904,com.ss.android.ugc.aweme.splash.SplashActivity,RESUME_ACTIVITY]01-21 17:15:19.001 2318 2318 I wm_on_resume_called: [50407132,com.android.launcher3.uioverrides.QuickstepLauncher,RESUME_ACTIVITY]01-21 17:16:10.807 3699 3699 I wm_on_resume_called: [43073904,com.ss.android.ugc.aweme.splash.SplashActivity,RESUME_ACTIVITY] 平台迁移systrace 只是Google给安卓设备的开发的，那么是否可以使用到server上或者确认书linux设备上呢？实际上是可以的，只不过没有安卓的相关事件，只有kernel原生的一些事件 当然需要做一些工作， 1https://github.com/liulangrenaaa/systrace 可以直接使用 python systrace.py -t 10 -v -e &quot;sched,irq&quot;输出 trace.html 参考google官方systrace介绍参考Gracker的android performance参考苍耳叔叔的博客参考sunwengang blog参考Android Systrace如何抓取分析问题参考confluence","link":"/2020/09/12/%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/systrace/"},{"title":"process_madvise","text":"","link":"/2020/09/18/memory/%E9%9D%A2%E8%AF%95/process_madvise/"},{"title":"vip_thread","text":"介绍在手机场景中，总有些进程对用户体验是相对其他进程来说高很多的，比如UI线程等，一旦调度不及时，就会给用户造成手机很卡的感觉，所以我们必须识别处这样的vip进程，然后给予更高的待遇，这是几种可行方案： 识别出 vip thread，然后来设置为 rt调度策略 识别出 vip thread, 动态给予很小的nice值，使得运行更多 识别出 vip thread, 修改 vruntime 的值，使得运行更多 识别出 vip thread, 给 cfs 中也添加 类似 rt的硬优先级 方案一识别出 vip thread，然后来设置为 rt调度策略优点：可以快速得到运行缺点：可能会抢占系统中本身的rt进程；但是rt的 thread可以抢占，会导致后面加入的进程反而会强占之前的进程 方案二和三识别出 vip thread, 动态给予很小的nice值 or 修改 vruntime的值，使得运行更多优点：可以使得vip thread 运行的更久一点缺点：但是可能延迟较大，不能立刻投入运行 方案四识别出 vip thread, 给 cfs 中也添加 类似 rt的硬优先级优点：相对于方案一来说，不会抢占系统中原油的 rt 进程；相对于方案二来说，实时性会大大提高缺点：需要自己扩展 cfs调度类 方案实现vip_thread 方案主要分为 识别vip_thread vip_thread 数据结构怎么组织 vip_thread怎么调度 vip_thread 对资源的依赖 识别 vip_thread如何识别 vip_thread 是一切的开始，vip_thread 定义是和用户体验及其相关的进程，主要是UI线程，kernel 自身很难感知到，但是 framework层是可以感知到的，直接提供proc 节点出去给framework 来帮助识别 vip_thread vip_thread 数据结构怎么组织被识别成为vip_thread 的进程主要是 ui进程，在安卓中的调度策略都是cfs，所以将 vip_thread 的相关数据结构添加到 cfs_rq 中： 123456#ifdef CONFIG_HW_VIP_THREAD /*task list for vip thread*/ struct list_head vip_thread_list; int active_vip_balance; struct cpu_stop_work vip_balance_work;#endif 当前为了实现简单，将 vip_thread 以 链表数据结构链接到 vip_thread_list 中，active_vip_balance 表明此 rq 是否正在做 vip_thread 的 load_balance，vip_balance_work 是 load_balance 的 work结构。 还需要在 struct task_struct 中添加如下结构： 12345678#ifdef CONFIG_HW_VIP_THREAD int static_vip; int vip_depth; atomic64_t dynamic_vip; struct list_head vip_entry; u64 enqueue_time; u64 dynamic_vip_start;#endif vip_thread 怎么调度参考HW_P50_888_Code","link":"/2021/04/30/schedule/HW/vip_thread/"},{"title":"perf c2c","text":"介绍perf c2c 是 perf 的一个子工具, c2c 代表着 cache to cache。 1The perf c2c tool provides means for Shared Data C2C/HITM analysis. It allows you to track down the cacheline contentions. 可以用来分析 cache 伪共享的问题 使用查看系统 cache line size123456789101112131415161718ubuntu@zeku_server:~/workspace/share/test_modules $ cat /proc/cpuinfo | grep sizecache size : 12288 KBclflush size : 64address sizes : 39 bits physical, 48 bits virtualcache size : 12288 KBclflush size : 64address sizes : 39 bits physical, 48 bits virtual......cache size : 12288 KBclflush size : 64address sizes : 39 bits physical, 48 bits virtualcache size : 12288 KBclflush size : 64address sizes : 39 bits physical, 48 bits virtualcache size : 12288 KBclflush size : 64address sizes : 39 bits physical, 48 bits virtualubuntu@zeku_server:~/workspace/share/test_modules $ clflush size 就是系统中 cache line flush size的大小，其实就是 cache line size大小，单位是 byte。 更直接的方式是 /sys/devices/system/cpu/cpu1/cache 目录下 1234567ubuntu@zeku_server:/sys/devices/system/cpu/cpu1/cache $ cat index0/coherency_line_size64ubuntu@zeku_server:/sys/devices/system/cpu/cpu1/cache $ cat index0/typeDataubuntu@zeku_server:/sys/devices/system/cpu/cpu1/cache $ cat index0/size32Kubuntu@zeku_server:/sys/devices/system/cpu/cpu1/cache $ ### 参考C2C - False Sharing Detection in Linux Perf参考man perf c2c参考检测错误共享","link":"/2021/11/03/%E5%86%85%E6%A0%B8%E8%A7%82%E6%B5%8B/perf%E7%9B%B8%E5%85%B3/perf%20c2c/"},{"title":"kallsyms","text":"介绍kallsyms 是内核的一个机制，和内核符号表有关，和他有关的宏是 123456ubuntu@zeku_server:~/workspace/linux $ cat ./out/.config | grep CONFIG_KALLSYMSCONFIG_KALLSYMS=yCONFIG_KALLSYMS_ALL=yCONFIG_KALLSYMS_ABSOLUTE_PERCPU=yCONFIG_KALLSYMS_BASE_RELATIVE=yubuntu@zeku_server:~/workspace/linux $ 在 scripts/link-vmlinux.sh 脚本中加入 下面的patch, 重新编译内核 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465diff --git a/scripts/link-vmlinux.sh b/scripts/link-vmlinux.shindex d74cee5c4326..c85541cbdd9d 100755--- a/scripts/link-vmlinux.sh+++ b/scripts/link-vmlinux.sh@@ -309,6 +309,14 @@ cleanup() rm -f .vmlinux.d }+echo &quot;link-vmlinux.sh starting ......&quot;+echo &quot;xxxxx \\$KBUILD_VERBOSE = &quot;$KBUILD_VERBOSE+echo &quot;xxxxx \\$1 = &quot;$1+echo &quot;xxxxx \\$2 = &quot;$2+echo &quot;xxxxx \\$3 = &quot;$3+echo &quot;xxxxx \\${MAKE} = &quot;${MAKE}++# echo &quot;xxxxx \\$4 = &quot;$4 # Use &quot;make V=1&quot; to debug this script case &quot;${KBUILD_VERBOSE}&quot; in *1*)@@ -338,8 +346,11 @@ fi; ${MAKE} -f &quot;${srctree}/scripts/Makefile.build&quot; obj=init need-builtin=1 #link vmlinux.o+echo &quot;xxxxx modpost_link start......&quot; modpost_link vmlinux.o+echo &quot;xxxxx objtool_link start...... &quot; objtool_link vmlinux.o+echo &quot;xxxxx objtool_link end......&quot; # modpost vmlinux.o to check for section mismatches ${MAKE} -f &quot;${srctree}/scripts/Makefile.modpost&quot; MODPOST_VMLINUX=1 @@ -369,14 +380,14 @@ if [ -n &quot;${CONFIG_KALLSYMS}&quot; ]; then # kallsyms support # Generate section listing all symbols and add it into vmlinux # It's a three step process:- # 1) Link .tmp_vmlinux1 so it has all symbols and sections,+ # 1) Link .tmp_vmlinux.kallsyms1 so it has all symbols and sections, # but __kallsyms is empty. # Running kallsyms on that gives us .tmp_kallsyms1.o with # the right size- # 2) Link .tmp_vmlinux2 so it now has a __kallsyms section of+ # 2) Link .tmp_vmlinux.kallsyms2 so it now has a __kallsyms section of # the right size, but due to the added section, some # addresses have shifted.- # From here, we generate a correct .tmp_kallsyms2.o+ # From here, we generate a correct .tmp_vmlinux.kallsyms2.o # 3) That link may have expanded the kernel image enough that # more linker branch stubs / trampolines had to be added, which # introduces new names, which further expands kallsyms. Do another@@ -389,7 +400,9 @@ if [ -n &quot;${CONFIG_KALLSYMS}&quot; ]; then # a) Verify that the System.map from vmlinux matches the map from # ${kallsymso}.+ echo &quot;xxxxx kallsyms_step 1......&quot; kallsyms_step 1+ echo &quot;xxxxx kallsyms_step 2......&quot; kallsyms_step 2 # step 3@@ -433,3 +446,5 @@ fi # For fixdep echo &quot;vmlinux: $0&quot; &gt; .vmlinux.d++echo &quot;link-vmlinux.sh ending ......&quot; 在编译阶段可以看到： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253make[1]: 进入目录“/home/ubuntu/workspace/linux/out” GEN Makefile DESCEND objtool CALL ../scripts/atomic/check-atomics.sh CALL ../scripts/checksyscalls.sh CHK include/generated/compile.hlink-vmlinux.sh starting ......xxxxx $KBUILD_VERBOSE = 0xxxxx $1 = ldxxxxx $2 = -m elf_x86_64xxxxx $3 = --emit-relocs --discard-none -z max-page-size=0x200000 --build-id=sha1 --orphan-handling=warnxxxxx ${MAKE} = make GEN .version CHK include/generated/compile.h UPD include/generated/compile.h CC init/version.o AR init/built-in.axxxxx modpost_link start...... LD vmlinux.oxxxxx objtool_link start......xxxxx objtool_link end...... MODPOST vmlinux.symvers MODINFO modules.builtin.modinfo GEN modules.builtinxxxxx kallsyms_step 1...... LD .tmp_vmlinux.kallsyms1 KSYMS .tmp_vmlinux.kallsyms1.S AS .tmp_vmlinux.kallsyms1.Sxxxxx kallsyms_step 2...... LD .tmp_vmlinux.kallsyms2 KSYMS .tmp_vmlinux.kallsyms2.S AS .tmp_vmlinux.kallsyms2.S LD vmlinux SORTTAB vmlinux SYSMAP System.maplink-vmlinux.sh ending ...... CC arch/x86/boot/version.o VOFFSET arch/x86/boot/compressed/../voffset.h OBJCOPY arch/x86/boot/compressed/vmlinux.bin RELOCS arch/x86/boot/compressed/vmlinux.relocs CC arch/x86/boot/compressed/kaslr.o CC arch/x86/boot/compressed/misc.o GZIP arch/x86/boot/compressed/vmlinux.bin.gz MKPIGGY arch/x86/boot/compressed/piggy.S AS arch/x86/boot/compressed/piggy.o LD arch/x86/boot/compressed/vmlinux ZOFFSET arch/x86/boot/zoffset.h OBJCOPY arch/x86/boot/vmlinux.bin AS arch/x86/boot/header.o LD arch/x86/boot/setup.elf OBJCOPY arch/x86/boot/setup.bin BUILD arch/x86/boot/bzImageKernel: arch/x86/boot/bzImage is ready (#136) 代码分析link_vmlinux.sh 脚本分析System.map 生成解析link-vmlinux.sh 中可以看到 System.map 生成过程： 1234567891011# Create map file with all symbols from ${1}# See mksymap for additional detailsmksysmap(){ ${CONFIG_SHELL} &quot;${srctree}/scripts/mksysmap&quot; ${1} ${2}}......info SYSMAP System.mapmksysmap vmlinux System.map 主要是用 scripts/mksysmap vmlinux System.map 命令生成的 scripts/mksysmap 脚本内容： 1234567891011121314151617181920212223242526272829303132333435363738394041424344#!/bin/sh -x# Based on the vmlinux file create the System.map file# System.map is used by module-init tools and some debugging# tools to retrieve the actual addresses of symbols in the kernel.## Usage# mksysmap vmlinux System.map###### Generate System.map (actual filename passed as second argument)# $NM produces the following output:# f0081e80 T alloc_vfsmnt# The second row specify the type of the symbol:# A = Absolute# B = Uninitialised data (.bss)# C = Common symbol# D = Initialised data# G = Initialised data for small objects# I = Indirect reference to another symbol# N = Debugging symbol# R = Read only# S = Uninitialised data for small objects# T = Text code symbol# U = Undefined symbol# V = Weak symbol# W = Weak symbol# Corresponding small letters are local symbols# For System.map filter away:# a - local absolute symbols# U - undefined global symbols# N - debugging symbols# w - local weak symbols# readprofile starts reading symbols when _stext is found, and# continue until it finds a symbol which is not either of 'T', 't',# 'W' or 'w'. __crc_ are 'A' and placed in the middle# so we just ignore them to let readprofile continue to work.# (At least sparc64 has __crc_ in the middle).# NM=gcc-nm$NM -n $1 | grep -v '\\( [aNUw] \\)\\|\\(__crc_\\)\\|\\( \\$[adt]\\)\\|\\( \\.L\\)' &gt; $2 实际最后生成 System.map 的命令是： 1gcc-nm -n vmlinux | grep -v '\\( [aNUw] \\)\\|\\(__crc_\\)\\|\\( \\$[adt]\\)\\|\\( \\.L\\)' &gt; System.map kernel/kallsyms.c 模块分析","link":"/2021/01/07/%E5%86%85%E6%A0%B8%E8%A7%82%E6%B5%8B/tools/kallsyms/"},{"title":"reboot 流程","text":"简介我们将 linux power management 分为两大类， Generic PM Runtime PM。 Generic PM: halt restart poweroff 对 Generic PM 指的是 传统 粗放的、静态的、被动的 linux 内核电源管理机制。","link":"/2020/09/12/power/reboot%20%E6%B5%81%E7%A8%8B/"},{"title":"gdb 使用","text":"简介GDB是一个由GNU开源组织发布的、UNIX/LINUX操作系统下的、基于命令行的、功能强大的程序调试工具。不仅仅可以用来调试 应用程序，也可以用来调试 linux kernel。 tips记录几个 gdb 重要的技巧 gdb 启动初始化用户可以设置 启动gdb 时加载个性化配置，类似于 .bashrc or .vimrc那样gdb初始化文件 位于 ~/.gdbinit 下常用的 ~/.gdbinit 内容 123456789101112131415 # 保存历史命令set history filename ~/.gdb_historyset history save on# 退出时不显示提示信息set confirm off# 按照派生类型打印对象set print object on# 打印数组的索引下标set print array-indexes on# 每行打印一个结构体成员set print pretty on 保存gdb 日志有时候debug现场很难复现，就需要将gdb debug的现场保留下来，后续查看 开启 gdb 日志 1set logging on 设置保存gdb日志的文件 1set logging file log.txt gdb 执行shell 命令在 gdb 窗口中，可以直接 shell cmd 来执行 shell 命令 gdb 不显示启动信息 1gdb -q gdb 不显示退出信息 1set confirm off gdb 输出多余一屏幕信息时不会暂停输出 1set height 0 只是退出当前执行的函数 fin 8. vscode + qemu + gdb + task.json + launch.json 调试 linux kernel task.json 配置123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169{ // See https://go.microsoft.com/fwlink/?LinkId=733558 // for the documentation about the tasks.json format &quot;version&quot;: &quot;2.0.0&quot;, &quot;tasks&quot;: [ { &quot;label&quot;: &quot;start-biscuitos&quot;, &quot;type&quot;: &quot;shell&quot;, &quot;command&quot;: &quot;/BiscuitOS/output/linux-5.10-aarch/RunBiscuitOS.sh&quot;, &quot;presentation&quot;: { &quot;echo&quot;: true, &quot;clear&quot;: true, &quot;group&quot;: &quot;vm&quot; }, &quot;problemMatcher&quot;: [] }, { &quot;label&quot;: &quot;debug-biscuitos&quot;, &quot;type&quot;: &quot;shell&quot;, &quot;command&quot;: &quot;/BiscuitOS/output/linux-5.10-aarch/RunBiscuitOS.sh debug&quot;, &quot;presentation&quot;: { &quot;echo&quot;: true, &quot;clear&quot;: true, &quot;group&quot;: &quot;vm&quot; }, &quot;problemMatcher&quot;: [] }, { &quot;label&quot;: &quot;start-arm64-ubuntu&quot;, &quot;type&quot;: &quot;shell&quot;, &quot;command&quot;: &quot; sudo /BiscuitOS/output/linux-5.10-aarch/qemu-system/qemu-4.0.0/aarch64-softmmu/qemu-system-aarch64 -hda /root/myspace/qemu_build/ubuntu/aarch64_ubuntu.img -kernel /root/workspace/linux-5.10/out/arch/arm64/boot/Image -append 'console=ttyAMA0 root=/dev/vda1' -m 2048M -smp 8 -M virt -cpu cortex-a72 -net nic -net user,hostfwd=tcp::2222-:22 -nographic -fsdev local,id=fs1,path=/root/workspace/share,security_model=none -device virtio-9p-pci,fsdev=fs1,mount_tag=host_share&quot;, &quot;presentation&quot;: { &quot;echo&quot;: true, &quot;clear&quot;: true, &quot;group&quot;: &quot;vm&quot; }, &quot;problemMatcher&quot;: [] }, { &quot;label&quot;: &quot;vm-stop&quot;, &quot;type&quot;: &quot;shell&quot;, &quot;command&quot;: &quot;sudo pkill qemu ; echo 'qemu stoped'&quot;, &quot;presentation&quot;: { &quot;echo&quot;: true, &quot;clear&quot;: true, &quot;group&quot;: &quot;vm_stop&quot; } }, { &quot;label&quot;: &quot;make&quot;, &quot;type&quot;: &quot;shell&quot;, &quot;command&quot;: &quot;make -j12 ARCH=arm64 CROSS_COMPILE=/usr/bin/aarch64-linux-gnu- Image O=./out/ &amp;&amp; echo '****************make is ready**************'&quot;, &quot;group&quot;: { &quot;kind&quot;: &quot;build&quot;, &quot;isDefault&quot;: true }, &quot;presentation&quot;: { &quot;echo&quot;: false, &quot;group&quot;: &quot;build&quot; } }, { &quot;label&quot;: &quot;make clean&quot;, &quot;type&quot;: &quot;shell&quot;, &quot;command&quot;: &quot;make mrproper ARCH=arm64&quot;, &quot;group&quot;: { &quot;kind&quot;: &quot;build&quot;, &quot;isDefault&quot;: true }, &quot;presentation&quot;: { &quot;echo&quot;: false, &quot;group&quot;: &quot;build&quot; } }, { &quot;label&quot;: &quot;gen label&quot;, &quot;type&quot;: &quot;shell&quot;, &quot;command&quot;: &quot;python ./scripts/clang-tools/gen_compile_commands.py -d ./out/&quot;, &quot;group&quot;: { &quot;kind&quot;: &quot;build&quot;, &quot;isDefault&quot;: true }, &quot;presentation&quot;: { &quot;echo&quot;: false, &quot;group&quot;: &quot;label&quot; } }, { &quot;label&quot;: &quot;remove_point&quot;, &quot;type&quot;: &quot;shell&quot;, &quot;command&quot;: &quot;rm /tmp/ssh_point&quot;, &quot;presentation&quot;: { &quot;echo&quot;: false, &quot;group&quot;: &quot;point&quot; } }, { &quot;label&quot;: &quot;touch_point&quot;, &quot;type&quot;: &quot;shell&quot;, &quot;command&quot;: &quot;touch /tmp/ssh_point&quot;, &quot;presentation&quot;: { &quot;echo&quot;: false, &quot;group&quot;: &quot;point&quot; } }, { &quot;label&quot;: &quot;detect_point&quot;, &quot;type&quot;: &quot;shell&quot;, &quot;command&quot;: &quot;sleep 3 &amp;&amp; for i in $(seq 1 400); do if [ ! -f /tmp/ssh_point ];then echo making &amp;&amp; sleep 2; fi; done&quot;, &quot;presentation&quot;: { &quot;echo&quot;: false, &quot;group&quot;: &quot;point&quot; } }, { &quot;label&quot;: &quot;ssh_vma&quot;, &quot;type&quot;: &quot;shell&quot;, &quot;command&quot;: &quot;for i in $(seq 1 5); do echo '******sshing******' &amp;&amp; if [ `ps -a | grep qemu | wc -l` -eq 1 ];then ssh ubuntu@127.0.0.1 -p 2222; fi ; done&quot;, &quot;presentation&quot;: { &quot;echo&quot;: true, &quot;clear&quot;: true, &quot;group&quot;: &quot;ssh&quot; }, &quot;problemMatcher&quot;: [], &quot;dependsOrder&quot;: &quot;sequence&quot;, &quot;dependsOn&quot;: [ &quot;detect_point&quot; ] }, { &quot;label&quot;: &quot;RUN-BiscuitOS&quot;, &quot;dependsOrder&quot;: &quot;sequence&quot;, &quot;dependsOn&quot;: [ &quot;vm-stop&quot;, &quot;make&quot;, &quot;start-biscuitos&quot; ] }, { &quot;label&quot;: &quot;DEBUG-BiscuitOS&quot;, &quot;dependsOrder&quot;: &quot;sequence&quot;, &quot;dependsOn&quot;: [ &quot;vm-stop&quot;, &quot;make&quot;, &quot;debug-biscuitos&quot; ], &quot;problemMatcher&quot;: [] }, { &quot;label&quot;: &quot;RUN-arm64&quot;, &quot;dependsOrder&quot;: &quot;sequence&quot;, &quot;dependsOn&quot;: [ &quot;remove_point&quot;, &quot;vm-stop&quot;, &quot;make&quot;, &quot;touch_point&quot;, &quot;start-arm64-ubuntu&quot; ] }, { &quot;label&quot;: &quot;RUN-ARM64-SSH&quot;, &quot;dependsOn&quot;: [ &quot;RUN-arm64&quot;, &quot;ssh_vma&quot; ], &quot;problemMatcher&quot;: [] } ]} launch.json 配置1234567891011121314151617181920212223242526{ // Use IntelliSense to learn about possible attributes. // Hover to view descriptions of existing attributes. // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387 &quot;version&quot;: &quot;0.2.0&quot;, &quot;configurations&quot;: [ { &quot;name&quot;: &quot;kernel-debug&quot;, &quot;type&quot;: &quot;cppdbg&quot;, &quot;request&quot;: &quot;launch&quot;, &quot;miDebuggerServerAddress&quot;: &quot;127.0.0.1:1234&quot;, &quot;miDebuggerPath&quot;: &quot;/usr/bin/gdb-multiarch&quot;, &quot;program&quot;: &quot;${workspaceFolder}/out/vmlinux&quot;, &quot;args&quot;: [], &quot;stopAtEntry&quot;: false, &quot;cwd&quot;: &quot;${workspaceFolder}&quot;, &quot;environment&quot;: [], &quot;externalConsole&quot;: true, &quot;hardwareBreakpoints&quot;: {}, &quot;logging&quot;: { &quot;engineLogging&quot;: false }, &quot;MIMode&quot;: &quot;gdb&quot;, } ]} 上面是 vscode 两个 json文件的配置，配置之后可以很快的进行debug。 我是用的 RunBiscuitOS 的环境，可以使用他的 docker 来进行构建，中间遇到过几个权限问题（具体什么问题忘了。。不过都好解决，以后配置机会不多，一个环境应该可以长期用下去），但是用物理机环境搞我是遇到过很多问题，建议用 docker搞。 debug 启动时用的 qemu, 在 docker 环境中直接使用 /BiscuitOS/output/linux-5.10-aarch/RunBiscuitOS.sh debug 即可，也可以使用qemu去启动tools 更加全面的 ubuntu image。 vscode debug 过程中的有不一样的是 左边 堆栈可以看到每个 cpu core 的堆栈，很方便了解各个cpu执行流程 监视视图： 可以输入 fair_tmp_i &gt; 10 等这种 表达式 关于断点: 可以在每一行最左边右击鼠标 选择 添加断点 or 添加条件断点 or 添加记录点 在 debug console 中，执行所有命令都是报错-var-create: unable to create variable object，因为是设计使然（by design）。要是想在 调试控制台上直接输入命令，需要 加一个前缀 -exec cmd for example 1234567891011121314151617181920212223watch fair_tmp_i &gt; 1350-var-create: unable to create variable objectwatch fair_tmp_i &gt; 1350-var-create: unable to create variable objectexec watch fair_tmp_i &gt; 1350-var-create: unable to create variable object-exec watch fair_tmp_i &gt; 1350Hardware watchpoint 12: fair_tmp_i &gt; 1350r-var-create: unable to create variable object-exec rThe &quot;remote&quot; target does not support &quot;run&quot;. Try &quot;help target&quot; or &quot;continue&quot;.-exec cContinuing.Thread 2 hit Hardware watchpoint 12: fair_tmp_i &gt; 1350Old value = 0New value = 1select_task_rq_fair (p=0xffff000003076040, prev_cpu=1, sd_flag=8, wake_flags=0) at ../kernel/sched/fair.c:67396739 if (sd_flag &amp; SD_BALANCE_WAKE) { 参考 b站视频1参考 b站视频2参考 100个gdb小技巧参考 b站视频","link":"/2022/03/28/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/gdb%20%E4%BD%BF%E7%94%A8/"},{"title":"KASAN 原理分析","text":"原理granule size == 8tags num == 2^8 == 256 代码实现分析tag 有哪些，怎么产生的kernel 认为 0xFF 的tag 都是有效的，将不可访问的内存都标记为 0xFE，认为是无效的。在不使能 CONFIG_KASAN_HW_TAGS 特性下，最小的 TAG是 0x00；使能情况下（只有aarch64支持），最小的 TAG是 0xF0。 123456789#define KASAN_TAG_KERNEL 0xFF /* native kernel pointers tag */#define KASAN_TAG_INVALID 0xFE /* inaccessible memory tag */#define KASAN_TAG_MAX 0xFD /* maximum value for random tags */#ifdef CONFIG_KASAN_HW_TAGS#define KASAN_TAG_MIN 0xF0 /* minimum value for random tags */#else#define KASAN_TAG_MIN 0x00 /* minimum value for random tags */#endif 通过 kasan_random_tag 可以随机产生一个tag val 123456789u8 kasan_random_tag(void){ u32 state = this_cpu_read(prng_state); state = 1664525 * state + 1013904223; this_cpu_write(prng_state, state); return (u8)(state % (KASAN_TAG_MAX + 1));} kasan hook点做了啥kasan_kmalloc kasan_kfree MTEMTE 是一种内存错误检测工具，或者至少是硬件加速工具。第一个实现类似功能的硬件架构 是 SPARCE， HWKASAN参考KASAN 原理解释 参考MTE HWKASAN 原理解释 参考MTE 是什么","link":"/2021/01/27/%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86/KASAN%20%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90/"},{"title":"futex","text":"futex 简介futex (fast userspace mutex) 是Linux的一个基础组件，可以用来构建各种更高级别的同步机制，比如锁或者信号量等等，POSIX信号量就是基于futex构建的。大多数时候编写应用程序并不需要直接使用futex，一般用基于它所实现的系统库就够了。 传统的SystemV IPC(inter process communication)进程间同步机制都是通过内核对象来实现的，以 semaphore 为例，当进程间要同步的时候，必须通过系统调用semop(2)进入内核进行PV操作。系统调用的缺点是开销很大，需要从user mode切换到kernel mode、保存寄存器状态、从user stack切换到kernel stack、等等，通常要消耗上百条指令。事实上，有一部分系统调用是可以避免的，因为现实中很多同步操作进行的时候根本不存在竞争，即某个进程从持有semaphore直至释放semaphore的这段时间内，常常没有其它进程对同一semaphore有需求，在这种情况下，内核的参与本来是不必要的，可是在传统机制下，持有semaphore必须先调用semop(2)进入内核去看看有没有人和它竞争，释放semaphore也必须调用semop(2)进入内核去看看有没有人在等待同一semaphore，这些不必要的系统调用造成了大量的性能损耗。 futex的解决思路是：在无竞争的情况下操作完全在user space进行，不需要系统调用，仅在发生竞争的时候进入内核去完成相应的处理(wait 或者 wake up)。所以说，futex是一种user mode和kernel mode混合的同步机制，需要两种模式合作才能完成，futex本质就是一个位于user space的变量，而不是内核对象，futex的代码也分为user mode和kernel mode两部分，无竞争的情况下在user mode，发生竞争时则通过sys_futex系统调用进入kernel mode进行处理。但是内核也为了 futex更加健壮，给futex机制增加了好几个syscall: 1234567891011121314SYSCALL_DEFINE2(set_robust_list, struct robust_list_head __user *, head, size_t, len);SYSCALL_DEFINE3(get_robust_list, int, pid, struct robust_list_head __user * __user *, head_ptr, size_t __user *, len_ptr);SYSCALL_DEFINE6(futex, u32 __user *, uaddr, int, op, u32, val, const struct __kernel_timespec __user *, utime, u32 __user *, uaddr2, u32, val3);SYSCALL_DEFINE5(futex_waitv, struct futex_waitv __user *, waiters, unsigned int, nr_futexes, unsigned int, flags, struct __kernel_timespec __user *, timeout, clockid_t, clockid); 主要优点是： futex 自身只是userspace的一个变量。 减少了系统调用，只有竞争的情况下回发生系统调用，无竞争的情况下都是用户空间的操作 避免了不必要的上下文切换（导致的TLB失效等）。 futex userspace 使用案列在 test_modules 中的 futex_demo 案列。 POSIX sema mutex 实现内核部分实现参考posix mutex userspace 锁实现参考posix mutex kernelspace 锁实现参考轻量级的 PI-futex参考健壮的 robust futex","link":"/2021/01/27/%E9%94%81%E6%9C%BA%E5%88%B6/futex/"}],"tags":[{"name":"docker","slug":"docker","link":"/tags/docker/"},{"name":"cgroup","slug":"cgroup","link":"/tags/cgroup/"},{"name":"x86","slug":"x86","link":"/tags/x86/"},{"name":"通用寄存器","slug":"通用寄存器","link":"/tags/%E9%80%9A%E7%94%A8%E5%AF%84%E5%AD%98%E5%99%A8/"},{"name":"栈","slug":"栈","link":"/tags/%E6%A0%88/"},{"name":"函数调用","slug":"函数调用","link":"/tags/%E5%87%BD%E6%95%B0%E8%B0%83%E7%94%A8/"},{"name":"oops","slug":"oops","link":"/tags/oops/"},{"name":"panic","slug":"panic","link":"/tags/panic/"},{"name":"空指针","slug":"空指针","link":"/tags/%E7%A9%BA%E6%8C%87%E9%92%88/"},{"name":"文件系统","slug":"文件系统","link":"/tags/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/"},{"name":"page_cache","slug":"page-cache","link":"/tags/page-cache/"},{"name":"qemu","slug":"qemu","link":"/tags/qemu/"},{"name":"ubuntu","slug":"ubuntu","link":"/tags/ubuntu/"},{"name":"kvm","slug":"kvm","link":"/tags/kvm/"},{"name":"服务器","slug":"服务器","link":"/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/"},{"name":"内核同步","slug":"内核同步","link":"/tags/%E5%86%85%E6%A0%B8%E5%90%8C%E6%AD%A5/"},{"name":"锁粒度","slug":"锁粒度","link":"/tags/%E9%94%81%E7%B2%92%E5%BA%A6/"},{"name":"per-cpu","slug":"per-cpu","link":"/tags/per-cpu/"},{"name":"软中断","slug":"软中断","link":"/tags/%E8%BD%AF%E4%B8%AD%E6%96%AD/"},{"name":"tasklet","slug":"tasklet","link":"/tags/tasklet/"},{"name":"ksoftirqd","slug":"ksoftirqd","link":"/tags/ksoftirqd/"},{"name":"死锁检测","slug":"死锁检测","link":"/tags/%E6%AD%BB%E9%94%81%E6%A3%80%E6%B5%8B/"},{"name":"kdump","slug":"kdump","link":"/tags/kdump/"},{"name":"crash","slug":"crash","link":"/tags/crash/"},{"name":"makedumpfile","slug":"makedumpfile","link":"/tags/makedumpfile/"},{"name":"开发工具","slug":"开发工具","link":"/tags/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/"},{"name":"shell","slug":"shell","link":"/tags/shell/"},{"name":"生活感悟","slug":"生活感悟","link":"/tags/%E7%94%9F%E6%B4%BB%E6%84%9F%E6%82%9F/"},{"name":"杂项","slug":"杂项","link":"/tags/%E6%9D%82%E9%A1%B9/"},{"name":"read code","slug":"read-code","link":"/tags/read-code/"},{"name":"kernel patch","slug":"kernel-patch","link":"/tags/kernel-patch/"},{"name":"用户内存泄漏","slug":"用户内存泄漏","link":"/tags/%E7%94%A8%E6%88%B7%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F/"},{"name":"内存泄漏","slug":"内存泄漏","link":"/tags/%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F/"},{"name":"valgrind","slug":"valgrind","link":"/tags/valgrind/"},{"name":"内核内存越界","slug":"内核内存越界","link":"/tags/%E5%86%85%E6%A0%B8%E5%86%85%E5%AD%98%E8%B6%8A%E7%95%8C/"},{"name":"KASAN","slug":"KASAN","link":"/tags/KASAN/"},{"name":"内核内存泄漏","slug":"内核内存泄漏","link":"/tags/%E5%86%85%E6%A0%B8%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F/"},{"name":"kmemleak","slug":"kmemleak","link":"/tags/kmemleak/"},{"name":"page_owner","slug":"page-owner","link":"/tags/page-owner/"},{"name":"slub_debug","slug":"slub-debug","link":"/tags/slub-debug/"},{"name":"deadlock","slug":"deadlock","link":"/tags/deadlock/"},{"name":"lockdep","slug":"lockdep","link":"/tags/lockdep/"},{"name":"资源泄漏","slug":"资源泄漏","link":"/tags/%E8%B5%84%E6%BA%90%E6%B3%84%E6%BC%8F/"},{"name":"文件描述符泄漏","slug":"文件描述符泄漏","link":"/tags/%E6%96%87%E4%BB%B6%E6%8F%8F%E8%BF%B0%E7%AC%A6%E6%B3%84%E6%BC%8F/"},{"name":"内存泄露","slug":"内存泄露","link":"/tags/%E5%86%85%E5%AD%98%E6%B3%84%E9%9C%B2/"},{"name":"虚拟地址空间泄漏","slug":"虚拟地址空间泄漏","link":"/tags/%E8%99%9A%E6%8B%9F%E5%9C%B0%E5%9D%80%E7%A9%BA%E9%97%B4%E6%B3%84%E6%BC%8F/"},{"name":"进程调度","slug":"进程调度","link":"/tags/%E8%BF%9B%E7%A8%8B%E8%B0%83%E5%BA%A6/"},{"name":"hungtask","slug":"hungtask","link":"/tags/hungtask/"},{"name":"hardlockup","slug":"hardlockup","link":"/tags/hardlockup/"},{"name":"softlockup","slug":"softlockup","link":"/tags/softlockup/"},{"name":"preempt_count","slug":"preempt-count","link":"/tags/preempt-count/"},{"name":"内核抢占","slug":"内核抢占","link":"/tags/%E5%86%85%E6%A0%B8%E6%8A%A2%E5%8D%A0/"},{"name":"kthread","slug":"kthread","link":"/tags/kthread/"},{"name":"bpf","slug":"bpf","link":"/tags/bpf/"},{"name":"bpftrace","slug":"bpftrace","link":"/tags/bpftrace/"},{"name":"性能稳定性","slug":"性能稳定性","link":"/tags/%E6%80%A7%E8%83%BD%E7%A8%B3%E5%AE%9A%E6%80%A7/"},{"name":"问题系统","slug":"问题系统","link":"/tags/%E9%97%AE%E9%A2%98%E7%B3%BB%E7%BB%9F/"},{"name":"irq","slug":"irq","link":"/tags/irq/"},{"name":"ftrace","slug":"ftrace","link":"/tags/ftrace/"},{"name":"vmtouch","slug":"vmtouch","link":"/tags/vmtouch/"},{"name":"pagecache","slug":"pagecache","link":"/tags/pagecache/"},{"name":"内管管理","slug":"内管管理","link":"/tags/%E5%86%85%E7%AE%A1%E7%AE%A1%E7%90%86/"},{"name":"OOM","slug":"OOM","link":"/tags/OOM/"},{"name":"systemTap","slug":"systemTap","link":"/tags/systemTap/"},{"name":"event trace","slug":"event-trace","link":"/tags/event-trace/"},{"name":"memleak","slug":"memleak","link":"/tags/memleak/"},{"name":"slub","slug":"slub","link":"/tags/slub/"},{"name":"vscode","slug":"vscode","link":"/tags/vscode/"},{"name":"task.json","slug":"task-json","link":"/tags/task-json/"},{"name":"launch.json","slug":"launch-json","link":"/tags/launch-json/"},{"name":"gdb","slug":"gdb","link":"/tags/gdb/"},{"name":"内存泄漏越界","slug":"内存泄漏越界","link":"/tags/%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F%E8%B6%8A%E7%95%8C/"},{"name":"use after free","slug":"use-after-free","link":"/tags/use-after-free/"},{"name":"double free","slug":"double-free","link":"/tags/double-free/"},{"name":"stack_overflow","slug":"stack-overflow","link":"/tags/stack-overflow/"},{"name":"内核栈溢出","slug":"内核栈溢出","link":"/tags/%E5%86%85%E6%A0%B8%E6%A0%88%E6%BA%A2%E5%87%BA/"},{"name":"kprobes","slug":"kprobes","link":"/tags/kprobes/"},{"name":"kretprobes","slug":"kretprobes","link":"/tags/kretprobes/"},{"name":"perf","slug":"perf","link":"/tags/perf/"},{"name":"drop_caches","slug":"drop-caches","link":"/tags/drop-caches/"},{"name":"cache","slug":"cache","link":"/tags/cache/"},{"name":"node","slug":"node","link":"/tags/node/"},{"name":"zone","slug":"zone","link":"/tags/zone/"},{"name":"vmstat","slug":"vmstat","link":"/tags/vmstat/"},{"name":"kvmtool","slug":"kvmtool","link":"/tags/kvmtool/"},{"name":"filesystem","slug":"filesystem","link":"/tags/filesystem/"},{"name":"ext2","slug":"ext2","link":"/tags/ext2/"},{"name":"mount","slug":"mount","link":"/tags/mount/"},{"name":"xattr","slug":"xattr","link":"/tags/xattr/"},{"name":"task_struct","slug":"task-struct","link":"/tags/task-struct/"},{"name":"thread_info","slug":"thread-info","link":"/tags/thread-info/"},{"name":"psi","slug":"psi","link":"/tags/psi/"},{"name":"pressure","slug":"pressure","link":"/tags/pressure/"},{"name":"intrrrupt","slug":"intrrrupt","link":"/tags/intrrrupt/"},{"name":"hrtimer","slug":"hrtimer","link":"/tags/hrtimer/"},{"name":"intrrrupt storm","slug":"intrrrupt-storm","link":"/tags/intrrrupt-storm/"},{"name":"rcu","slug":"rcu","link":"/tags/rcu/"},{"name":"schedule","slug":"schedule","link":"/tags/schedule/"},{"name":"sched latency","slug":"sched-latency","link":"/tags/sched-latency/"},{"name":"memory direct reclaim","slug":"memory-direct-reclaim","link":"/tags/memory-direct-reclaim/"},{"name":"oom","slug":"oom","link":"/tags/oom/"},{"name":"slab","slug":"slab","link":"/tags/slab/"},{"name":"linux kernel","slug":"linux-kernel","link":"/tags/linux-kernel/"},{"name":"QCOM","slug":"QCOM","link":"/tags/QCOM/"},{"name":"cgroup v1","slug":"cgroup-v1","link":"/tags/cgroup-v1/"},{"name":"cgroup v2","slug":"cgroup-v2","link":"/tags/cgroup-v2/"},{"name":"namespace","slug":"namespace","link":"/tags/namespace/"},{"name":"uts namespace","slug":"uts-namespace","link":"/tags/uts-namespace/"},{"name":"pid namespace","slug":"pid-namespace","link":"/tags/pid-namespace/"},{"name":"file attr","slug":"file-attr","link":"/tags/file-attr/"},{"name":"fsck","slug":"fsck","link":"/tags/fsck/"},{"name":"kfence","slug":"kfence","link":"/tags/kfence/"},{"name":"file hole","slug":"file-hole","link":"/tags/file-hole/"},{"name":"sync","slug":"sync","link":"/tags/sync/"},{"name":"hugepage","slug":"hugepage","link":"/tags/hugepage/"},{"name":"memory","slug":"memory","link":"/tags/memory/"},{"name":"vdso","slug":"vdso","link":"/tags/vdso/"},{"name":"dd","slug":"dd","link":"/tags/dd/"},{"name":"dumpe2fs","slug":"dumpe2fs","link":"/tags/dumpe2fs/"},{"name":"debugfs","slug":"debugfs","link":"/tags/debugfs/"},{"name":"interrupt","slug":"interrupt","link":"/tags/interrupt/"},{"name":"aarch64","slug":"aarch64","link":"/tags/aarch64/"},{"name":"preemption","slug":"preemption","link":"/tags/preemption/"},{"name":"地址空间布局随机化","slug":"地址空间布局随机化","link":"/tags/%E5%9C%B0%E5%9D%80%E7%A9%BA%E9%97%B4%E5%B8%83%E5%B1%80%E9%9A%8F%E6%9C%BA%E5%8C%96/"},{"name":"randomize_va_space","slug":"randomize-va-space","link":"/tags/randomize-va-space/"},{"name":"tracepoint","slug":"tracepoint","link":"/tags/tracepoint/"},{"name":"linux native aio","slug":"linux-native-aio","link":"/tags/linux-native-aio/"},{"name":"psoix aio","slug":"psoix-aio","link":"/tags/psoix-aio/"},{"name":"BRK","slug":"BRK","link":"/tags/BRK/"},{"name":"static_key","slug":"static-key","link":"/tags/static-key/"},{"name":"pagemap","slug":"pagemap","link":"/tags/pagemap/"},{"name":"cpu调频","slug":"cpu调频","link":"/tags/cpu%E8%B0%83%E9%A2%91/"},{"name":"eas","slug":"eas","link":"/tags/eas/"},{"name":"acl","slug":"acl","link":"/tags/acl/"},{"name":"android12 gdb64调试进程","slug":"android12-gdb64调试进程","link":"/tags/android12-gdb64%E8%B0%83%E8%AF%95%E8%BF%9B%E7%A8%8B/"},{"name":"android framework","slug":"android-framework","link":"/tags/android-framework/"},{"name":"lock_stat","slug":"lock-stat","link":"/tags/lock-stat/"},{"name":"systrace","slug":"systrace","link":"/tags/systrace/"},{"name":"process_madvise","slug":"process-madvise","link":"/tags/process-madvise/"},{"name":"面试","slug":"面试","link":"/tags/%E9%9D%A2%E8%AF%95/"},{"name":"HW","slug":"HW","link":"/tags/HW/"},{"name":"perf c2c","slug":"perf-c2c","link":"/tags/perf-c2c/"},{"name":"kallsyms","slug":"kallsyms","link":"/tags/kallsyms/"},{"name":"功耗","slug":"功耗","link":"/tags/%E5%8A%9F%E8%80%97/"},{"name":"power management","slug":"power-management","link":"/tags/power-management/"},{"name":"futex","slug":"futex","link":"/tags/futex/"}],"categories":[{"name":"docker","slug":"docker","link":"/categories/docker/"},{"name":"linux内核","slug":"linux内核","link":"/categories/linux%E5%86%85%E6%A0%B8/"},{"name":"server","slug":"server","link":"/categories/server/"},{"name":"开发工具","slug":"开发工具","link":"/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/"},{"name":"shell脚本","slug":"shell脚本","link":"/categories/shell%E8%84%9A%E6%9C%AC/"},{"name":"生活感悟","slug":"生活感悟","link":"/categories/%E7%94%9F%E6%B4%BB%E6%84%9F%E6%82%9F/"},{"name":"kernel debug","slug":"kernel-debug","link":"/categories/kernel-debug/"},{"name":"linux kernel","slug":"linux-kernel","link":"/categories/linux-kernel/"},{"name":"qemu","slug":"qemu","link":"/categories/qemu/"},{"name":"linux schedule","slug":"linux-kernel/linux-schedule","link":"/categories/linux-kernel/linux-schedule/"},{"name":"deadline schedule","slug":"linux-kernel/linux-schedule/deadline-schedule","link":"/categories/linux-kernel/linux-schedule/deadline-schedule/"},{"name":"frequency governer","slug":"linux-kernel/linux-schedule/frequency-governer","link":"/categories/linux-kernel/linux-schedule/frequency-governer/"},{"name":"schedule util","slug":"linux-kernel/linux-schedule/frequency-governer/schedule-util","link":"/categories/linux-kernel/linux-schedule/frequency-governer/schedule-util/"},{"name":"QCOM","slug":"QCOM","link":"/categories/QCOM/"},{"name":"cgroup v1","slug":"cgroup-v1","link":"/categories/cgroup-v1/"},{"name":"namespace","slug":"namespace","link":"/categories/namespace/"},{"name":"cgroup v2","slug":"cgroup-v2","link":"/categories/cgroup-v2/"},{"name":"cgroup v2","slug":"cgroup-v1/cgroup-v2","link":"/categories/cgroup-v1/cgroup-v2/"},{"name":"uts namespace","slug":"namespace/uts-namespace","link":"/categories/namespace/uts-namespace/"},{"name":"idle","slug":"linux-kernel/linux-schedule/idle","link":"/categories/linux-kernel/linux-schedule/idle/"},{"name":"pid namespace","slug":"namespace/pid-namespace","link":"/categories/namespace/pid-namespace/"},{"name":"schedule","slug":"schedule","link":"/categories/schedule/"},{"name":"cache false sharing","slug":"kernel-debug/cache-false-sharing","link":"/categories/kernel-debug/cache-false-sharing/"},{"name":"power management","slug":"power-management","link":"/categories/power-management/"}]}